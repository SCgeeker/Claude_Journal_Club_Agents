#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ä¿®å¾© cite_key
è®€å–æ˜ å°„æ–‡ä»¶ï¼Œæ‰¹é‡æ›´æ–°è«–æ–‡å…ƒæ•¸æ“š
"""

import sqlite3
import sys
import subprocess
import json
import re
from pathlib import Path
from datetime import datetime

sys.stdout.reconfigure(encoding='utf-8')

def analyze_pdf_and_extract_metadata(pdf_path, paper_id):
    """åˆ†æ PDF ä¸¦æå–å…ƒæ•¸æ“š"""
    pdf_path = Path(pdf_path)

    if not pdf_path.exists():
        return None, f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"

    print(f"  ğŸ”„ æ­£åœ¨åˆ†æ: {pdf_path.name}...")

    # æ­¥é©Ÿ 1: å¾æ–‡ä»¶åæå–å¯èƒ½çš„ cite_key
    pdf_stem = pdf_path.stem
    potential_cite_key = None

    # å˜—è©¦åŒ¹é…å¸¸è¦‹æ ¼å¼
    patterns = [
        r'^([A-Z][a-z]+)-?(\d{4})[a-z]?$',  # Her-2012, Her2012a
        r'^([A-Z][a-z]+[A-Z][a-z]+)-?(\d{4})$',  # ChenYiRu-2020
        r'^([A-Z][a-z]+)_([A-Z][a-z]+)-?(\d{4})$',  # Glenberg_Kaschak-2002
    ]

    for pattern in patterns:
        match = re.match(pattern, pdf_stem)
        if match:
            if len(match.groups()) == 2:
                author = match.group(1)
                year = match.group(2)
            else:
                author = match.group(1) + match.group(2)
                year = match.group(3)
            potential_cite_key = f"{author}-{year}"
            print(f"     å¾æ–‡ä»¶åæå–: {potential_cite_key}")
            break

    # æ­¥é©Ÿ 2: ä½¿ç”¨ analyze_paper.py åˆ†æ PDF
    temp_json = Path(f"temp_analysis_{paper_id}.json")

    cmd = [
        'python', 'analyze_paper.py',
        str(pdf_path),
        '--format', 'json',
        '--output-json', str(temp_json)
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=180,
            errors='replace'
        )

        if result.returncode == 0 and temp_json.exists():
            with open(temp_json, 'r', encoding='utf-8') as f:
                analysis = json.load(f)

            # æå–å…ƒæ•¸æ“š
            metadata = {
                'cite_key': potential_cite_key,
                'title': analysis.get('title', ''),
                'authors': analysis.get('authors', []),
                'year': analysis.get('year'),
                'abstract': analysis.get('abstract', ''),
                'keywords': analysis.get('keywords', [])
            }

            # å¦‚æœæ²’æœ‰å¾æ–‡ä»¶åæå–åˆ° cite_keyï¼Œå˜—è©¦å¾å…ƒæ•¸æ“šç”Ÿæˆ
            if not metadata['cite_key'] and metadata['authors'] and metadata['year']:
                first_author = metadata['authors'][0].split()[-1] if metadata['authors'] else ''
                if first_author:
                    metadata['cite_key'] = f"{first_author}-{metadata['year']}"

            temp_json.unlink()
            return metadata, None
        else:
            if temp_json.exists():
                temp_json.unlink()
            error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥éŒ¯èª¤"
            return None, f"åˆ†æå¤±æ•—: {error_msg}"

    except subprocess.TimeoutExpired:
        if temp_json.exists():
            temp_json.unlink()
        return None, "è™•ç†è¶…æ™‚ (180ç§’)"
    except Exception as e:
        if temp_json.exists():
            temp_json.unlink()
        return None, f"ç•°å¸¸: {str(e)}"

def update_paper_metadata(conn, paper_id, metadata):
    """æ›´æ–°è«–æ–‡å…ƒæ•¸æ“š"""
    cursor = conn.cursor()

    cite_key = metadata['cite_key']

    if not cite_key:
        return False, "ç¼ºå°‘ cite_key"

    # æª¢æŸ¥ cite_key è¡çª
    cursor.execute('SELECT id FROM papers WHERE cite_key = ?', (cite_key,))
    existing = cursor.fetchone()

    if existing and existing[0] != paper_id:
        # æ·»åŠ å¾Œç¶´
        suffix = 'a'
        while True:
            new_cite_key = f"{cite_key}{suffix}"
            cursor.execute('SELECT id FROM papers WHERE cite_key = ?', (new_cite_key,))
            if not cursor.fetchone():
                cite_key = new_cite_key
                metadata['cite_key'] = cite_key
                print(f"     âš ï¸  cite_key è¡çªï¼Œä½¿ç”¨: {cite_key}")
                break
            suffix = chr(ord(suffix) + 1)

    # æ§‹å»ºæ›´æ–°èªå¥
    update_fields = []
    update_values = []

    if cite_key:
        update_fields.append('cite_key = ?')
        update_values.append(cite_key)

    if metadata['title'] and metadata['title'] != 'Untitled':
        update_fields.append('title = ?')
        update_values.append(metadata['title'])

    if metadata['year']:
        update_fields.append('year = ?')
        update_values.append(metadata['year'])

    if metadata['authors']:
        authors_str = ', '.join(metadata['authors'])
        update_fields.append('authors = ?')
        update_values.append(authors_str)

    if metadata['abstract']:
        update_fields.append('abstract = ?')
        update_values.append(metadata['abstract'])

    if metadata['keywords']:
        keywords_str = ', '.join(metadata['keywords'])
        update_fields.append('keywords = ?')
        update_values.append(keywords_str)

    if update_fields:
        update_values.append(paper_id)
        sql = f"UPDATE papers SET {', '.join(update_fields)} WHERE id = ?"
        cursor.execute(sql, update_values)
        conn.commit()
        return True, None
    else:
        return False, "æ²’æœ‰å¯æ›´æ–°çš„å­—æ®µ"

def main():
    # æ˜ å°„æ–‡ä»¶æ ¼å¼ï¼špaper_id: pdf_path
    mapping_file = Path("pdf_path_mapping.txt")

    if not mapping_file.exists():
        print("è«‹å‰µå»º pdf_path_mapping.txt æ–‡ä»¶")
        print("æ ¼å¼ï¼šæ¯è¡Œä¸€å€‹æ˜ å°„ï¼Œæ ¼å¼ç‚º: paper_id|pdf_path")
        print()
        print("ç¯„ä¾‹:")
        print("1|D:\\PDFs\\Her-2012.pdf")
        print("3|D:\\PDFs\\Zwaan-2002.pdf")
        print()
        print("æˆ–è€…è¼¸å…¥ 'q' è·³éæ‰¹è™•ç†ï¼Œæ”¹ç”¨å–®å€‹è™•ç†")
        return

    # è®€å–æ˜ å°„
    mappings = []
    with open(mapping_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split('|')
            if len(parts) == 2:
                paper_id = int(parts[0].strip())
                pdf_path = parts[1].strip().strip('"').strip("'")
                mappings.append((paper_id, pdf_path))

    if not mappings:
        print("æ˜ å°„æ–‡ä»¶ç‚ºç©º")
        return

    print("=" * 80)
    print(f"æ‰¹é‡ä¿®å¾© cite_key - å…± {len(mappings)} ç¯‡è«–æ–‡")
    print("=" * 80)
    print()

    conn = sqlite3.connect('knowledge_base/index.db')
    results = {
        'success': 0,
        'failed': 0,
        'errors': []
    }

    for paper_id, pdf_path in mappings:
        print(f"[{results['success'] + results['failed'] + 1}/{len(mappings)}] Paper {paper_id}")

        # åˆ†æ PDF
        metadata, error = analyze_pdf_and_extract_metadata(pdf_path, paper_id)

        if error:
            print(f"  âŒ {error}")
            results['failed'] += 1
            results['errors'].append({
                'paper_id': paper_id,
                'pdf_path': pdf_path,
                'error': error
            })
            continue

        # æ›´æ–°æ•¸æ“šåº«
        success, error = update_paper_metadata(conn, paper_id, metadata)

        if success:
            print(f"  âœ… æˆåŠŸæ›´æ–°")
            print(f"     cite_key: {metadata['cite_key']}")
            print(f"     year: {metadata['year'] if metadata['year'] else 'N/A'}")
            results['success'] += 1
        else:
            print(f"  âŒ {error}")
            results['failed'] += 1
            results['errors'].append({
                'paper_id': paper_id,
                'pdf_path': pdf_path,
                'error': error
            })

        print()

    conn.close()

    # é¡¯ç¤ºç¸½çµ
    print("=" * 80)
    print("è™•ç†ç¸½çµ")
    print("=" * 80)
    print(f"æˆåŠŸ: {results['success']}")
    print(f"å¤±æ•—: {results['failed']}")

    if results['failed'] > 0:
        print()
        print("å¤±æ•—çš„è«–æ–‡:")
        for error in results['errors']:
            print(f"  Paper {error['paper_id']}: {error['error']}")

    # ä¿å­˜æ—¥èªŒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = Path(f"batch_fix_log_{timestamp}.json")
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print()
    print(f"æ—¥èªŒå·²ä¿å­˜: {log_file}")

if __name__ == '__main__':
    main()
