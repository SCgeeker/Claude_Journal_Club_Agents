#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆPDFæ¨¡ç³ŠåŒ¹é…å·¥å…·
å¾Markdownå®Œæ•´å…§å®¹ä¸­æå–ä½œè€…å’Œå¹´ä»½ä¿¡æ¯é€²è¡ŒåŒ¹é…
"""

import sys
import io
import sqlite3
from pathlib import Path
import re

# Windows UTF-8
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def extract_author_year_from_markdown(md_path):
    """
    å¾Markdownæ–‡ä»¶ä¸­æå–ä½œè€…å’Œå¹´ä»½ä¿¡æ¯

    ç­–ç•¥:
    1. æŸ¥æ‰¾ "To cite this article: Author (YYYY):"
    2. æŸ¥æ‰¾ "Published online: DD Mon YYYY"
    3. æŸ¥æ‰¾ä½œè€…åˆ—è¡¨
    4. æŸ¥æ‰¾å¹´ä»½ (YYYY)
    """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–å‰2000å­—å…ƒï¼ˆé€šå¸¸åŒ…å«å…ƒæ•¸æ“šï¼‰
        header = content[:2000]

        authors = []
        year = None

        # ç­–ç•¥1: æŸ¥æ‰¾ "To cite this article: Author (YYYY):"
        cite_match = re.search(r'To cite this article:?\s*(.+?)\s*\((\d{4})\)', header, re.IGNORECASE)
        if cite_match:
            author_str = cite_match.group(1)
            year = int(cite_match.group(2))

            # æå–ä½œè€…å§“æ°
            # æ ¼å¼: "B. de Koning, S. Wassenburg" æˆ– "Koning, B. & Smith, J."
            author_parts = re.split(r'[,&]', author_str)
            for part in author_parts:
                # ç§»é™¤é¦–å­—æ¯ç¸®å¯«
                part = re.sub(r'\b[A-Z]\.\s*', '', part.strip())
                # æå–å§“æ°ï¼ˆæœ€å¾Œä¸€å€‹å–®è©ï¼‰
                words = part.split()
                if words:
                    # è™•ç† "de Koning" é€™é¡è¤‡åˆå§“æ°
                    if len(words) >= 2 and words[-2].lower() in ['de', 'van', 'von', 'del', 'la']:
                        last_name = ' '.join(words[-2:])
                    else:
                        last_name = words[-1]

                    # æ¸…ç†å§“æ°
                    last_name = re.sub(r'[^a-zA-Z]', '', last_name)
                    if last_name and len(last_name) > 2:
                        authors.append(last_name)

        # ç­–ç•¥2: å¦‚æœæ²’æ‰¾åˆ°å¹´ä»½ï¼ŒæŸ¥æ‰¾ "Published online: DD Mon YYYY"
        if not year:
            pub_match = re.search(r'Published.*?(\d{4})', header, re.IGNORECASE)
            if pub_match:
                year = int(pub_match.group(1))

        # ç­–ç•¥3: å¦‚æœæ²’æ‰¾åˆ°å¹´ä»½ï¼ŒæŸ¥æ‰¾æœŸåˆŠå¼•ç”¨ä¸­çš„å¹´ä»½
        # æ ¼å¼: "Journal Name YYYY 13: 168" æˆ– "Journal (YYYY)"
        if not year:
            journal_year = re.search(r'(?:Science|Psychology|Journal|Review)\s+(\d{4})\s+\d+:', header, re.IGNORECASE)
            if journal_year:
                year = int(journal_year.group(1))

        # ç­–ç•¥4: å¦‚æœé‚„æ²’æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾ç¨ç«‹çš„å››ä½æ•¸å¹´ä»½ï¼ˆä½†æ’é™¤2025ï¼Œé‚£æ˜¯å‰µå»ºæ™‚é–“ï¼‰
        if not year:
            year_matches = re.findall(r'\b(19\d{2}|20[01]\d|202[0-4])\b', header)
            # æ’é™¤ created: 2025 é€™é¡çš„å¹´ä»½
            filtered_years = [y for y in year_matches if y != '2025']
            if filtered_years:
                year = int(filtered_years[0])

        # ç­–ç•¥4: å¦‚æœæ²’æ‰¾åˆ°ä½œè€…ï¼ŒæŸ¥æ‰¾ä½œè€…åˆ—è¡¨æ ¼å¼
        if not authors:
            # æ ¼å¼: "Author1, Author2 & Author3"
            author_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Z]\.\s+)?[A-Z][a-z]+(?:\s*[,&]\s*[A-Z][a-z]+(?:\s+[A-Z]\.\s+)?[A-Z][a-z]+)*)', header)
            if author_match:
                author_str = author_match.group(1)
                author_parts = re.split(r'[,&]', author_str)
                for part in author_parts:
                    words = part.strip().split()
                    if words:
                        last_name = words[-1]
                        last_name = re.sub(r'[^a-zA-Z]', '', last_name)
                        if last_name and len(last_name) > 2:
                            authors.append(last_name)

        return authors[:3], year  # åªè¿”å›å‰3ä½ä½œè€…

    except Exception as e:
        print(f"âš ï¸  è®€å–å¤±æ•— {md_path}: {e}")
        return [], None


def generate_possible_bibkeys(authors, year):
    """
    æ ¹æ“šä½œè€…å’Œå¹´ä»½ç”Ÿæˆå¯èƒ½çš„bibkeyçµ„åˆ

    Args:
        authors: ['Koning', 'Wassenburg', 'Bos']
        year: 2017

    Returns:
        ['Koning-2017', 'deKoning-2017', 'Wassenburg-2017', ...]
    """
    if not year:
        return []

    bibkeys = []

    for author in authors:
        # åŸºæœ¬æ ¼å¼: Author-YYYY
        bibkeys.append(f"{author}-{year}")

        # é¦–å­—æ¯å°å¯«: author-YYYY
        bibkeys.append(f"{author.lower()}-{year}")

        # è™•ç†è¤‡åˆå§“æ°
        if ' ' in author:
            parts = author.split()
            # de Koning -> deKoning-2017
            bibkeys.append(f"{''.join(parts)}-{year}")
            # de Koning -> Koning-2017 (åªç”¨æœ€å¾Œéƒ¨åˆ†)
            bibkeys.append(f"{parts[-1]}-{year}")

    return list(set(bibkeys))  # å»é‡


def main():
    db_path = "knowledge_base/index.db"
    pdf_folder = Path("D:/core/research/Program_verse/+/pdf")

    print(f"\n{'='*80}")
    print(f"ğŸ” å¢å¼·ç‰ˆPDFæ¨¡ç³ŠåŒ¹é…å·¥å…·")
    print(f"{'='*80}\n")

    # æƒæPDFè³‡æ–™å¤¾
    print(f"ğŸ“ æƒæPDFè³‡æ–™å¤¾: {pdf_folder}")
    pdf_files = {f.stem: str(f) for f in pdf_folder.glob('*.pdf')}
    print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶\n")

    # æŸ¥è©¢æ²’æœ‰å°æ‡‰PDFçš„è«–æ–‡
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, title, cite_key, file_path
        FROM papers
        ORDER BY id
    ''')

    all_papers = cursor.fetchall()
    conn.close()

    # ç¯©é¸å‡ºæ²’æœ‰å°æ‡‰PDFçš„è«–æ–‡
    no_pdf_papers = []

    for pid, title, cite_key, file_path in all_papers:
        md_stem = Path(file_path).stem if file_path else None
        has_pdf = False

        # æª¢æŸ¥æ˜¯å¦æœ‰PDF
        if md_stem and md_stem in pdf_files:
            has_pdf = True
        elif cite_key and cite_key in pdf_files:
            has_pdf = True

        if not has_pdf:
            no_pdf_papers.append((pid, title, cite_key, file_path))

    print(f"ğŸ“Š çµ±è¨ˆ:")
    print(f"  ç¸½è«–æ–‡æ•¸: {len(all_papers)}")
    print(f"  æœ‰PDF: {len(all_papers) - len(no_pdf_papers)}")
    print(f"  ç„¡PDF: {len(no_pdf_papers)}")

    if not no_pdf_papers:
        print("\nâœ… æ‰€æœ‰è«–æ–‡éƒ½æœ‰å°æ‡‰çš„PDFï¼")
        return

    print(f"\n{'='*80}")
    print(f"ğŸ” å¾Markdownå…§å®¹æå–ä½œè€…å’Œå¹´ä»½")
    print(f"{'='*80}\n")

    # ç”¨æ–¼å„²å­˜åŒ¹é…çµæœ
    matched_papers = []

    for i, (pid, title, cite_key, file_path) in enumerate(no_pdf_papers, 1):
        print(f"[{i}/{len(no_pdf_papers)}] ID {pid}: {title[:55]}")

        # å¾Markdownæå–ä½œè€…å’Œå¹´ä»½
        authors, year = extract_author_year_from_markdown(file_path)

        if authors:
            print(f"  ğŸ“ æå–ä½œè€…: {', '.join(authors)}")
        if year:
            print(f"  ğŸ“… æå–å¹´ä»½: {year}")

        # ç”Ÿæˆå¯èƒ½çš„bibkey
        possible_bibkeys = generate_possible_bibkeys(authors, year)

        if possible_bibkeys:
            print(f"  ğŸ”‘ å¯èƒ½çš„bibkey: {', '.join(possible_bibkeys[:5])}")

            # æª¢æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„PDF
            found_pdfs = []
            for bibkey in possible_bibkeys:
                if bibkey in pdf_files:
                    found_pdfs.append((bibkey, pdf_files[bibkey]))

            if found_pdfs:
                print(f"  âœ… æ‰¾åˆ° {len(found_pdfs)} å€‹åŒ¹é…çš„PDF:")
                for bibkey, pdf_path in found_pdfs:
                    print(f"     â†’ {bibkey}.pdf")

                matched_papers.append({
                    'paper_id': pid,
                    'paper_title': title,
                    'authors': authors,
                    'year': year,
                    'matches': found_pdfs
                })
            else:
                print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„PDF")
        else:
            print(f"  âš ï¸  ç„¡æ³•æå–ä½œè€…/å¹´ä»½ä¿¡æ¯")

        print()

    # é¡¯ç¤ºç¸½çµ
    print(f"{'='*80}")
    print(f"ğŸ“Š åŒ¹é…ç¸½çµ")
    print(f"{'='*80}\n")

    print(f"æ‰¾åˆ°åŒ¹é…çš„è«–æ–‡: {len(matched_papers)}/{len(no_pdf_papers)}")
    print(f"æœªæ‰¾åˆ°åŒ¹é…: {len(no_pdf_papers) - len(matched_papers)}")

    if matched_papers:
        print(f"\nğŸ“‹ åŒ¹é…è©³æƒ…:\n")
        for item in matched_papers:
            print(f"ID {item['paper_id']:2d}: {item['paper_title'][:50]}")
            print(f"      ä½œè€…: {', '.join(item['authors'])}")
            print(f"      å¹´ä»½: {item['year']}")
            print(f"      PDF: {', '.join([Path(m[1]).name for m in item['matches']])}")
            print()

        # ä¿å­˜çµæœ
        import json
        with open('enhanced_match_results.json', 'w', encoding='utf-8') as f:
            # è½‰æ›Pathç‚ºå­—ä¸²ä»¥ä¾¿JSONåºåˆ—åŒ–
            results_for_json = []
            for item in matched_papers:
                item_copy = item.copy()
                item_copy['matches'] = [(bibkey, str(path)) for bibkey, path in item['matches']]
                results_for_json.append(item_copy)

            json.dump(results_for_json, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ åŒ¹é…çµæœå·²ä¿å­˜åˆ°: enhanced_match_results.json")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   ä½¿ç”¨ interactive_repair.py è™•ç†é€™äº›è«–æ–‡")

    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    main()
