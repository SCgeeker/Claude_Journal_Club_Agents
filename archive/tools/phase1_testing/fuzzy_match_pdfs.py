#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFæ¨™é¡Œæ¨¡ç³ŠåŒ¹é…å·¥å…·
å°æ²’æœ‰å°æ‡‰PDFçš„è«–æ–‡ï¼Œä½¿ç”¨æ¨™é¡Œåœ¨PDFè³‡æ–™å¤¾ä¸­æ¨¡ç³Šæœç´¢
"""

import sys
import io
import sqlite3
from pathlib import Path
from difflib import SequenceMatcher
import re

# Windows UTF-8
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def normalize_title(title):
    """
    æ¨™æº–åŒ–æ¨™é¡Œç”¨æ–¼åŒ¹é…
    - è½‰å°å¯«
    - ç§»é™¤æ¨™é»žç¬¦è™Ÿ
    - ç§»é™¤å¤šé¤˜ç©ºæ ¼
    """
    # è½‰å°å¯«
    title = title.lower()

    # ç§»é™¤ç‰¹æ®Šå­—å…ƒ
    title = re.sub(r'[:\-â€“â€”()[\]{},.;!?\'\"&]', ' ', title)

    # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    title = ' '.join(title.split())

    return title


def extract_title_from_bibkey(bibkey):
    """
    å¾žbibkeyæå–å¯èƒ½çš„æ¨™é¡Œé—œéµè©ž
    ä¾‹å¦‚: "Altmann-2019" -> "Altmann"
    """
    parts = bibkey.split('-')
    if len(parts) >= 2:
        # ç§»é™¤å¹´ä»½éƒ¨åˆ†
        return parts[0]
    return bibkey


def similarity_score(str1, str2):
    """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    return SequenceMatcher(None, str1, str2).ratio()


def find_matching_pdfs(paper_title, pdf_files, threshold=0.5, max_results=5):
    """
    åœ¨PDFæª”æ¡ˆä¸­å°‹æ‰¾èˆ‡è«–æ–‡æ¨™é¡Œç›¸ä¼¼çš„æª”æ¡ˆ

    Args:
        paper_title: è«–æ–‡æ¨™é¡Œ
        pdf_files: PDFæª”æ¡ˆåˆ—è¡¨ {bibkey: Path}
        threshold: æœ€ä½Žç›¸ä¼¼åº¦é–¾å€¼
        max_results: æœ€å¤šè¿”å›žçµæžœæ•¸

    Returns:
        [(bibkey, pdf_path, similarity), ...]
    """
    normalized_title = normalize_title(paper_title)

    matches = []

    for bibkey, pdf_path in pdf_files.items():
        # å¾žbibkeyæå–ä½œè€…å
        author_name = extract_title_from_bibkey(bibkey)
        normalized_bibkey = normalize_title(bibkey)
        normalized_author = normalize_title(author_name)

        # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆå¤šç¨®ç­–ç•¥ï¼‰

        # ç­–ç•¥1: æ¨™é¡Œèˆ‡bibkeyæ•´é«”ç›¸ä¼¼åº¦
        score1 = similarity_score(normalized_title, normalized_bibkey)

        # ç­–ç•¥2: æ¨™é¡ŒåŒ…å«ä½œè€…å
        score2 = 0
        if normalized_author in normalized_title or normalized_title in normalized_bibkey:
            score2 = 0.7

        # ç­–ç•¥3: æå–æ¨™é¡Œé—œéµè©žèˆ‡bibkeyåŒ¹é…
        title_words = set(normalized_title.split())
        bibkey_words = set(normalized_bibkey.split())

        # Jaccardç›¸ä¼¼åº¦
        if title_words and bibkey_words:
            intersection = title_words & bibkey_words
            union = title_words | bibkey_words
            score3 = len(intersection) / len(union) if union else 0
        else:
            score3 = 0

        # ç¶œåˆè©•åˆ†ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
        final_score = max(score1 * 0.4, score2 * 0.3, score3 * 0.3)

        if final_score >= threshold:
            matches.append((bibkey, pdf_path, final_score))

    # æŒ‰ç›¸ä¼¼åº¦æŽ’åº
    matches.sort(key=lambda x: x[2], reverse=True)

    return matches[:max_results]


def main():
    db_path = "knowledge_base/index.db"
    pdf_folder = Path("D:/core/research/Program_verse/+/pdf")

    print(f"\n{'='*80}")
    print(f"ðŸ” PDFæ¨™é¡Œæ¨¡ç³ŠåŒ¹é…å·¥å…·")
    print(f"{'='*80}\n")

    # æŽƒæPDFè³‡æ–™å¤¾
    print(f"ðŸ“ æŽƒæPDFè³‡æ–™å¤¾: {pdf_folder}")
    pdf_files = {f.stem: f for f in pdf_folder.glob('*.pdf')}
    print(f"ðŸ“„ æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶\n")

    # æŸ¥è©¢æ²’æœ‰å°æ‡‰PDFçš„è«–æ–‡
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, title, cite_key, file_path
        FROM papers
        ORDER BY id
    ''')

    all_papers = cursor.fetchall()
    conn.close()

    # ç¯©é¸å‡ºæ²’æœ‰å°æ‡‰PDFçš„è«–æ–‡
    no_pdf_papers = []

    for pid, title, cite_key, file_path in all_papers:
        md_stem = Path(file_path).stem if file_path else None
        has_pdf = False

        # æª¢æŸ¥æ˜¯å¦æœ‰PDF
        if md_stem and md_stem in pdf_files:
            has_pdf = True
        elif cite_key and cite_key in pdf_files:
            has_pdf = True

        if not has_pdf:
            no_pdf_papers.append((pid, title, cite_key, file_path))

    print(f"ðŸ“Š çµ±è¨ˆ:")
    print(f"  ç¸½è«–æ–‡æ•¸: {len(all_papers)}")
    print(f"  æœ‰PDF: {len(all_papers) - len(no_pdf_papers)}")
    print(f"  ç„¡PDF: {len(no_pdf_papers)}")

    if not no_pdf_papers:
        print("\nâœ… æ‰€æœ‰è«–æ–‡éƒ½æœ‰å°æ‡‰çš„PDFï¼")
        return

    print(f"\n{'='*80}")
    print(f"ðŸ” é–‹å§‹æ¨¡ç³ŠåŒ¹é… {len(no_pdf_papers)} ç¯‡è«–æ–‡")
    print(f"{'='*80}\n")

    # ç”¨æ–¼å„²å­˜åŒ¹é…çµæžœ
    matched_papers = []

    for i, (pid, title, cite_key, file_path) in enumerate(no_pdf_papers, 1):
        print(f"[{i}/{len(no_pdf_papers)}] ID {pid}: {title[:60]}")

        # å°‹æ‰¾åŒ¹é…çš„PDF
        matches = find_matching_pdfs(title, pdf_files, threshold=0.3, max_results=5)

        if matches:
            print(f"  ðŸ“Œ æ‰¾åˆ° {len(matches)} å€‹å¯èƒ½çš„åŒ¹é…:")
            for j, (bibkey, pdf_path, score) in enumerate(matches, 1):
                confidence = "é«˜" if score >= 0.7 else "ä¸­" if score >= 0.5 else "ä½Ž"
                print(f"     {j}. [{score*100:.1f}% ç›¸ä¼¼åº¦ - {confidence}] {bibkey}.pdf")

            matched_papers.append({
                'paper_id': pid,
                'paper_title': title,
                'matches': matches
            })
        else:
            print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„PDF")

        print()

    # é¡¯ç¤ºç¸½çµ
    print(f"{'='*80}")
    print(f"ðŸ“Š åŒ¹é…ç¸½çµ")
    print(f"{'='*80}\n")

    high_confidence = sum(1 for p in matched_papers if p['matches'][0][2] >= 0.7)
    medium_confidence = sum(1 for p in matched_papers if 0.5 <= p['matches'][0][2] < 0.7)
    low_confidence = sum(1 for p in matched_papers if p['matches'][0][2] < 0.5)

    print(f"æ‰¾åˆ°åŒ¹é…çš„è«–æ–‡: {len(matched_papers)}/{len(no_pdf_papers)}")
    print(f"  é«˜ä¿¡åº¦ (â‰¥70%): {high_confidence}")
    print(f"  ä¸­ä¿¡åº¦ (50-69%): {medium_confidence}")
    print(f"  ä½Žä¿¡åº¦ (<50%): {low_confidence}")
    print(f"\næœªæ‰¾åˆ°åŒ¹é…: {len(no_pdf_papers) - len(matched_papers)}")

    # ä¿å­˜çµæžœåˆ°æ–‡ä»¶ä¾›å¾ŒçºŒè™•ç†
    if matched_papers:
        import json
        with open('fuzzy_match_results.json', 'w', encoding='utf-8') as f:
            json.dump(matched_papers, f, ensure_ascii=False, indent=2)

        print(f"\nðŸ’¾ åŒ¹é…çµæžœå·²ä¿å­˜åˆ°: fuzzy_match_results.json")
        print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æŸ¥çœ‹ fuzzy_match_results.json ç¢ºèªåŒ¹é…")
        print(f"   2. ä½¿ç”¨ confirm_fuzzy_matches.py ç¢ºèªä¸¦ä¿®å¾©")

    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    main()
