#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”ŸæˆçŸ¥è­˜åº«å…ƒæ•¸æ“šè³ªé‡å ±å‘Š
"""

import sqlite3
import sys
import io
from pathlib import Path
from typing import Dict, List
import json

# Windows UTF-8 æ”¯æ´
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def analyze_quality(db_path: str = "knowledge_base/index.db") -> Dict:
    """åˆ†æå…ƒæ•¸æ“šè³ªé‡"""

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ç²å–æ‰€æœ‰è«–æ–‡
    cursor.execute("""
        SELECT id, title, authors, year, keywords, abstract, file_path
        FROM papers
    """)

    papers = cursor.fetchall()

    stats = {
        'total': len(papers),
        'missing_year': 0,
        'missing_keywords': 0,
        'missing_abstract': 0,
        'invalid_title': 0,
        'file_not_found': 0,
        'complete': 0,
        'quality_score': 0
    }

    issues = []

    for pid, title, authors, year, keywords, abstract, file_path in papers:
        paper_issues = []

        # æª¢æŸ¥å¹´ä»½
        if not year:
            stats['missing_year'] += 1
            paper_issues.append('ç¼ºå°‘å¹´ä»½')

        # æª¢æŸ¥é—œéµè©
        if not keywords or keywords == '[]':
            stats['missing_keywords'] += 1
            paper_issues.append('ç¼ºå°‘é—œéµè©')
        else:
            try:
                kw_list = json.loads(keywords)
                if len(kw_list) < 3:
                    paper_issues.append(f'é—œéµè©éå°‘ ({len(kw_list)}å€‹)')
            except:
                pass

        # æª¢æŸ¥æ‘˜è¦
        if not abstract or abstract == 'None' or len(abstract) < 50:
            stats['missing_abstract'] += 1
            paper_issues.append('ç¼ºå°‘æ‘˜è¦')

        # æª¢æŸ¥æ¨™é¡Œ
        invalid_title_patterns = ['Journal Pre-proof', 'https://', 'http://', 'downloaded by']
        if any(pattern in title for pattern in invalid_title_patterns):
            stats['invalid_title'] += 1
            paper_issues.append('ç„¡æ•ˆæ¨™é¡Œ')

        # æª¢æŸ¥æª”æ¡ˆ
        if not Path(file_path).exists():
            stats['file_not_found'] += 1
            paper_issues.append('æª”æ¡ˆä¸å­˜åœ¨')

        # å®Œæ•´åº¦
        if not paper_issues:
            stats['complete'] += 1

        if paper_issues:
            issues.append({
                'id': pid,
                'title': title[:50],
                'issues': paper_issues
            })

    # è¨ˆç®—è³ªé‡åˆ†æ•¸
    total = stats['total']
    if total > 0:
        completeness = (total - stats['missing_year'] - stats['missing_keywords'] - stats['missing_abstract']) / (total * 3)
        stats['quality_score'] = int(completeness * 100)

    conn.close()

    return stats, issues

def generate_report(stats: Dict, issues: List[Dict], output_path: str = "QUALITY_REPORT.md"):
    """ç”Ÿæˆ Markdown å ±å‘Š"""

    report = f"""# çŸ¥è­˜åº«å…ƒæ•¸æ“šè³ªé‡å ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {Path().resolve()}
**è³‡æ–™åº«**: knowledge_base/index.db

---

## ğŸ“Š ç¸½è¦½

| æŒ‡æ¨™ | æ•¸é‡ | ç™¾åˆ†æ¯” |
|------|------|--------|
| **ç¸½è«–æ–‡æ•¸** | {stats['total']} | 100% |
| **å®Œæ•´è«–æ–‡** | {stats['complete']} | {stats['complete']/stats['total']*100:.1f}% |
| **ç¼ºå°‘å¹´ä»½** | {stats['missing_year']} | {stats['missing_year']/stats['total']*100:.1f}% |
| **ç¼ºå°‘é—œéµè©** | {stats['missing_keywords']} | {stats['missing_keywords']/stats['total']*100:.1f}% |
| **ç¼ºå°‘æ‘˜è¦** | {stats['missing_abstract']} | {stats['missing_abstract']/stats['total']*100:.1f}% |
| **ç„¡æ•ˆæ¨™é¡Œ** | {stats['invalid_title']} | {stats['invalid_title']/stats['total']*100:.1f}% |
| **æª”æ¡ˆä¸å­˜åœ¨** | {stats['file_not_found']} | {stats['file_not_found']/stats['total']*100:.1f}% |

**æ•´é«”è³ªé‡åˆ†æ•¸**: {stats['quality_score']}/100

---

## âš ï¸ å•é¡Œè«–æ–‡åˆ—è¡¨

å…± {len(issues)} ç¯‡è«–æ–‡æœ‰å•é¡Œï¼š

"""

    for issue in issues:
        report += f"\n### ID {issue['id']}: {issue['title']}\n\n"
        for problem in issue['issues']:
            report += f"- âŒ {problem}\n"

    report += f"""

---

## ğŸ’¡ ä¿®å¾©å»ºè­°

### ç«‹å³è¡Œå‹•

1. **ä¿®å¾©ç¼ºå°‘å¹´ä»½** ({stats['missing_year']} ç¯‡)
   ```bash
   python fix_metadata.py --batch --field year
   ```

2. **ä¿®å¾©ç¼ºå°‘é—œéµè©** ({stats['missing_keywords']} ç¯‡)
   ```bash
   python llm_metadata_generator.py --batch --provider gemini
   ```

3. **ä¿®å¾©ç¼ºå°‘æ‘˜è¦** ({stats['missing_abstract']} ç¯‡)
   ```bash
   python llm_metadata_generator.py --batch --provider gemini
   ```

4. **æ¸…ç†æª”æ¡ˆä¸å­˜åœ¨çš„è¨˜éŒ„** ({stats['file_not_found']} ç¯‡)
   ```bash
   python cleanup_db.py --delete
   ```

### é æœŸæ”¹é€²

ä¿®å¾©å¾Œé è¨ˆè³ªé‡åˆ†æ•¸å¯é”åˆ° **85+/100**

---

**å ±å‘Šç”Ÿæˆå·¥å…·**: generate_quality_report.py
"""

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… å ±å‘Šå·²ç”Ÿæˆ: {output_path}")

if __name__ == "__main__":
    print("åˆ†æçŸ¥è­˜åº«å…ƒæ•¸æ“šè³ªé‡...")
    stats, issues = analyze_quality()

    print(f"\nç¸½è«–æ–‡æ•¸: {stats['total']}")
    print(f"å®Œæ•´è«–æ–‡: {stats['complete']} ({stats['complete']/stats['total']*100:.1f}%)")
    print(f"è³ªé‡åˆ†æ•¸: {stats['quality_score']}/100\n")

    generate_report(stats, issues)
