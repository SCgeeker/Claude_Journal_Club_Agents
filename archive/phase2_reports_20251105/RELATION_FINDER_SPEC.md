# relation-finder è©³ç´°å¯¦ä½œè¦æ ¼

**ç‰ˆæœ¬**: 1.0
**é–‹å§‹æ—¥æœŸ**: 2025-11-02
**å„ªå…ˆç´š**: Phase 2.1 P1
**é è¨ˆå®Œæˆ**: 3-4å¤©

---

## ğŸ“‹ å¯è¦–åŒ–æ ¼å¼è¦ç¯„ï¼ˆä½¿ç”¨ç¾æœ‰Zettelkastenæ ¼å¼ï¼‰

### Mermaidåœ–è¡¨æ ¼å¼

åŸºæ–¼ç¾æœ‰ `output/zettelkasten_notes/zettel_index.md` çš„Mermaidæ ¼å¼ï¼Œrelation-finderå°‡æ¡ç”¨ç›¸åŒçš„**graph TD**é¢¨æ ¼ï¼š

```mermaid
graph TD
    A["ç¯€é»æ¨™é¡Œ1"]
    B["ç¯€é»æ¨™é¡Œ2"]
    C["ç¯€é»æ¨™é¡Œ3"]

    A --> B  # å¯¦ç·šé‚Šï¼ˆå¼·å¼•ç”¨/ä¸»è¦é—œä¿‚ï¼‰
    B -.-> C  # è™›ç·šé‚Šï¼ˆå¼±å¼•ç”¨/åƒè€ƒé—œä¿‚ï¼‰
```

### å¼•ç”¨é—œä¿‚å¯è¦–åŒ–ç¤ºä¾‹

**åœ–è¡¨çµæ§‹**:
- **ç¯€é»**: è«–æ–‡IDå’Œæ¨™é¡Œ `Paper-14["è«–æ–‡æ¨™é¡Œ"]`
- **é‚Š**:
  - `--> ` (å¯¦ç·š): é«˜ç›¸ä¼¼åº¦å¼•ç”¨é—œä¿‚ (similarity >= 0.70)
  - `-.->`(è™›ç·š): ä¸­ç­‰ç›¸ä¼¼åº¦åƒè€ƒé—œä¿‚ (0.65-0.70)
- **é…ç½®**: Top-Downæ–¹å‘ï¼ˆ`graph TD`ï¼‰

```mermaid
graph TD
    P14["è«–æ–‡14: Neural Networks in AI"]
    P5["è«–æ–‡5: Deep Learning Fundamentals"]
    P23["è«–æ–‡23: Optimization Methods"]
    P8["è«–æ–‡8: Neural Architecture"]

    P14 --> P5
    P14 --> P8
    P5 -.-> P23
    P8 --> P23
```

### å…±åŒä½œè€…ç¶²çµ¡å¯è¦–åŒ–

```mermaid
graph TD
    subgraph Authors["å…±åŒä½œè€…ç¶²çµ¡"]
        A1["Smith, John (5ç¯‡)"]
        A2["Doe, Jane (3ç¯‡)"]
        A3["Brown, Bob (2ç¯‡)"]
    end

    A1 --> A2
    A1 --> A3
    A2 --> A3
```

### æ¦‚å¿µå…±ç¾ç¶²çµ¡å¯è¦–åŒ–

```mermaid
graph TD
    C1["æ·±åº¦å­¸ç¿’"]
    C2["ç¥ç¶“ç¶²çµ¡"]
    C3["å„ªåŒ–"]
    C4["åå‘å‚³æ’­"]

    C1 --> C2
    C1 --> C3
    C2 --> C4
    C3 -.-> C4
```

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ§‹

### ç›®éŒ„çµæ§‹

```
src/analyzers/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ relation_finder.py          # ä¸»æ¨¡çµ„ (~400è¡Œ)
â””â”€â”€ network_utils.py            # åœ–è«–å·¥å…·å‡½æ•¸ (~200è¡Œï¼Œå¯é¸)

.claude/skills/
â””â”€â”€ relation-finder.md          # Skillæ–‡æª”

tests/
â””â”€â”€ test_relation_finder.py     # æ¸¬è©¦å¥—ä»¶ (~400è¡Œ)

output/
â””â”€â”€ relations/                  # è¼¸å‡ºç›®éŒ„
    â”œâ”€â”€ citations.json
    â”œâ”€â”€ citations.mermaid.md
    â”œâ”€â”€ co_authors.json
    â”œâ”€â”€ co_authors.mermaid.md
    â”œâ”€â”€ concepts.json
    â”œâ”€â”€ concepts.mermaid.md
    â””â”€â”€ relations_report.html
```

### æ ¸å¿ƒé¡è¨­è¨ˆ

```python
# src/analyzers/relation_finder.py

from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class Citation:
    """å¼•ç”¨é—œä¿‚"""
    citing_paper_id: int
    cited_paper_id: int
    citing_title: str
    cited_title: str
    similarity_score: float
    confidence: str  # 'high'/'medium'/'low'
    common_concepts: List[str]

    def __repr__(self) -> str:
        return f"Citation({self.citing_paper_id} â†’ {self.cited_paper_id}, {self.similarity_score:.2f})"

@dataclass
class CoAuthorEdge:
    """å…±åŒä½œè€…é‚Š"""
    author1: str
    author2: str
    collaboration_count: int
    shared_papers: List[int]

class RelationFinder:
    """é—œä¿‚ç™¼ç¾ä¸»é¡"""

    def __init__(self, kb_manager, embedding_manager, config=None):
        self.kb = kb_manager
        self.embeddings = embedding_manager
        self.config = config or self._default_config()
        self._validate_dependencies()

    def _default_config(self) -> Dict:
        return {
            'citation_threshold': 0.65,
            'co_author_min_papers': 2,
            'concept_min_frequency': 2,
            'temporal_year_range': 5,
            'mermaid_format': 'graph TD',  # Mermaidæ ¼å¼
        }

    # 1. å¼•ç”¨é—œä¿‚æŠ½å–
    def find_citations(self, threshold=None, source_papers=None,
                      max_results=None) -> List[Citation]:
        """åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦æ¨æ¸¬å¼•ç”¨é—œä¿‚"""
        threshold = threshold or self.config['citation_threshold']

        # æ­¥é©Ÿ1: åŠ è¼‰è«–æ–‡å‘é‡
        papers = self.kb.get_all_papers()
        embeddings = self.embeddings.get_paper_embeddings([p['id'] for p in papers])

        # æ­¥é©Ÿ2: è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        similarity_matrix = cosine_similarity(embeddings)

        # æ­¥é©Ÿ3: æå–å¼•ç”¨é—œä¿‚
        citations = []
        for i, paper1 in enumerate(papers):
            for j, paper2 in enumerate(papers):
                if i != j:
                    sim_score = similarity_matrix[i][j]
                    if sim_score >= threshold:
                        citation = Citation(
                            citing_paper_id=paper1['id'],
                            cited_paper_id=paper2['id'],
                            citing_title=paper1['title'],
                            cited_title=paper2['title'],
                            similarity_score=sim_score,
                            confidence=self._get_confidence_level(sim_score),
                            common_concepts=self._extract_common_concepts(paper1, paper2),
                        )
                        citations.append(citation)

        # æ­¥é©Ÿ4: æ’åºå’Œéæ¿¾
        citations = sorted(citations, key=lambda x: x.similarity_score, reverse=True)
        if max_results:
            citations = citations[:max_results]

        return citations

    # 2. å…±åŒä½œè€…ç¶²çµ¡
    def find_co_authors(self, min_papers=None,
                       include_metadata=True) -> 'CoAuthorNetwork':
        """æ§‹å»ºä½œè€…å”ä½œç¶²çµ¡"""
        min_papers = min_papers or self.config['co_author_min_papers']

        # æ­¥é©Ÿ1: æå–æ‰€æœ‰ä½œè€…åŠå…¶è«–æ–‡
        author_papers = {}
        papers = self.kb.get_all_papers()

        for paper in papers:
            authors = self._parse_authors(paper.get('authors', ''))
            for author in authors:
                if author not in author_papers:
                    author_papers[author] = []
                author_papers[author].append(paper['id'])

        # æ­¥é©Ÿ2: è¨ˆç®—å”ä½œé—œä¿‚
        co_author_edges = []
        author_list = list(author_papers.keys())

        for i, author1 in enumerate(author_list):
            for author2 in author_list[i+1:]:
                shared_papers = set(author_papers[author1]) & set(author_papers[author2])
                if len(shared_papers) >= min_papers:
                    edge = CoAuthorEdge(
                        author1=author1,
                        author2=author2,
                        collaboration_count=len(shared_papers),
                        shared_papers=list(shared_papers),
                    )
                    co_author_edges.append(edge)

        # æ­¥é©Ÿ3: æ§‹å»ºç¶²çµ¡å°è±¡
        return CoAuthorNetwork(author_papers, co_author_edges)

    # 3. æ¦‚å¿µå…±ç¾åˆ†æ
    def find_co_occurrence(self, min_frequency=None,
                          top_k=None) -> 'ConceptCooccurrence':
        """åˆ†ææ¦‚å¿µå…±ç¾æ¨¡å¼"""
        min_frequency = min_frequency or self.config['concept_min_frequency']

        # æ­¥é©Ÿ1: æå–æ‰€æœ‰æ¦‚å¿µ
        concept_papers = {}
        papers = self.kb.get_all_papers()

        for paper in papers:
            concepts = self._extract_concepts(paper)
            for concept in concepts:
                if concept not in concept_papers:
                    concept_papers[concept] = []
                concept_papers[concept].append(paper['id'])

        # æ­¥é©Ÿ2: è¨ˆç®—å…±ç¾çŸ©é™£
        concept_list = list(concept_papers.keys())
        concept_pairs = []

        for i, concept1 in enumerate(concept_list):
            for concept2 in concept_list[i+1:]:
                shared_papers = set(concept_papers[concept1]) & set(concept_papers[concept2])
                if len(shared_papers) >= min_frequency:
                    pair = ConceptPair(
                        concept1=concept1,
                        concept2=concept2,
                        co_occurrence_count=len(shared_papers),
                        papers=list(shared_papers),
                        association_strength=len(shared_papers) / max(len(concept_papers[concept1]), len(concept_papers[concept2])),
                    )
                    concept_pairs.append(pair)

        # æ­¥é©Ÿ3: æ’åºå’Œé™åˆ¶
        concept_pairs = sorted(concept_pairs, key=lambda x: x.co_occurrence_count, reverse=True)
        if top_k:
            concept_pairs = concept_pairs[:top_k]

        return ConceptCooccurrence(concept_pairs, concept_papers)

    # 4. æ™‚é–“åºåˆ—åˆ†æ
    def build_timeline(self, start_year=None, end_year=None,
                      group_by='year') -> 'Timeline':
        """æ§‹å»ºæ™‚é–“ç·š"""
        papers = self.kb.get_all_papers()

        # æŒ‰å¹´ä»½åˆ†çµ„
        timeline_data = {}
        for paper in papers:
            year = paper.get('year')
            if year and 1900 <= year <= 2030:
                if year not in timeline_data:
                    timeline_data[year] = []
                timeline_data[year].append(paper)

        # æ§‹å»ºTimelineå°è±¡
        timepoints = []
        for year in sorted(timeline_data.keys()):
            timepoint = TimePoint(
                period=str(year),
                year=year,
                papers=[p['id'] for p in timeline_data[year]],
                paper_count=len(timeline_data[year]),
                top_concepts=self._get_top_concepts(timeline_data[year], k=5),
            )
            timepoints.append(timepoint)

        return Timeline(timepoints)

    # 5. Mermaidå°å‡º
    def export_to_mermaid(self, data, relation_type='citations',
                         output_path=None) -> str:
        """
        å°å‡ºç‚ºMermaidæ ¼å¼

        Args:
            data: Citations, CoAuthorNetwork, æˆ– ConceptCooccurrenceå°è±¡
            relation_type: 'citations'/'co_authors'/'concepts'
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            str: Mermaidä»£ç¢¼ï¼ˆå¦‚æœoutput_pathç‚ºNoneï¼‰æˆ–æª”æ¡ˆè·¯å¾‘
        """
        mermaid_code = "```mermaid\n"
        mermaid_code += "graph TD\n"

        if relation_type == 'citations':
            mermaid_code += self._citations_to_mermaid(data)
        elif relation_type == 'co_authors':
            mermaid_code += self._co_authors_to_mermaid(data)
        elif relation_type == 'concepts':
            mermaid_code += self._concepts_to_mermaid(data)

        mermaid_code += "```"

        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(mermaid_code)
            return output_path

        return mermaid_code

    def _citations_to_mermaid(self, citations: List[Citation]) -> str:
        """å°‡å¼•ç”¨è½‰æ›ç‚ºMermaid"""
        lines = []
        seen_nodes = set()

        for citation in citations:
            # æ·»åŠ ç¯€é»
            if citation.citing_paper_id not in seen_nodes:
                lines.append(f'    P{citation.citing_paper_id}["{citation.citing_title}"]')
                seen_nodes.add(citation.citing_paper_id)

            if citation.cited_paper_id not in seen_nodes:
                lines.append(f'    P{citation.cited_paper_id}["{citation.cited_title}"]')
                seen_nodes.add(citation.cited_paper_id)

            # æ·»åŠ é‚Šï¼ˆæ ¹æ“šconfidenceæ±ºå®šç·šå‹ï¼‰
            if citation.confidence == 'high':
                lines.append(f'    P{citation.citing_paper_id} --> P{citation.cited_paper_id}')
            else:
                lines.append(f'    P{citation.citing_paper_id} -.-> P{citation.cited_paper_id}')

        return '\n'.join(lines)

    def _co_authors_to_mermaid(self, network: 'CoAuthorNetwork') -> str:
        """å°‡å…±åŒä½œè€…ç¶²çµ¡è½‰æ›ç‚ºMermaid"""
        lines = []

        # æ·»åŠ subgraph
        lines.append('    subgraph Authors["å…±åŒä½œè€…ç¶²çµ¡"]')
        for author, paper_count in list(network.author_frequency.items())[:20]:
            lines.append(f'        A{hash(author)%10000}["{author} ({paper_count}ç¯‡)"]')
        lines.append('    end')

        # æ·»åŠ é‚Š
        for edge in network.edges[:50]:  # é™åˆ¶50æ¢é‚Šä»¥é¿å…éåº¦è¤‡é›œ
            author1_id = hash(edge.author1) % 10000
            author2_id = hash(edge.author2) % 10000
            lines.append(f'    A{author1_id} --> A{author2_id}')

        return '\n'.join(lines)

    def _concepts_to_mermaid(self, cooccurrence: 'ConceptCooccurrence') -> str:
        """å°‡æ¦‚å¿µå…±ç¾è½‰æ›ç‚ºMermaid"""
        lines = []
        seen_concepts = set()

        for pair in cooccurrence.pairs[:30]:  # é™åˆ¶30å€‹æ¦‚å¿µå°
            if pair.concept1 not in seen_concepts:
                lines.append(f'    C{hash(pair.concept1)%10000}["{pair.concept1}"]')
                seen_concepts.add(pair.concept1)

            if pair.concept2 not in seen_concepts:
                lines.append(f'    C{hash(pair.concept2)%10000}["{pair.concept2}"]')
                seen_concepts.add(pair.concept2)

            # æ ¹æ“šé—œè¯å¼·åº¦æ±ºå®šé‚Šå‹
            if pair.association_strength >= 0.5:
                lines.append(f'    C{hash(pair.concept1)%10000} --> C{hash(pair.concept2)%10000}')
            else:
                lines.append(f'    C{hash(pair.concept1)%10000} -.-> C{hash(pair.concept2)%10000}')

        return '\n'.join(lines)

    # å°å‡ºç‚ºJSON
    def export_to_json(self, output_path: str, include=None) -> str:
        """å°å‡ºç‚ºJSONæ ¼å¼"""
        # TODO: å¯¦ä½œ
        pass

    # è¼”åŠ©æ–¹æ³•
    def _validate_dependencies(self):
        """é©—è­‰ä¾è³´"""
        if not self.kb:
            raise ValueError("KnowledgeBaseManager instance required")
        if not self.embeddings:
            raise ValueError("EmbeddingManager instance required")

    def _get_confidence_level(self, similarity_score: float) -> str:
        """æ ¹æ“šç›¸ä¼¼åº¦ç¢ºå®šç½®ä¿¡åº¦"""
        if similarity_score >= 0.80:
            return 'high'
        elif similarity_score >= 0.70:
            return 'medium'
        else:
            return 'low'

    def _extract_common_concepts(self, paper1: Dict, paper2: Dict) -> List[str]:
        """æå–å…±åŒæ¦‚å¿µ"""
        concepts1 = set(self._extract_concepts(paper1))
        concepts2 = set(self._extract_concepts(paper2))
        return list(concepts1 & concepts2)

    def _extract_concepts(self, paper: Dict) -> List[str]:
        """å¾è«–æ–‡æå–æ¦‚å¿µ"""
        keywords = paper.get('keywords', '')
        if isinstance(keywords, str):
            return [k.strip() for k in keywords.split(',') if k.strip()]
        return keywords

    def _parse_authors(self, authors_str: str) -> List[str]:
        """è§£æä½œè€…å­—ç¬¦ä¸²"""
        if not authors_str:
            return []

        # æ”¯æ´å¤šç¨®æ ¼å¼: "Author1, Author2" æˆ– "Author1 and Author2"
        authors = authors_str.replace(' and ', ',').split(',')
        return [a.strip() for a in authors if a.strip()]

    def _get_top_concepts(self, papers: List[Dict], k: int = 5) -> List[str]:
        """å¾è«–æ–‡åˆ—è¡¨ä¸­ç²å–top-kæ¦‚å¿µ"""
        concept_freq = {}
        for paper in papers:
            concepts = self._extract_concepts(paper)
            for concept in concepts:
                concept_freq[concept] = concept_freq.get(concept, 0) + 1

        return [c[0] for c in sorted(concept_freq.items(), key=lambda x: x[1], reverse=True)[:k]]

    def get_statistics(self) -> Dict:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        # TODO: å¯¦ä½œ
        pass


# æ”¯æŒé¡
@dataclass
class ConceptPair:
    concept1: str
    concept2: str
    co_occurrence_count: int
    papers: List[int]
    association_strength: float

class ConceptCooccurrence:
    def __init__(self, pairs: List[ConceptPair], concept_papers: Dict[str, List[int]]):
        self.pairs = pairs
        self.concept_frequency = {c: len(ps) for c, ps in concept_papers.items()}

@dataclass
class TimePoint:
    period: str
    year: int
    papers: List[int]
    paper_count: int
    top_concepts: List[str]

class Timeline:
    def __init__(self, timepoints: List[TimePoint]):
        self.timepoints = timepoints

class CoAuthorNetwork:
    def __init__(self, author_papers: Dict[str, List[int]], edges: List[CoAuthorEdge]):
        self.author_papers = author_papers
        self.edges = edges
        self.author_frequency = {a: len(ps) for a, ps in author_papers.items()}
```

---

## ğŸ§ª Day 1 å¯¦ä½œè¨ˆç•«ï¼ˆ4å°æ™‚ï¼‰

### ä»»å‹™ 1: é …ç›®çµæ§‹å’Œé¡å®šç¾©ï¼ˆ1å°æ™‚ï¼‰
- [x] å‰µå»º `src/analyzers/` ç›®éŒ„
- [x] å‰µå»º `__init__.py` æª”æ¡ˆ
- [x] å®šç¾©æ‰€æœ‰dataclassï¼ˆCitation, CoAuthorEdge, ConceptPairç­‰ï¼‰
- [x] å®šç¾©ä¸»é¡RelationFinderçš„éª¨æ¶

### ä»»å‹™ 2: å¼•ç”¨é—œä¿‚æŠ½å–ï¼ˆ2å°æ™‚ï¼‰
- [ ] å¯¦ä½œ `find_citations()` æ–¹æ³•
- [ ] å¯¦ä½œ `_get_confidence_level()` æ–¹æ³•
- [ ] å¯¦ä½œ `_extract_common_concepts()` æ–¹æ³•
- [ ] ç·¨å¯«åŸºæœ¬å–®å…ƒæ¸¬è©¦

### ä»»å‹™ 3: Mermaidå°å‡ºï¼ˆ1å°æ™‚ï¼‰
- [ ] å¯¦ä½œ `export_to_mermaid()` æ–¹æ³•
- [ ] å¯¦ä½œ `_citations_to_mermaid()` æ–¹æ³•
- [ ] æ¸¬è©¦Mermaidè¼¸å‡ºæ ¼å¼

---

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™ | Day 1 | Day 2-3 | Day 4 |
|------|------|-------|---------|-------|
| å¼•ç”¨é—œä¿‚ | >50å€‹ | âœ…æ‰¾åˆ° | å¢å¼· | é©—è­‰ |
| å…±åŒä½œè€… | >20å° | - | âœ…å¯¦ä½œ | é©—è­‰ |
| æ¦‚å¿µå° | >20å€‹ | - | âœ…å¯¦ä½œ | é©—è­‰ |
| Mermaidæ ¼å¼ | âœ…ç¬¦åˆæ¨™æº– | âœ…å¯¦ä½œ | è¤‡ç”¨ | é©—è­‰ |
| ä»£ç¢¼è¦†è“‹ | >80% | 50% | 70% | 80%+ |
| æ¸¬è©¦é€šé | 100% | 60% | 80% | 100% |

