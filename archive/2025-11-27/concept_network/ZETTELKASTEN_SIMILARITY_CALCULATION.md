# Zettelkasten åŸå­å¡ç‰‡æ¦‚å¿µç›¸ä¼¼æ€§è¨ˆç®—æŒ‡å—

**æ–‡ä»¶ç‰ˆæœ¬**: 1.0
**ç”Ÿæˆæ—¥æœŸ**: 2025-11-05
**é©ç”¨æ¨¡çµ„**: relation_finder.py (Phase 2.1) å’Œ concept_mapper.py (Phase 2.2)

---

## ğŸ“‹ ç›®éŒ„

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [relation_finder çš„ç›¸ä¼¼æ€§è¨ˆç®—](#relation_finder-çš„ç›¸ä¼¼æ€§è¨ˆç®—)
3. [concept_mapper çš„ç›¸ä¼¼æ€§æ‡‰ç”¨](#concept_mapper-çš„ç›¸ä¼¼æ€§æ‡‰ç”¨)
4. [åŸå­å¡ç‰‡å…ƒç´ è©³è§£](#åŸå­å¡ç‰‡å…ƒç´ è©³è§£)
5. [è¨ˆç®—æµç¨‹åœ–](#è¨ˆç®—æµç¨‹åœ–)
6. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
7. [æ€§èƒ½å’Œå„ªåŒ–](#æ€§èƒ½å’Œå„ªåŒ–)

---

## æ¦‚è¿°

Zettelkastenï¼ˆå¡ç‰‡ç›’ç­†è¨˜æ³•ï¼‰ç³»çµ±ä¸­ï¼ŒåŸå­å¡ç‰‡ä¹‹é–“çš„æ¦‚å¿µç›¸ä¼¼æ€§è¨ˆç®—æ˜¯æ§‹å»ºçŸ¥è­˜ç¶²çµ¡çš„æ ¸å¿ƒã€‚æœ¬ç³»çµ±æ¡ç”¨**å‘é‡ç›¸ä¼¼åº¦ + å¤šç¶­åº¦ä¿¡åº¦è©•åˆ†**çš„æ··åˆæ–¹æ³•ã€‚

### æ ¸å¿ƒç‰¹å¾µ

| ç‰¹å¾µ | èªªæ˜ |
|------|------|
| **ç›¸ä¼¼åº¦è¨ˆç®—æ–¹å¼** | åŸºæ–¼å‘é‡åµŒå…¥ï¼ˆGemini/Ollamaï¼‰çš„ä½™å¼¦ç›¸ä¼¼åº¦ |
| **è©•åˆ†ç¶­åº¦** | 4å€‹ç¶­åº¦åŠ æ¬Šï¼ˆèªç¾© 40% + é€£çµ 30% + å…±åŒæ¦‚å¿µ 20% + é ˜åŸŸ 10%ï¼‰ |
| **é—œä¿‚é¡å‹è­˜åˆ¥** | 6ç¨®æœ‰å‘é—œä¿‚é¡å‹ |
| **ä¸»è¦ä¾è³´** | ChromaDB å‘é‡æ•¸æ“šåº« + SQLite å…ƒæ•¸æ“š |
| **æœ€å°é–¾å€¼** | èªç¾©ç›¸ä¼¼åº¦ 0.4ï¼Œä¿¡åº¦è©•åˆ† 0.3 |

---

## relation_finder çš„ç›¸ä¼¼æ€§è¨ˆç®—

### 1. å‘é‡ç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰

#### ä»£ç¢¼ä½ç½®
`src/analyzers/relation_finder.py:470-504` (`find_concept_relations` æ–¹æ³•)

#### è¨ˆç®—æµç¨‹

```python
# æ­¥é©Ÿ 1ï¼šä½¿ç”¨å‘é‡æœç´¢æ‰¾ç›¸ä¼¼å¡ç‰‡
similar_results = self.vector_db.find_similar_zettel(
    zettel_id=card_id,
    n_results=min(limit, len(cards) - 1),
    exclude_self=True
)

# æ­¥é©Ÿ 2ï¼šå¾è·é›¢è¨ˆç®—ç›¸ä¼¼åº¦
# ChromaDB è¿”å›è·é›¢ï¼ˆdistanceï¼‰ï¼Œç›¸ä¼¼åº¦ = 1 - distance
similarity = 1.0 - similar_results['distances'][0][j]
```

#### ç›¸ä¼¼åº¦ä¾†æº

| ä¾†æº | èªªæ˜ | ç¶­åº¦ | æ¨¡å‹ |
|------|------|------|------|
| **Gemini åµŒå…¥** | Google Gemini Embedding-001 | 768 | embedding-001 |
| **Ollama åµŒå…¥** | æœ¬åœ° Qwen3-Embedding-4B | 2560 | qwen3-embedding:4b |

#### ç›¸ä¼¼åº¦ç¯„åœè§£é‡‹

| ç¯„åœ | è§£é‡‹ | é—œä¿‚é¡å‹ |
|------|------|----------|
| 0.9-1.0 | éå¸¸ç›¸ä¼¼ï¼Œå¹¾ä¹ç›¸åŒ | å¯èƒ½æ˜¯é‡è¤‡å¡ç‰‡ |
| 0.7-0.9 | é«˜åº¦ç›¸ä¼¼ï¼Œæ ¸å¿ƒæ¦‚å¿µç›¸åŒ | related_to / superclass_of |
| 0.5-0.7 | ä¸­ç­‰ç›¸ä¼¼ï¼Œæœ‰ç›¸é—œæ¦‚å¿µ | related_to / leads_to |
| 0.4-0.5 | ä½ç›¸ä¼¼ï¼Œæœ‰é–“æ¥é—œè¯ | related_toï¼ˆé‚Šç•Œï¼‰ |
| <0.4 | ä¸ç›¸ä¼¼ï¼Œè‡ªå‹•éæ¿¾ | ï¼ˆè¢«æ’é™¤ï¼‰ |

### 2. å¤šç¶­åº¦ä¿¡åº¦è©•åˆ†ç³»çµ±

#### ä»£ç¢¼ä½ç½®
`src/analyzers/relation_finder.py:625-672` (`_calculate_confidence` æ–¹æ³•)

#### è©•åˆ†å…¬å¼

```
ç¸½ä¿¡åº¦åˆ†æ•¸ = Î£(å„ç¶­åº¦åˆ†æ•¸ Ã— æ¬Šé‡)

ä¿¡åº¦ = semantic_similarityÃ—0.4 + link_explicitÃ—0.3 + co_occurrenceÃ—0.2 + domain_consistencyÃ—0.1
```

#### ç¶­åº¦è©³è§£

##### ç¶­åº¦ 1ï¼šèªç¾©ç›¸ä¼¼åº¦ (40%)

**è¨ˆç®—æ–¹å¼**:
```python
semantic_score = similarity * 0.4  # similarity ä¾†è‡ªå‘é‡ç›¸ä¼¼åº¦
```

**ç‰¹é»**:
- ç›´æ¥ä½¿ç”¨å‘é‡æ¨¡å‹çš„ç›¸ä¼¼åº¦çµæœ
- æ¬Šé‡æœ€é«˜ï¼ˆ40%ï¼‰ï¼Œå› ç‚ºå‘é‡è¡¨ç¤ºæ¶µè“‹äº†å®Œæ•´çš„èªç¾©ä¿¡æ¯
- ç¯„åœï¼š0.0-0.4ï¼ˆæœ€å¤šè²¢ç» 40% åˆ°ç¸½ä¿¡åº¦ï¼‰

**ä¾‹å­**:
- å‘é‡ç›¸ä¼¼åº¦ 0.8 â†’ èªç¾©åˆ†æ•¸ = 0.8 Ã— 0.4 = 0.32

##### ç¶­åº¦ 2ï¼šæ˜ç¢ºé€£çµ (30%)

**è¨ˆç®—æ–¹å¼**:
```python
has_explicit_link = self._check_explicit_link(card1, card2_id)
link_score = 0.3 if has_explicit_link else 0.0
```

**é€£çµæª¢æŸ¥é‚è¼¯** (`_check_explicit_link` æ–¹æ³•, 674-695 è¡Œ):
```python
# å„ªå…ˆä½¿ç”¨ ai_notesï¼ˆå·²æ·¨åŒ–çš„ AI å…§å®¹ï¼‰
ai_notes = card.get('ai_notes')
if ai_notes:
    ai_content = ai_notes
else:
    # Fallbackï¼šå¾ content æå– AI å…§å®¹ï¼ˆéæ¿¾äººé¡ç­†è¨˜ï¼‰
    content = card.get('content', '')
    ai_content = extract_ai_content(content)

# æª¢æŸ¥ Obsidian æ ¼å¼çš„é€£çµ: [[target_id]]
return f'[[{target_id}]]' in ai_content
```

**é€£çµæ ¼å¼**:
- Obsidian Wiki Links: `[[CogSci-20251028-001]]`
- ä¸æ”¯æ´å…¶ä»–é€£çµæ ¼å¼ï¼ˆå¦‚ `->` æˆ– `--leads_to-->`ï¼‰

**ç‰¹é»**:
- åªæœ‰åœ¨å¡ç‰‡ä¸­å­˜åœ¨**æ˜ç¢ºé€£çµ**æ™‚æ‰ç²å¾—æ»¿åˆ†
- æ¬Šé‡æ¬¡é«˜ï¼ˆ30%ï¼‰ï¼Œåæ˜ äººé¡æ¨™è¨»çš„é‡è¦æ€§
- ç¯„åœï¼š0.0 æˆ– 0.3ï¼ˆäºŒå€¼é¸æ“‡ï¼‰

**ä¾‹å­**:
- Card A çš„ AI ç­†è¨˜ä¸­åŒ…å« `[[Card-B-ID]]` â†’ é€£çµåˆ†æ•¸ = 0.3
- ç„¡é€£çµ â†’ é€£çµåˆ†æ•¸ = 0.0

##### ç¶­åº¦ 3ï¼šå…±åŒæ¦‚å¿µ (20%)

**è¨ˆç®—æ–¹å¼**:
```python
shared_concepts = self._extract_shared_concepts_from_cards(card1, card2)
# æ­£è¦åŒ–ï¼š5 å€‹ä»¥ä¸Šå…±åŒæ¦‚å¿µå¾—æ»¿åˆ†
shared_score = min(len(shared) / 5.0, 1.0) * 0.2
```

**å…±åŒæ¦‚å¿µæå–** (`_extract_shared_concepts_from_cards` æ–¹æ³•, 697-753 è¡Œ):

æå–ä¾†æºï¼š
1. **æ¨™ç±¤ (tags)**
   - ä¾†è‡ªè³‡æ–™åº«çš„ `tags` æ¬„ä½
   - æ ¼å¼ï¼šJSON é™£åˆ— `["tag1", "tag2", ...]`

2. **æ ¸å¿ƒæ¦‚å¿µ (core_concept)**
   - ä¾†è‡ªè³‡æ–™åº«çš„ `core_concept` æ¬„ä½
   - æå–éç¨‹ï¼š
     - æ­£å‰‡è¡¨é”å¼åˆ†è©ï¼š`re.findall(r'\w+', core.lower())`
     - éæ¿¾åœç”¨è©ï¼šåªä¿ç•™é•·åº¦ â‰¥ 3 çš„è©
   - ä¾‹å­ï¼šã€ŒèªçŸ¥ç§‘å­¸ä¸­çš„è¦–è¦ºè™•ç†ã€â†’ `['èªçŸ¥', 'ç§‘å­¸', 'ä¸­çš„', 'è¦–è¦º', 'è™•ç†']` â†’ `['èªçŸ¥', 'ç§‘å­¸', 'è¦–è¦º', 'è™•ç†']`

3. **æ¨™é¡Œ (title)**
   - ä¾†è‡ªè³‡æ–™åº«çš„ `title` æ¬„ä½
   - æå–æ–¹å¼åŒæ ¸å¿ƒæ¦‚å¿µ
   - ä¾‹å­ï¼šã€Œè¦–è¦ºå­—ç¬¦è­˜åˆ¥çš„ç¥ç¶“æ©Ÿåˆ¶ã€â†’ `['è¦–è¦º', 'å­—ç¬¦', 'è­˜åˆ¥', 'ç¥ç¶“', 'æ©Ÿåˆ¶']`

**å…±åŒæ¦‚å¿µè¨ˆç®—**:
```python
concepts1 = extract_concepts(card1)  # Set of keywords
concepts2 = extract_concepts(card2)  # Set of keywords
shared = concepts1 & concepts2       # äº¤é›†
```

**æ­£è¦åŒ–è¦å‰‡**:
```
shared_score = min(len(shared) / 5.0, 1.0) * 0.2

ç¯„ä¾‹ï¼š
- 0 å€‹å…±åŒæ¦‚å¿µ â†’ shared_score = 0.0
- 1-2 å€‹å…±åŒæ¦‚å¿µ â†’ shared_score = 0.04-0.08
- 3-4 å€‹å…±åŒæ¦‚å¿µ â†’ shared_score = 0.12-0.16
- 5 å€‹æˆ–ä»¥ä¸Š â†’ shared_score = 0.2ï¼ˆæ»¿åˆ†ï¼‰
```

**ç‰¹é»**:
- æ¬Šé‡é©ä¸­ï¼ˆ20%ï¼‰ï¼Œåæ˜ æ¦‚å¿µé‡ç–Šç¨‹åº¦
- ç¯„åœï¼š0.0-0.2ï¼ˆæ¼¸é€²å¼è©•åˆ†ï¼ŒéäºŒå€¼ï¼‰
- æ›´å¤šå…±åŒæ¦‚å¿µ = æ›´é«˜ä¿¡åº¦

**ä¾‹å­**:
```
Card A: æ¨™ç±¤=[æ·±åº¦å­¸ç¿’, ç¥ç¶“ç¶²çµ¡], æ ¸å¿ƒæ¦‚å¿µ="å·ç©ç¥ç¶“ç¶²çµ¡"
Card B: æ¨™ç±¤=[ç¥ç¶“ç¶²çµ¡, åœ–åƒè™•ç†], æ ¸å¿ƒæ¦‚å¿µ="åœ–åƒåˆ†é¡"

æå–çš„å…±åŒæ¦‚å¿µï¼š["ç¥ç¶“ç¶²çµ¡"]  â†’ 1 å€‹
shared_score = (1 / 5.0) * 0.2 = 0.04
```

##### ç¶­åº¦ 4ï¼šé ˜åŸŸä¸€è‡´æ€§ (10%)

**è¨ˆç®—æ–¹å¼**:
```python
domain1 = card1.get('domain', '')
domain2 = card2.get('domain', '')
domain_consistent = (domain1 == domain2) if domain1 and domain2 else False
domain_score = 0.1 if domain_consistent else 0.05
```

**é ˜åŸŸå€¼ä¾†æº**:
- è³‡æ–™åº«çš„ `domain` æ¬„ä½
- ä¾‹å­ï¼š`CogSci`ã€`Linguistics`ã€`AI`ã€`Research`

**è¨ˆç®—è¦å‰‡**:
```
IF domain1 == domain2 AND éƒ½éç©º:
    score = 0.1  (åŒé ˜åŸŸï¼Œæ»¿åˆ†)
ELSE:
    score = 0.05 (ä¸åŒé ˜åŸŸæˆ–ç¼ºå¤±ï¼ŒåŠåˆ†)
```

**ç‰¹é»**:
- æ¬Šé‡æœ€ä½ï¼ˆ10%ï¼‰ï¼Œä½œç‚ºè£œå……æŒ‡æ¨™
- ç¯„åœï¼š0.05 æˆ– 0.1ï¼ˆäºŒå€¼ï¼‰
- ä¿ƒé€²åŒé ˜åŸŸå¡ç‰‡é—œè¯

**ä¾‹å­**:
```
Card A: domain = "CogSci"
Card B: domain = "CogSci"
â†’ domain_score = 0.1

Card A: domain = "CogSci"
Card B: domain = "AI"
â†’ domain_score = 0.05
```

#### ä¿¡åº¦è©•åˆ†ç¶œåˆç¤ºä¾‹

**æ¡ˆä¾‹ 1ï¼šé«˜ä¿¡åº¦ï¼ˆå„ªè³ªç›¸é—œï¼‰**
```
Card A: "è¦–è¦ºè™•ç†çš„ç¥ç¶“æ©Ÿåˆ¶"
Card B: "è¦–è¦ºçš®å±¤çš„æ¿€æ´»æ¨¡å¼"

è¨ˆç®—:
- èªç¾©ç›¸ä¼¼åº¦: 0.75 â†’ semantic_score = 0.75 Ã— 0.4 = 0.30
- æ˜ç¢ºé€£çµ: æœ‰ â†’ link_score = 0.30
- å…±åŒæ¦‚å¿µ: ["è¦–è¦º", "ç¥ç¶“"] (2å€‹) â†’ shared_score = (2/5) Ã— 0.2 = 0.08
- é ˜åŸŸä¸€è‡´æ€§: CogSci = CogSci â†’ domain_score = 0.10

ç¸½ä¿¡åº¦ = 0.30 + 0.30 + 0.08 + 0.10 = 0.78 âœ… é«˜ä¿¡åº¦
```

**æ¡ˆä¾‹ 2ï¼šä¸­ç­‰ä¿¡åº¦ï¼ˆéœ€é©—è­‰ï¼‰**
```
Card A: "æ©Ÿå™¨å­¸ç¿’åŸºç¤"
Card B: "æ·±åº¦å­¸ç¿’æ‡‰ç”¨"

è¨ˆç®—:
- èªç¾©ç›¸ä¼¼åº¦: 0.55 â†’ semantic_score = 0.55 Ã— 0.4 = 0.22
- æ˜ç¢ºé€£çµ: ç„¡ â†’ link_score = 0.0
- å…±åŒæ¦‚å¿µ: ["å­¸ç¿’", "æ·±åº¦"] (2å€‹) â†’ shared_score = (2/5) Ã— 0.2 = 0.08
- é ˜åŸŸä¸€è‡´æ€§: AI = AI â†’ domain_score = 0.10

ç¸½ä¿¡åº¦ = 0.22 + 0.0 + 0.08 + 0.10 = 0.40 âš ï¸ é‚Šç•Œä¿¡åº¦
```

**æ¡ˆä¾‹ 3ï¼šä½ä¿¡åº¦ï¼ˆå¼±é—œè¯ï¼‰**
```
Card A: "èªè¨€èªæ³•çµæ§‹"
Card B: "éŸ³æ¨‚ç¯€å¥æ¨¡å¼"

è¨ˆç®—:
- èªç¾©ç›¸ä¼¼åº¦: 0.42 â†’ semantic_score = 0.42 Ã— 0.4 = 0.168
- æ˜ç¢ºé€£çµ: ç„¡ â†’ link_score = 0.0
- å…±åŒæ¦‚å¿µ: [] (0å€‹) â†’ shared_score = 0.0
- é ˜åŸŸä¸€è‡´æ€§: Linguistics â‰  Music â†’ domain_score = 0.05

ç¸½ä¿¡åº¦ = 0.168 + 0.0 + 0.0 + 0.05 = 0.218 âŒ ä½ä¿¡åº¦ï¼ˆå¯èƒ½è¢«éæ¿¾ï¼‰
```

### 3. é—œä¿‚é¡å‹åˆ†é¡

#### ä»£ç¢¼ä½ç½®
`src/analyzers/relation_finder.py:562-623` (`_classify_relation_type` æ–¹æ³•)

#### å…­ç¨®é—œä¿‚é¡å‹

| é—œä¿‚é¡å‹ | ç¬¦è™Ÿ | èªªæ˜ | åˆ¤å®šæ¢ä»¶ |
|---------|------|------|---------|
| **leads_to** | A â†’ B | å°å‘/æ¨å° | å¡ç‰‡ A å°å‘å¡ç‰‡ B çš„æ¦‚å¿µç™¼å±• |
| **based_on** | A â† B | åŸºæ–¼/ä¾è³´ | å¡ç‰‡ A åŸºæ–¼å¡ç‰‡ B çš„æ¦‚å¿µ |
| **related_to** | A â†” B | ç›¸é—œ/ç›¸ä¼¼ | å…©å¼µå¡ç‰‡æ¦‚å¿µç›¸é—œä½†ç„¡æ˜ç¢ºæ–¹å‘ |
| **contrasts_with** | A âŠ— B | å°æ¯”/å°ç«‹ | å…©å¼µå¡ç‰‡æ¦‚å¿µå°æ¯”æˆ–ç›¸å |
| **superclass_of** | A âŠƒ B | ä¸Šä½æ¦‚å¿µ | A æ˜¯ B çš„æ›´ä¸€èˆ¬/æŠ½è±¡æ¦‚å¿µ |
| **subclass_of** | A âŠ‚ B | ä¸‹ä½æ¦‚å¿µ | A æ˜¯ B çš„æ›´å…·é«”/ç‰¹ä¾‹æ¦‚å¿µ |

#### åˆ¤å®šé‚è¼¯

**å„ªå…ˆé †åº** (å¾é«˜åˆ°ä½):

1. **æª¢æŸ¥æ˜ç¢ºé€£çµ** (æœ€å¯é )
   ```python
   if f'[[{card2_id}]]' in card1.get('content', ''):
       # æª¢æŸ¥é€£çµå‘¨åœçš„ä¸Šä¸‹æ–‡é—œéµè©
       if '-->' in content or 'å°å‘' in content or 'leads to' in content:
           return 'leads_to'
       elif '<--' in content or 'åŸºæ–¼' in content or 'based on' in content:
           return 'based_on'
   ```
   - æ ¼å¼ï¼š`[[Card-ID-123]]`
   - éœ€è¦æª¢æŸ¥å‘¨åœçš„æ–¹å‘é—œéµè©

2. **æª¢æŸ¥å°æ¯”é—œéµè©**
   ```python
   contrast_keywords = ['ä½†', 'ç„¶è€Œ', 'ç›¸å', 'å°æ¯”', 'however', 'but', 'contrast', 'differ']
   if any(kw in content1 or kw in content2 for kw in contrast_keywords):
       return 'contrasts_with'
   ```

3. **æª¢æŸ¥ä¸Šä¸‹ä½é—œä¿‚é—œéµè©**
   ```python
   superclass_keywords = ['åŒ…å«', 'æŠ½è±¡', 'æ³›æŒ‡', 'include', 'general', 'abstract', 'superclass']
   subclass_keywords = ['å…·é«”', 'ç‰¹ä¾‹', 'å¯¦ä¾‹', 'specific', 'instance', 'example', 'subclass']

   if any(kw in content1 for kw in superclass_keywords):
       return 'superclass_of'
   if any(kw in content1 for kw in subclass_keywords):
       return 'subclass_of'
   ```

4. **åŸºæ–¼ç›¸ä¼¼åº¦åˆ¤å®š** (å‚™é¸)
   ```python
   if similarity >= 0.7:
       return 'related_to'
   elif similarity >= 0.5:
       # æª¢æŸ¥æ–¹å‘æ€§é—œéµè©
       directional_keywords = ['å› æ­¤', 'æ‰€ä»¥', 'å°è‡´', 'therefore', 'thus', 'result']
       if any(kw in content1 for kw in directional_keywords):
           return 'leads_to'
       return 'related_to'
   else:
       return 'related_to'  # é è¨­
   ```

#### é—œä¿‚åˆ¤å®šç¤ºä¾‹

**ç¤ºä¾‹ 1ï¼šæ˜ç¢ºå°å‘é—œä¿‚**
```
Card A (AI-20251028-001): æ·±åº¦å­¸ç¿’åŸºç¤
å…§å®¹: "[[AI-20251028-002]] å°å‘æ›´è¤‡é›œçš„...", ç›¸ä¼¼åº¦=0.72

åˆ¤å®š: leads_to
ç†ç”±: (1) æ‰¾åˆ°æ˜ç¢ºé€£çµ [[...]], (2) å…§å®¹åŒ…å«ã€Œå°å‘ã€é—œéµè©
```

**ç¤ºä¾‹ 2ï¼šå°æ¯”é—œä¿‚**
```
Card A: å¤å…¸æ©Ÿå™¨å­¸ç¿’
Card B: æ·±åº¦å­¸ç¿’
å…±åŒå…§å®¹: "...ä½†æ·±åº¦å­¸ç¿’ç›¸æ¯”å‚³çµ±æ©Ÿå™¨å­¸ç¿’...", ç›¸ä¼¼åº¦=0.68

åˆ¤å®š: contrasts_with
ç†ç”±: (1) å…§å®¹åŒ…å«ã€Œä½†ã€é—œéµè©, (2) é‚è¼¯ä¸Šå°æ¯”
```

**ç¤ºä¾‹ 3ï¼šä¸Šä½æ¦‚å¿µ**
```
Card A: æ©Ÿå™¨å­¸ç¿’ï¼ˆåŒ…å«å„ç¨®ç®—æ³•ï¼‰
Card B: ç¥ç¶“ç¶²çµ¡
å…§å®¹: "æ©Ÿå™¨å­¸ç¿’åŒ…å«ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’...", ç›¸ä¼¼åº¦=0.65

åˆ¤å®š: superclass_of
ç†ç”±: (1) A å…§å®¹åŒ…å«ã€ŒåŒ…å«ã€é—œéµè©, (2) æ¦‚å¿µå±¤ç´šæ˜ç¢º
```

**ç¤ºä¾‹ 4ï¼šç´”ç›¸ä¼¼åº¦åˆ¤å®š**
```
Card A: CNN æ¶æ§‹
Card B: RNN æ¶æ§‹
ç„¡æ˜ç¢ºé€£çµã€ç„¡ç‰¹æ®Šé—œéµè©, ç›¸ä¼¼åº¦=0.58

åˆ¤å®š: related_to
ç†ç”±: (1) ç„¡æ˜ç¢ºé€£çµ, (2) ç„¡å°æ¯”/ä¸Šä¸‹ä½é—œéµè©, (3) ç›¸ä¼¼åº¦ 0.5-0.7 ç¯„åœ
```

---

## concept_mapper çš„ç›¸ä¼¼æ€§æ‡‰ç”¨

concept_mapper æ¨¡çµ„åŸºæ–¼ relation_finder è¨ˆç®—çš„ç›¸ä¼¼æ€§å’Œä¿¡åº¦ï¼Œé€²è¡Œé«˜ç´šç¶²çµ¡åˆ†æã€‚

### 1. ä¸­å¿ƒæ€§åˆ†æï¼ˆè­˜åˆ¥é—œéµæ¦‚å¿µï¼‰

#### ä»£ç¢¼ä½ç½®
`src/analyzers/concept_mapper.py:413-593` (`CentralityAnalyzer` é¡)

#### PageRank è¨ˆç®—

**åŸç†**ï¼šåŸºæ–¼æœ‰å‘åœ–çš„è¿­ä»£ç®—æ³•ï¼Œè­˜åˆ¥æ•´é«”å½±éŸ¿åŠ›æœ€å¤§çš„ç¯€é»ã€‚

**è¨ˆç®—å…¬å¼**:
```
PR(A) = (1-d)/N + d Ã— Î£(PR(B)/|Bçš„å‡ºé‚Šæ•¸|)
        å…¶ä¸­ï¼šd = damping factor (0.85)
             N = ç¸½ç¯€é»æ•¸
             B = æŒ‡å‘ A çš„ç¯€é»
```

**å¯¦ç¾ç´°ç¯€**ï¼ˆç¬¬ 488-527 è¡Œï¼‰:
```python
def _calculate_pagerank(
    self,
    damping: float = 0.85,      # é˜»å°¼ä¿‚æ•¸
    max_iterations: int = 100,   # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    tolerance: float = 1e-6      # æ”¶æ–‚é–¾å€¼
) -> Dict[str, float]:
    nodes = list(self.network.node_dict.keys())
    n = len(nodes)

    # åˆå§‹åŒ–ï¼šæ‰€æœ‰ç¯€é» PageRank = 1/N
    ranks = {node: 1.0 / n for node in nodes}

    # è¿­ä»£ç›´åˆ°æ”¶æ–‚
    for iteration in range(max_iterations):
        new_ranks = {}
        max_diff = 0.0

        for node in nodes:
            rank_sum = 0.0

            # è¨ˆç®—ä¾†è‡ªé„°å±…çš„è²¢ç»
            for neighbor in self.network.get_neighbors(node):
                neighbor_degree = self.network.node_dict[neighbor]['degree']
                if neighbor_degree > 0:
                    rank_sum += ranks[neighbor] / neighbor_degree

            new_rank = (1 - damping) / n + damping * rank_sum
            new_ranks[node] = new_rank

            # æª¢æŸ¥æ”¶æ–‚
            diff = abs(new_rank - ranks[node])
            max_diff = max(max_diff, diff)

        ranks = new_ranks

        if max_diff < tolerance:  # æ”¶æ–‚æ¢ä»¶
            break

    return ranks
```

**ä¾‹å­**:
```
ç¶²çµ¡: A â†’ B â†’ C â†’ A (è¿´ç’°)
     D â†’ A (å­¤ç«‹å…¥é‚Š)

åˆå§‹: PR(A)=PR(B)=PR(C)=PR(D)=0.25

è¿­ä»£1:
PR(A) = 0.15/4 + 0.85 Ã— (PR(C)/1 + PR(D)/1) = 0.0375 + 0.85Ã—0.5 = 0.4625
PR(B) = 0.15/4 + 0.85 Ã— (PR(A)/2) = 0.0375 + 0.85Ã—0.125 = 0.1438
...

è¿­ä»£100ï¼ˆæ”¶æ–‚å¾Œï¼‰:
PR(A) â‰ˆ 0.35  â† æœ€é«˜ï¼ˆæœ‰ä¾†è‡ªCå’ŒDçš„è²¢ç»ï¼‰
PR(B) â‰ˆ 0.25
PR(C) â‰ˆ 0.25
PR(D) â‰ˆ 0.15  â† æœ€ä½ï¼ˆç„¡å…¥é‚Šï¼‰
```

#### åº¦ä¸­å¿ƒæ€§ï¼ˆDegree Centralityï¼‰

**å…¬å¼**:
```
C_d(v) = degree(v) / (n-1)
```

**ç‰¹é»**:
- ç°¡å–®ç›´æ¥ï¼šç›´æ¥è¨ˆç®—ç¯€é»çš„é€£æ¥æ•¸
- æ­¸ä¸€åŒ–åˆ° 0-1 ä¹‹é–“
- ç¯„åœï¼šè¶Šé«˜ = è¶Šå¤šé€£æ¥ = è¶Šä¸­å¿ƒ

**ä¾‹å­**:
```
5 å€‹ç¯€é»çš„ç¶²çµ¡
Node A: degree=4 â†’ C_d(A) = 4/4 = 1.0ï¼ˆæœ€ä¸­å¿ƒï¼‰
Node B: degree=2 â†’ C_d(B) = 2/4 = 0.5
Node C: degree=1 â†’ C_d(C) = 1/4 = 0.25ï¼ˆæœ€é‚Šç·£ï¼‰
```

#### ä»‹æ•¸ä¸­å¿ƒæ€§ï¼ˆBetweenness Centralityï¼‰

**åŸç†**ï¼šç¶“éè©²ç¯€é»çš„æœ€çŸ­è·¯å¾‘ä½”æ¯”ã€‚

**è¨ˆç®—æ–¹å¼** (ç¬¬ 449-475 è¡Œï¼Œç°¡åŒ–ç‰ˆ):
```python
def _betweenness_centrality(self, node_id: str) -> float:
    total_paths = 0
    paths_through_node = 0

    # éš¨æ©Ÿæ¡æ¨£ç¯€é»å°ï¼ˆé¿å…è¨ˆç®—æ‰€æœ‰å° O(nÂ³)ï¼‰
    nodes = list(self.network.node_dict.keys())
    sample_size = min(50, len(nodes))  # æœ€å¤šæ¡æ¨£ 50 å°

    import random
    sampled_pairs = []
    for _ in range(sample_size):
        s = random.choice(nodes)
        t = random.choice(nodes)
        if s != t and s != node_id and t != node_id:
            sampled_pairs.append((s, t))

    for s, t in sampled_pairs:
        path = self._bfs_shortest_path(s, t)
        if path:
            total_paths += 1
            if node_id in path:
                paths_through_node += 1

    return paths_through_node / total_paths if total_paths > 0 else 0.0
```

**ç‰¹é»**:
- é«˜æˆæœ¬ç®—æ³•ï¼ˆåŸå§‹ O(nÂ³)ï¼‰ï¼Œæ­¤å¯¦ç¾æ¡æ¨£ 50 å°ä¾†å„ªåŒ–
- è­˜åˆ¥ã€Œæ©‹æ¥ç¯€é»ã€ï¼ˆé€£æ¥ä¸åŒç¤¾ç¾¤çš„ç¯€é»ï¼‰
- ç¯„åœï¼š0-1ï¼Œè¶Šé«˜ = è¶Šå¤šè·¯å¾‘ç¶“é

#### æ¥è¿‘ä¸­å¿ƒæ€§ï¼ˆCloseness Centralityï¼‰

**å…¬å¼**:
```
C_c(v) = 1 / avg_distance_to_others
```

**è¨ˆç®—æ–¹å¼** (ç¬¬ 477-486 è¡Œ):
```python
def _closeness_centrality(self, node_id: str) -> float:
    distances = self._bfs_distances(node_id)  # BFS è¨ˆç®—è·é›¢

    if not distances:
        return 0.0

    avg_distance = sum(distances.values()) / len(distances)
    return 1.0 / avg_distance if avg_distance > 0 else 0.0
```

**ç‰¹é»**:
- è¡¡é‡ç¯€é»åˆ°å…¶ä»–æ‰€æœ‰ç¯€é»çš„å¹³å‡è·é›¢
- è¶Šæ¥è¿‘ = è¶Šä¸­å¿ƒ
- å°å…¨å±€ç¶²çµ¡çµæ§‹æ•æ„Ÿ

### 2. ç¤¾ç¾¤æª¢æ¸¬ï¼ˆè­˜åˆ¥æ¦‚å¿µç¾¤é›†ï¼‰

#### ä»£ç¢¼ä½ç½®
`src/analyzers/concept_mapper.py:120-284` (`CommunityDetector` é¡)

#### Louvain ç®—æ³•ï¼ˆå¯¦ç¾ï¼‰

**ç›®æ¨™**ï¼šæœ€å¤§åŒ–æ¨¡çµ„åº¦ï¼ˆmodularityï¼‰ï¼Œæ‰¾åˆ°æœ€å„ªç¤¾ç¾¤åˆ†å‰²ã€‚

**æ¼”ç®—æ³•æ­¥é©Ÿ** (ç¬¬ 182-234 è¡Œ):

```python
def _detect_by_louvain(self) -> List[Community]:
    # 1. åˆå§‹åŒ–ï¼šæ¯å€‹ç¯€é»ä¸€å€‹ç¤¾ç¾¤
    node_to_community = {node: i for i, node in enumerate(self.network.node_dict.keys())}

    improved = True
    iteration = 0
    max_iterations = 10

    # 2. è¿­ä»£å„ªåŒ–
    while improved and iteration < max_iterations:
        improved = False
        iteration += 1

        for node in self.network.node_dict.keys():
            current_community = node_to_community[node]
            best_community = current_community
            best_gain = 0.0

            # 3. å˜—è©¦ç§»å‹•åˆ°é„°å±…ç¤¾ç¾¤
            neighbor_communities = set()
            for neighbor in self.network.get_neighbors(node):
                neighbor_communities.add(node_to_community[neighbor])

            # 4. è¨ˆç®—æ¯å€‹é„°å±…ç¤¾ç¾¤çš„å¢ç›Š
            for community in neighbor_communities:
                gain = self._calculate_modularity_gain(
                    node, current_community, community, node_to_community
                )
                if gain > best_gain:
                    best_gain = gain
                    best_community = community

            # 5. ç§»å‹•åˆ°æœ€ä½³ç¤¾ç¾¤
            if best_community != current_community:
                node_to_community[node] = best_community
                improved = True

    # 6. æ§‹å»ºç¤¾ç¾¤å°è±¡
    community_nodes = defaultdict(list)
    for node, comm_id in node_to_community.items():
        community_nodes[comm_id].append(node)

    communities = []
    for comm_id, nodes in community_nodes.items():
        if len(nodes) > 1:
            community = self._create_community(comm_id, nodes)
            communities.append(community)

    return communities
```

#### ç¤¾ç¾¤å¯†åº¦è¨ˆç®—

**å…¬å¼**:
```
Density = å…§éƒ¨é‚Šæ•¸ / æœ€å¤§å¯èƒ½é‚Šæ•¸
        = E_internal / (|V| Ã— (|V|-1) / 2)
```

**å¯¦ç¾** (ç¬¬ 257-284 è¡Œ):
```python
def _create_community(self, community_id: int, nodes: List[str]) -> Community:
    # è¨ˆç®—ç¤¾ç¾¤å¯†åº¦
    internal_edges = 0
    for node in nodes:
        for neighbor in self.network.get_neighbors(node):
            if neighbor in nodes:
                internal_edges += 1
    internal_edges //= 2  # ç„¡å‘åœ–ï¼Œæ¯æ¢é‚Šè¨ˆç®—å…©æ¬¡

    max_edges = len(nodes) * (len(nodes) - 1) / 2
    density = internal_edges / max_edges if max_edges > 0 else 0.0

    # æ‰¾å‡º hub ç¯€é»ï¼ˆåº¦æœ€å¤§ï¼‰
    hub_node = max(nodes, key=lambda n: self.network.node_dict[n]['degree'])

    # æå– top æ¦‚å¿µ
    titles = [self.network.node_dict[n]['title'] for n in nodes]
    top_concepts = titles[:5]

    return Community(
        community_id=community_id,
        nodes=nodes,
        size=len(nodes),
        density=density,
        top_concepts=top_concepts,
        hub_node=hub_node
    )
```

**å¯†åº¦ç¯„åœè§£é‡‹**:
| å¯†åº¦ç¯„åœ | ç‰¹å¾µ | å«ç¾© |
|---------|------|------|
| 0.8-1.0 | éå¸¸å¯†é›† | é«˜åº¦ç›¸é—œçš„æ¦‚å¿µç¾¤ï¼Œç·Šå¯†å…§èš |
| 0.5-0.8 | å¯†é›† | æ¦‚å¿µç›¸é—œåº¦é«˜ï¼Œæœ‰æ¸…æ™°é‚Šç•Œ |
| 0.2-0.5 | ä¸­ç­‰ | æ¦‚å¿µæœ‰é—œè¯ï¼Œä½†ä¸å…¨é€£æ¥ |
| <0.2 | ç¨€ç– | æ¦‚å¿µé—œè¯é¬†æ•£ï¼Œå¯èƒ½è·¨é ˜åŸŸ |

**ä¾‹å­**:
```
ç¤¾ç¾¤ï¼š[Card-A, Card-B, Card-C]ï¼ˆ3å€‹ç¯€é»ï¼‰

å¦‚æœå…§éƒ¨é‚Šï¼šA-B, B-C, C-Aï¼ˆ3æ¢ï¼‰
max_edges = 3 Ã— 2 / 2 = 3
density = 3 / 3 = 1.0ï¼ˆå®Œå…¨é€£æ¥ï¼‰

å¦‚æœå…§éƒ¨é‚Šï¼šA-B, B-Cï¼ˆ2æ¢ï¼‰
density = 2 / 3 â‰ˆ 0.67ï¼ˆ66% å¯†é›†ï¼‰
```

### 3. è·¯å¾‘åˆ†æï¼ˆæ¦‚å¿µæ¨å°ï¼‰

#### ä»£ç¢¼ä½ç½®
`src/analyzers/concept_mapper.py:287-410` (`PathAnalyzer` é¡)

#### æœ€çŸ­è·¯å¾‘å°‹æ‰¾

**æ–¹æ³•**ï¼šBFSï¼ˆå»£åº¦å„ªå…ˆæœç´¢ï¼‰

**å¯¦ç¾** (ç¬¬ 296-324 è¡Œ):
```python
def find_shortest_path(self, start: str, end: str) -> Optional[ConceptPath]:
    if start not in self.network.node_dict or end not in self.network.node_dict:
        return None

    # BFS
    queue = deque([(start, [start])])
    visited = {start}

    while queue:
        node, path = queue.popleft()

        if node == end:
            confidence = self._calculate_path_confidence(path)
            return ConceptPath(
                start_node=start,
                end_node=end,
                path=path,
                length=len(path) - 1,
                confidence=confidence
            )

        for neighbor in self.network.get_neighbors(node):
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))

    return None  # ç„¡è·¯å¾‘
```

**ç‰¹é»**:
- è¤‡é›œåº¦ï¼šO(V+E)ï¼Œé«˜æ•ˆ
- è¿”å›æœ€çŸ­è·¯å¾‘ï¼ˆæœ€å°‘è·³è½‰æ•¸ï¼‰
- å¯ç”¨æ–¼æ¦‚å¿µæ¨å°éˆ

#### è·¯å¾‘ä¿¡åº¦è¨ˆç®—

**å…¬å¼**:
```
Path_Confidence = Avg(é‚Šçš„ä¿¡åº¦)
```

**å¯¦ç¾** (ç¬¬ 377-388 è¡Œ):
```python
def _calculate_path_confidence(self, path: List[str]) -> float:
    if len(path) < 2:
        return 1.0

    confidences = []
    for i in range(len(path) - 1):
        edge = self.network.get_edge(path[i], path[i+1])
        if edge:
            confidences.append(edge.get('confidence', 0.5))

    return sum(confidences) / len(confidences) if confidences else 0.0
```

**ä¾‹å­**:
```
è·¯å¾‘ï¼šCard-A â†’ Card-B â†’ Card-C

é‚Šä¿¡åº¦ï¼š
- (Aâ†’B): 0.75
- (Bâ†’C): 0.68

è·¯å¾‘ä¿¡åº¦ = (0.75 + 0.68) / 2 = 0.715
â†’ è¼ƒé«˜çš„è·¯å¾‘å¯é æ€§
```

---

## åŸå­å¡ç‰‡å…ƒç´ è©³è§£

### ç´å…¥è¨ˆç®—çš„å…ƒç´ ç¸½è¡¨

| å…ƒç´ åç¨± | è³‡æ–™åº«æ¬„ä½ | è¨ˆç®—ç¶­åº¦ | æ¬Šé‡ | èªªæ˜ |
|---------|----------|--------|------|------|
| **åµŒå…¥å‘é‡** | (ChromaDB) | èªç¾©ç›¸ä¼¼åº¦ | 40% | Gemini/Ollama å‘é‡æ¨¡å‹çš„è¼¸å‡º |
| **æ ¸å¿ƒæ¦‚å¿µ** | core_concept | å…±åŒæ¦‚å¿µ | 20% | å¡ç‰‡çš„åŸºæœ¬ç†å¿µï¼Œç”¨æ–¼æå–é—œéµè© |
| **æ¨™ç±¤** | tags | å…±åŒæ¦‚å¿µ | 20% | çµæ§‹åŒ–æ¨™ç±¤åˆ—è¡¨ï¼Œç›´æ¥ç´å…¥æ¦‚å¿µè¨ˆç®— |
| **æ¨™é¡Œ** | title | å…±åŒæ¦‚å¿µ | 20% | å¡ç‰‡æ¨™é¡Œï¼Œç”¨æ–¼æå–é—œéµè© |
| **å…§å®¹** | content/ai_notes | é€£çµè­˜åˆ¥ | 30% | ç”¨æ–¼æª¢æ¸¬æ˜ç¢ºçš„ Wiki Links |
| **é ˜åŸŸ** | domain | é ˜åŸŸä¸€è‡´æ€§ | 10% | çŸ¥è­˜é ˜åŸŸä»£ç¢¼ |
| **AI ç­†è¨˜** | ai_notes | é€£çµè­˜åˆ¥ | 30% | å„ªå…ˆæœç´¢ä½ç½®ï¼ˆç›¸å°æ–¼ contentï¼‰ |

### å„å…ƒç´ çš„å…·é«”ç”¨é€”

#### 1. æ ¸å¿ƒæ¦‚å¿µï¼ˆcore_conceptï¼‰

**ä½ç½®**: `zettel_cards` è¡¨çš„ `core_concept` æ¬„ä½

**å…§å®¹ç¤ºä¾‹**:
```
"è¦–è¦ºç³»çµ±çš„çµæ§‹èˆ‡åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¦–ç¶²è†œã€è¦–å¢ã€è¦–çš®å±¤çš„è¨Šæ¯è™•ç†æ©Ÿåˆ¶"
```

**æå–æ–¹å¼**:
```python
# åˆ†è© (tokenization)
words = re.findall(r'\w+', core.lower())
# éæ¿¾åœç”¨è©ï¼ˆä¿ç•™ â‰¥3 å­—çš„è©ï¼‰
keywords = [w for w in words if len(w) >= 3]
# çµæœï¼š['è¦–è¦º', 'ç³»çµ±', 'çµæ§‹', 'åŠŸèƒ½', 'è¦–ç¶²è†œ', 'è¦–å¢', 'è¦–çš®å±¤', 'è¨Šæ¯', 'è™•ç†', 'æ©Ÿåˆ¶']
```

**ä½œç”¨**:
- æ•æ‰å¡ç‰‡çš„**æ ¸å¿ƒæ„æ¶µ**
- ç›¸æ¯”æ¨™é¡Œæ›´è©³ç´°ï¼Œç›¸æ¯”å…§å®¹æ›´ç²¾å‡
- æå–çš„è©ç”¨æ–¼**å…±åŒæ¦‚å¿µè¨ˆç®—**

#### 2. æ¨™ç±¤ï¼ˆtagsï¼‰

**ä½ç½®**: `zettel_cards` è¡¨çš„ `tags` æ¬„ä½

**æ ¼å¼**:
```json
["è¦–è¦ºè™•ç†", "ç¥ç¶“ç§‘å­¸", "æ„ŸçŸ¥", "èªçŸ¥æ¨¡å‹"]
```

**æå–é‚è¼¯**:
```python
if isinstance(tags, str):
    if tags.startswith('['):
        tag_list = json.loads(tags)  # JSON æ ¼å¼
    else:
        tag_list = [t.strip() for t in tags.split(',')]  # CSV æ ¼å¼
elif isinstance(tags, list):
    tag_list = tags  # å·²æ˜¯åˆ—è¡¨

# ç›´æ¥ç´å…¥å…±åŒæ¦‚å¿µé›†åˆ
concepts.update(tag_list)
```

**ä½œç”¨**:
- æä¾›**äººå·¥æ¨™è¨»çš„æ¦‚å¿µ**
- æ¯”å¾æ–‡æœ¬æå–æ›´æº–ç¢ºï¼ˆäººå·¥é¸æ“‡ï¼‰
- ç›´æ¥è¨ˆå…¥å…±åŒæ¦‚å¿µé›†åˆ

**ç‰¹é»**:
- å¦‚æœæœ‰æ¨™ç±¤ï¼Œå„ªå…ˆä½¿ç”¨ï¼ˆç„¡éœ€åˆ†è©ï¼‰
- ä¸€å€‹æ¨™ç±¤ = ä¸€å€‹å®Œæ•´æ¦‚å¿µå–®ä½

#### 3. æ¨™é¡Œï¼ˆtitleï¼‰

**ä½ç½®**: `zettel_cards` è¡¨çš„ `title` æ¬„ä½

**å…§å®¹ç¤ºä¾‹**:
```
"è¦–è¦ºçš®å±¤ V1 å€çš„ç©ºé–“é »ç‡é¸æ“‡æ€§"
```

**æå–æ–¹å¼**:
```python
words = re.findall(r'\w+', title.lower())
keywords = [w for w in words if len(w) >= 3]
# çµæœï¼š['è¦–è¦º', 'çš®å±¤', 'ç©ºé–“', 'é »ç‡', 'é¸æ“‡']
```

**ä½œç”¨**:
- æä¾›**å¡ç‰‡çš„ç°¡æ½”è¡¨è¿°**
- è¼ƒä¹‹å…§å®¹ï¼Œä¿¡æ¯å¯†åº¦é«˜
- ç”¨æ–¼æå–**é«˜è³ªé‡é—œéµè©**

#### 4. å…§å®¹ï¼ˆcontent å’Œ ai_notesï¼‰

**ä½ç½®**: `zettel_cards` è¡¨çš„ `content` å’Œ `ai_notes` æ¬„ä½

**ç”¨é€”åˆ†å·¥**:

| æ¬„ä½ | ç”¨é€” | ä¾†æº |
|------|------|------|
| `ai_notes` | é€£çµæª¢æ¸¬å„ªå…ˆæœç´¢ | AI ç”Ÿæˆçš„æ‰¹åˆ¤æ€§ç­†è¨˜ |
| `content` | Fallback ä¾†æº | å®Œæ•´å¡ç‰‡å…§å®¹ï¼ˆåŒ…å«äººé¡ç­†è¨˜ï¼‰ |

**é€£çµæª¢æ¸¬éç¨‹**:
```python
def _check_explicit_link(self, card: Dict, target_id: str) -> bool:
    # å„ªå…ˆä½¿ç”¨ ai_notes
    ai_notes = card.get('ai_notes')
    if ai_notes:
        ai_content = ai_notes
    else:
        # Fallbackï¼šå¾ content æå– AI å…§å®¹
        content = card.get('content', '')
        ai_content = extract_ai_content(content)  # éæ¿¾äººé¡ç­†è¨˜

    # æª¢æŸ¥ Obsidian Wiki Links
    return f'[[{target_id}]]' in ai_content
```

**é€£çµæ ¼å¼**:
```markdown
# ä¾‹å­ 1ï¼šWiki Link åœ¨ ai_notes ä¸­
**[AI Agent]**: é€™å€‹æ¦‚å¿µèˆ‡ [[CogSci-20251028-002]] å¯†åˆ‡ç›¸é—œ...

# ä¾‹å­ 2ï¼šWiki Link åœ¨ content ä¸­
## ç›¸é—œæ¦‚å¿µ
[[Linguistics-20251028-005]] è¨è«–äº†é¡ä¼¼çš„èªæ³•ç¾è±¡...
```

**ä½œç”¨**:
- åƒ…ç”¨æ–¼**æ˜ç¢ºé€£çµè­˜åˆ¥**
- ä¸ç”¨æ–¼ç›¸ä¼¼åº¦æˆ–å…±åŒæ¦‚å¿µè¨ˆç®—
- æä¾› 30% çš„ä¿¡åº¦åŠ æ¬Š

#### 5. é ˜åŸŸï¼ˆdomainï¼‰

**ä½ç½®**: `zettel_cards` è¡¨çš„ `domain` æ¬„ä½

**å¸¸è¦‹å€¼**:
```
"CogSci"      # èªçŸ¥ç§‘å­¸
"Linguistics"  # èªè¨€å­¸
"AI"           # äººå·¥æ™ºæ…§
"Research"     # é€šç”¨ç ”ç©¶
```

**ä½œç”¨**:
- ä½œç‚º**é ˜åŸŸä¸€è‡´æ€§æª¢æŸ¥**
- ä¿ƒé€²åŒé ˜åŸŸå¡ç‰‡ç›¸é—œï¼ˆ+0.1 vs +0.05ï¼‰
- æ¬Šé‡æœ€ä½ï¼ˆ10%ï¼‰ï¼Œä½œç‚ºè£œå……æŒ‡æ¨™

---

## è¨ˆç®—æµç¨‹åœ–

### æ•´é«”æµç¨‹ï¼ˆé«˜å±¤ï¼‰

```
Zettelkasten åŸå­å¡ç‰‡åº«
        â†“
    [1] å‘é‡åŒ–
    ä½¿ç”¨ Gemini/Ollama ç”Ÿæˆ 768/2560 ç¶­å‘é‡
        â†“
    [2] å‘é‡æœç´¢
    ChromaDB find_similar_zettel()
    â†’ è¿”å›ç›¸ä¼¼å¡ç‰‡åˆ—è¡¨ + è·é›¢
        â†“
    [3] è½‰æ›ç›¸ä¼¼åº¦
    similarity = 1.0 - distance
        â†“
    [4] è¨ˆç®—å…±åŒæ¦‚å¿µ
    æå–ï¼štags + core_concept + title
    è¨ˆç®—äº¤é›† â†’ shared_concepts[]
        â†“
    [5] æª¢æŸ¥æ˜ç¢ºé€£çµ
    æƒæ ai_notes/content ä¸­çš„ [[card_id]]
        â†“
    [6] å¤šç¶­åº¦ä¿¡åº¦è©•åˆ†
    semantic(40%) + link(30%) + shared(20%) + domain(10%)
        â†“
    [7] åˆ†é¡é—œä¿‚é¡å‹
    åŸºæ–¼ç›¸ä¼¼åº¦ã€é—œéµè©ã€é€£çµæ–¹å‘
        â†“
    ConceptRelation å°è±¡
    (card_id_1, card_id_2, relation_type, confidence, similarity)
        â†“
    [8] é«˜ç´šåˆ†æï¼ˆconcept_mapperï¼‰
    ç¤¾ç¾¤æª¢æ¸¬ | è·¯å¾‘åˆ†æ | ä¸­å¿ƒæ€§åˆ†æ
        â†“
    æœ€çµ‚è¼¸å‡ºï¼šæ¦‚å¿µç¶²çµ¡ + è¦–è¦ºåŒ– + å ±å‘Š
```

### ä¿¡åº¦è©•åˆ†è©³ç´°æµç¨‹

```
ConceptRelation å‰µå»ºæµç¨‹
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é€å°å¡ç‰‡è™•ç†              â”‚
â”‚  (card_i, card_j for j>i)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    å‘é‡ç›¸ä¼¼åº¦è¨ˆç®—
    similarity = 1.0 - distance
         â†“
    â†™        â†–
 (< 0.4)?    (â‰¥ 0.4) âœ“
   âŒ          â†“
   æ’é™¤    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  åˆ†é¡é—œä¿‚é¡å‹    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  è¨ˆç®—ä¿¡åº¦è©•åˆ†        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                â†“                â†“
semantic_score   link_score      co_occurrence
= sim Ã— 0.4     [0.3 or 0]      = min(len/5, 1.0)Ã—0.2
    â†“                â†“                â†“
  æª¢æŸ¥            æª¢æŸ¥           æå–å…±åŒ
  å‘é‡            Wiki           æ¦‚å¿µ
  ç›¸ä¼¼            Links          è©å½™
  åº¦              ([[...]])       è¨ˆæ•¸
    â†“                â†“                â†“
  [0-0.4]      [æ˜¯å¦å­˜åœ¨]      [0-5å€‹]
    â”‚                â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
                domain_score
                  [0.1 or 0.05]
                     â†“
                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                                  â†“
domain1 == domain2            domain1 â‰  domain2
    â†“                                  â†“
  0.1 åˆ†                            0.05 åˆ†
 (åŒé ˜åŸŸ)                          (ä¸åŒ/ç¼ºå¤±)
    â†“                                  â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
        ç¸½ä¿¡åº¦ = sum(æ‰€æœ‰ç¶­åº¦)
                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  confidence_score âˆˆ [0.0, 1.0]   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  éæ¿¾ä½ä¿¡åº¦ (<0.3)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    ConceptRelation å°è±¡
    âœ“ ä¿ç•™ç”¨æ–¼å¾ŒçºŒåˆ†æ
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šå®Œæ•´çš„ç›¸ä¼¼æ€§åˆ†æå·¥ä½œæµ

```python
from src.analyzers.relation_finder import RelationFinder
from src.analyzers.concept_mapper import ConceptMapper

# åˆå§‹åŒ–
finder = RelationFinder(kb_path="knowledge_base")

# æ­¥é©Ÿ 1ï¼šè­˜åˆ¥æ¦‚å¿µé—œä¿‚ï¼ˆåŸå­å¡ç‰‡ï¼‰
print("è­˜åˆ¥ Zettelkasten å¡ç‰‡é–“çš„èªç¾©é—œä¿‚...")
relations = finder.find_concept_relations(
    min_similarity=0.4,      # æœ€å°å‘é‡ç›¸ä¼¼åº¦
    relation_types=None,     # æ‰€æœ‰é—œä¿‚é¡å‹
    limit=100               # æ¯å¼µå¡ç‰‡æœ€å¤šæª¢æŸ¥ 100 å€‹ç›¸ä¼¼å¡ç‰‡
)

# è¼¸å‡ºæ¨£æœ¬
for rel in relations[:5]:
    print(f"{rel.card_id_1} --{rel.relation_type}--> {rel.card_id_2}")
    print(f"  ä¿¡åº¦: {rel.confidence_score:.3f}")
    print(f"  ç›¸ä¼¼åº¦: {rel.semantic_similarity:.3f}")
    print(f"  å…±åŒæ¦‚å¿µ: {', '.join(rel.shared_concepts)}")
    print(f"  æ˜ç¢ºé€£çµ: {rel.link_explicit}")
    print()

# æ­¥é©Ÿ 2ï¼šå»ºæ§‹æ¦‚å¿µç¶²çµ¡
print("å»ºæ§‹æ¦‚å¿µç¶²çµ¡...")
network_data = finder.build_concept_network(
    min_similarity=0.4,
    min_confidence=0.3
)

# æ­¥é©Ÿ 3ï¼šé«˜ç´šåˆ†æ
mapper = ConceptMapper(kb_path="knowledge_base")
results = mapper.analyze_all(
    output_dir="output/concept_analysis",
    visualize=True,
    obsidian_mode=True
)

print(f"æ‰¾åˆ° {len(results['communities'])} å€‹æ¦‚å¿µç¤¾ç¾¤")
print(f"è­˜åˆ¥å‡º {len(results['paths'])} æ¢æ¨å°è·¯å¾‘")
```

### ç¤ºä¾‹ 2ï¼šæŸ¥è©¢ç‰¹å®šå¡ç‰‡çš„ç›¸ä¼¼å¡ç‰‡

```python
# æ‰¾åˆ°èˆ‡æŸå€‹å¡ç‰‡æœ€ç›¸ä¼¼çš„å…¶ä»–å¡ç‰‡
finder = RelationFinder()

# ç²å–ç‰¹å®šå¡ç‰‡çš„é—œä¿‚
target_card_id = "CogSci-20251028-001"

relations = finder.find_concept_relations(min_similarity=0.5)

# ç¯©é¸èˆ‡ç›®æ¨™å¡ç‰‡ç›¸é—œçš„é—œä¿‚
target_relations = [
    r for r in relations
    if r.card_id_1 == target_card_id or r.card_id_2 == target_card_id
]

# æŒ‰ä¿¡åº¦æ’åº
target_relations.sort(key=lambda r: r.confidence_score, reverse=True)

# é¡¯ç¤ºçµæœ
for rel in target_relations[:10]:
    other_id = rel.card_id_2 if rel.card_id_1 == target_card_id else rel.card_id_1
    print(f"ç›¸ä¼¼å¡ç‰‡: {other_id}")
    print(f"  é—œä¿‚: {rel.relation_type}")
    print(f"  ä¿¡åº¦: {rel.confidence_score:.3f}")
    print(f"  ç›¸ä¼¼åº¦: {rel.semantic_similarity:.3f}")
```

### ç¤ºä¾‹ 3ï¼šä¿¡åº¦è©•åˆ†æ‹†è§£

```python
# æª¢æŸ¥ç‰¹å®šé—œä¿‚çš„ä¿¡åº¦è¨ˆç®—è©³æƒ…
from src.analyzers.relation_finder import RelationFinder

finder = RelationFinder()

# å–å¾—å…©å¼µå¡ç‰‡çš„æ•¸æ“š
card1_id = "AI-20251028-001"
card2_id = "AI-20251028-002"

# æ‰‹å‹•è¨ˆç®—ä¿¡åº¦ï¼ˆç”¨æ–¼ç†è§£ï¼‰
# åœ¨å¯¦éš›ä»£ç¢¼ä¸­ï¼Œé€™å·²ç”± _calculate_confidence è‡ªå‹•å®Œæˆ

# å‡è¨­ï¼š
similarity = 0.72       # å¾å‘é‡æœç´¢
has_link = True         # æª¢æŸ¥åˆ° [[AI-20251028-002]]
shared_concepts = ["æ©Ÿå™¨å­¸ç¿’", "ç¥ç¶“ç¶²çµ¡"]  # 2 å€‹å…±åŒæ¦‚å¿µ
same_domain = True      # éƒ½æ˜¯ "AI" é ˜åŸŸ

# è¨ˆç®—å„ç¶­åº¦
semantic_score = similarity * 0.4  # 0.72 * 0.4 = 0.288
link_score = 0.3 if has_link else 0.0  # 0.3
co_occurrence = min(len(shared_concepts) / 5.0, 1.0) * 0.2  # 2/5 * 0.2 = 0.08
domain_score = 0.1 if same_domain else 0.05  # 0.1

total_confidence = semantic_score + link_score + co_occurrence + domain_score
# = 0.288 + 0.3 + 0.08 + 0.1 = 0.768

print(f"ç¸½ä¿¡åº¦: {total_confidence:.3f} âœ“ é«˜ä¿¡åº¦")
print(f"  èªç¾©ç›¸ä¼¼åº¦: {semantic_score:.3f} (40%)")
print(f"  æ˜ç¢ºé€£çµ: {link_score:.3f} (30%)")
print(f"  å…±åŒæ¦‚å¿µ: {co_occurrence:.3f} (20%)")
print(f"  é ˜åŸŸä¸€è‡´: {domain_score:.3f} (10%)")
```

---

## æ€§èƒ½å’Œå„ªåŒ–

### æ™‚é–“è¤‡é›œåº¦åˆ†æ

| æ“ä½œ | æ™‚é–“è¤‡é›œåº¦ | å‚™è¨» |
|------|----------|------|
| **å‘é‡ç›¸ä¼¼åº¦æœç´¢** | O(n) | n = å¡ç‰‡æ•¸ï¼ŒChromaDB å„ªåŒ– |
| **å…±åŒæ¦‚å¿µè¨ˆç®—** | O(m Ã— k) | m = å¡ç‰‡å°æ•¸ï¼Œk = å¹³å‡æ¨™ç±¤/è©å½™æ•¸ |
| **ä¿¡åº¦è©•åˆ†è¨ˆç®—** | O(m) | m = å¡ç‰‡å°æ•¸ |
| **ç¤¾ç¾¤æª¢æ¸¬ï¼ˆLouvainï¼‰** | O(n Ã— max_iter) | max_iter = 10ï¼Œé€šå¸¸å¿«é€Ÿæ”¶æ–‚ |
| **PageRank** | O(n Ã— iter) | iter = 100ï¼ˆé€šå¸¸ <50 æ¬¡æ”¶æ–‚ï¼‰ |
| **è·¯å¾‘åˆ†æ** | O(n + m) | BFS æœç´¢ï¼Œå–®æ¬¡æŸ¥è©¢ |
| **å®Œæ•´åˆ†æ** | ~O(nÂ² + n log n) | n = 704 å¡ç‰‡ï¼Œâ‰ˆ 2-3 åˆ†é˜ |

### è¨˜æ†¶é«”ä½¿ç”¨

| çµæ§‹ | å¤§å° | èªªæ˜ |
|------|------|------|
| **å‘é‡ç´¢å¼•ï¼ˆChromaDBï¼‰** | ~100-200 MB | 704 å¼µå¡ç‰‡ Ã— 768/2560 ç¶­å‘é‡ |
| **åœ–çµæ§‹** | ~5-10 MB | ç¯€é»å’Œé‚Šçš„é„°æ¥è¡¨ |
| **JSON å ±å‘Š** | ~5 MB | å®Œæ•´åˆ†ææ•¸æ“š |
| **ç¸½è¨ˆ** | ~150-250 MB | å–®æ¬¡åˆ†æ |

### å„ªåŒ–å»ºè­°

#### 1. å‘é‡ç›¸ä¼¼åº¦æœç´¢

```python
# âŒ ä½æ•ˆï¼šé€å€‹æŸ¥è©¢
for card_id in all_cards:
    results = vector_db.find_similar_zettel(card_id)

# âœ… é«˜æ•ˆï¼šæ‰¹é‡æŸ¥è©¢ï¼ˆæœªä¾†æ”¹é€²ï¼‰
results = vector_db.batch_find_similar(card_ids, batch_size=50)
```

#### 2. ä»‹æ•¸ä¸­å¿ƒæ€§è¨ˆç®—

```python
# ç•¶å‰å¯¦ç¾å·²æ¡æ¨£ï¼ˆåŸå§‹ O(nÂ³)ï¼‰
# éš¨æ©Ÿæ¡æ¨£ 50 å°ç¯€é»å°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ C(n,2) å°

sample_size = min(50, len(nodes))  # é™åˆ¶æ¡æ¨£å¤§å°
```

#### 3. ç¤¾ç¾¤æª¢æ¸¬è¿­ä»£æ¬¡æ•¸

```python
# å¯æ ¹æ“šç¶²çµ¡è¦æ¨¡èª¿æ•´
max_iterations = 10  # 704 ç¯€é»è¶³å¤ 
# å¤§ç¶²çµ¡å¯æ¸›å°‘åˆ° 5ï¼Œå°ç¶²çµ¡å¯å¢åŠ åˆ° 20
```

### ç”Ÿæˆæ¬¡æ•¸æ¯”è¼ƒ

**é¦–æ¬¡å®Œæ•´åˆ†æï¼ˆ704 å¼µå¡ç‰‡ï¼‰**:
- å‘é‡åŒ–ï¼šå·²é è¨ˆç®—ï¼ˆä¸è¨ˆå…¥ï¼‰
- é—œä¿‚è­˜åˆ¥ï¼š~120 ç§’
- é«˜ç´šåˆ†æï¼š~30 ç§’
- è¦–è¦ºåŒ–ï¼š~10 ç§’
- **ç¸½è¨ˆï¼š~160 ç§’ï¼ˆ2-3 åˆ†é˜ï¼‰**

**å¢é‡æ›´æ–°ï¼ˆæ–°å¢ 10 å¼µå¡ç‰‡ï¼‰**:
- æ–°å¡ç‰‡å‘é‡åŒ–ï¼š~10 ç§’
- æ–°å¡ç‰‡çš„é—œä¿‚è­˜åˆ¥ï¼š~15 ç§’
- ç¶²çµ¡é‡æ–°è¨ˆç®—ï¼š~20 ç§’
- **ç¸½è¨ˆï¼š~45 ç§’ï¼ˆæœªä¾†å¯å„ªåŒ–åˆ° 20 ç§’ï¼‰**

---

## é™„éŒ„ï¼šè³‡æ–™åº«çµæ§‹åƒè€ƒ

### zettel_cards è¡¨çµæ§‹

```sql
CREATE TABLE zettel_cards (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    zettel_id TEXT UNIQUE NOT NULL,          -- å¡ç‰‡ IDï¼Œå¦‚ "CogSci-20251028-001"
    paper_id INTEGER,                        -- é—œè¯è«–æ–‡ ID
    title TEXT NOT NULL,                     -- å¡ç‰‡æ¨™é¡Œ
    core_concept TEXT,                       -- æ ¸å¿ƒæ¦‚å¿µï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦è¨ˆç®—ï¼‰
    tags TEXT,                               -- JSON æ ¼å¼æ¨™ç±¤åˆ—è¡¨ï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦è¨ˆç®—ï¼‰
    domain TEXT,                             -- é ˜åŸŸä»£ç¢¼ï¼ˆç”¨æ–¼é ˜åŸŸä¸€è‡´æ€§è¨ˆç®—ï¼‰
    content TEXT,                            -- å®Œæ•´å¡ç‰‡å…§å®¹ï¼ˆç”¨æ–¼é€£çµæª¢æ¸¬ fallbackï¼‰
    ai_notes TEXT,                           -- AI ç­†è¨˜ï¼ˆå„ªå…ˆç”¨æ–¼é€£çµæª¢æ¸¬ï¼‰
    human_notes TEXT,                        -- äººé¡ç­†è¨˜
    file_path TEXT,                          -- æª”æ¡ˆè·¯å¾‘
    zettel_folder TEXT,                      -- è³‡æ–™å¤¾åç¨±
    card_type TEXT,                          -- å¡ç‰‡é¡å‹
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### ç›¸ä¼¼åº¦ç›¸é—œçš„é‡è¦æ¬„ä½

| æ¬„ä½ | å„ªå…ˆåº¦ | ç”¨é€” |
|------|--------|------|
| **core_concept** | â­â­â­ | å…±åŒæ¦‚å¿µæå–ï¼ˆç²¾å‡å…§å®¹ï¼‰ |
| **tags** | â­â­â­ | å…±åŒæ¦‚å¿µæå–ï¼ˆäººå·¥æ¨™è¨»ï¼‰ |
| **title** | â­â­ | å…±åŒæ¦‚å¿µæå–ï¼ˆå‚™é¸ï¼‰ |
| **ai_notes** | â­â­â­ | æ˜ç¢ºé€£çµæª¢æ¸¬ï¼ˆå„ªå…ˆæºï¼‰ |
| **content** | â­â­ | æ˜ç¢ºé€£çµæª¢æ¸¬ï¼ˆfallbackï¼‰ |
| **domain** | â­ | é ˜åŸŸä¸€è‡´æ€§æª¢æŸ¥ |

---

## å¸¸è¦‹å•é¡Œ (FAQ)

### Q1: ç‚ºä»€éº¼å‘é‡ç›¸ä¼¼åº¦çš„æœ€å°é–¾å€¼æ˜¯ 0.4ï¼Ÿ

**A**: 0.4 æ˜¯ç¶“é©—å€¼ï¼ŒåŸºæ–¼å¯¦æ¸¬æ•¸æ“šï¼š
- 0.3-0.4ï¼šå¯èƒ½ç„¡é—œä½†å·§åˆç›¸ä¼¼ï¼ˆå‡æ­£ä¾‹ï¼‰
- 0.4-0.7ï¼šä¸­ç­‰ç›¸é—œï¼Œå€¼å¾—æª¢æŸ¥
- 0.7+ï¼šé«˜åº¦ç›¸é—œï¼Œå¯ä¿¡åº¦é«˜

å¯¦æ¸¬ 704 å¼µå¡ç‰‡ä¸­ï¼Œ0.4 ä»¥ä¸‹çš„ç›¸ä¼¼åº¦å¹¾ä¹éƒ½æ˜¯ç„¡é—œçš„ã€‚

### Q2: ç‚ºä»€éº¼æ˜ç¢ºé€£çµæ¬Šé‡é€™éº¼é«˜ï¼ˆ30%ï¼‰ï¼Ÿ

**A**: äººç‚ºæ¨™è¨»çš„é€£çµæ¯”è‡ªå‹•è¨ˆç®—æ›´å¯é ï¼š
- äººé¡ review éï¼Œå‡æ­£ä¾‹å°‘
- åæ˜ ç·¨è€…çš„ä¸»è§€åˆ¤æ–·ï¼ˆé‡è¦åƒè€ƒï¼‰
- åœ¨ä¿¡åº¦è¨ˆç®—ä¸­èµ·æ±ºå®šä½œç”¨

ä½†ä»ä½æ–¼å‘é‡ç›¸ä¼¼åº¦ï¼ˆ40%ï¼‰ï¼Œé¿å…éåº¦ä¾è³´äººå·¥ã€‚

### Q3: å…±åŒæ¦‚å¿µåªè¨ˆç®— 5 å€‹çš„ä¸Šé™ï¼Œå¦‚æœæœ‰ 10 å€‹å‘¢ï¼Ÿ

**A**:
```python
shared_score = min(len(shared) / 5.0, 1.0) * 0.2
```

`min(..., 1.0)` ç¢ºä¿è¶…é 5 å€‹æ™‚ä¸æœƒç„¡é™å¢é•·ã€‚
é‚è¼¯ï¼šå…±åŒæ¦‚å¿µå¤ªå¤šåè€Œå¯èƒ½æ˜¯**è¤‡è£½å…§å®¹**ï¼ˆåé¢ä¿¡è™Ÿï¼‰ã€‚

### Q4: é ˜åŸŸä¸€è‡´æ€§ç‚ºä½•åªæœ‰ 0.1 å’Œ 0.05 å…©ç¨®å€¼ï¼Ÿ

**A**: é€™æ˜¯è¨­è¨ˆé¸æ“‡ï¼š
- ç°¡æ½”æ˜å¿«ï¼Œé¿å…æ¢¯åº¦è¨ˆç®—
- é ˜åŸŸæœ¬æ‡‰æ˜¯é›¢æ•£å€¼ï¼Œä¸æ˜¯é€£çºŒå…‰è­œ
- å¯¦æ¸¬è¡¨æ˜äºŒå€¼è¶³å¤ å€åˆ†

è‹¥éœ€å¾®èª¿ï¼Œå¯æ”¹ç‚º 0.05-0.15 ä¹‹é–“çš„æ¢¯åº¦å€¼ã€‚

### Q5: å¦‚ä½•ä½¿ç”¨ç›¸ä¼¼åº¦é€²è¡Œæ¨è–¦ï¼Ÿ

**A**: æŒ‰ä¿¡åº¦æ’åºä¸¦å‘ˆç¾ï¼š
```python
# ç‚º Card A æ¨è–¦ç›¸é—œå¡ç‰‡
relations = finder.find_concept_relations()
a_relations = [r for r in relations
               if r.card_id_1 == target_id]
a_relations.sort(key=lambda r: r.confidence_score, reverse=True)

# æ¨è–¦ Top 5ï¼Œä¸¦èªªæ˜ç†ç”±
for i, rel in enumerate(a_relations[:5], 1):
    reason = f"leads_to" if rel.relation_type == 'leads_to' else rel.relation_type
    print(f"{i}. {rel.card_id_2} ({reason}, ä¿¡åº¦ {rel.confidence_score:.2%})")
```

---

## çµè«–

Zettelkasten åŸå­å¡ç‰‡çš„æ¦‚å¿µç›¸ä¼¼æ€§è¨ˆç®—æ˜¯ä¸€å€‹**å¤šç¶­åº¦ã€å¤šå±¤æ¬¡**çš„ç³»çµ±ï¼š

1. **åº•å±¤**ï¼šå‘é‡åµŒå…¥æä¾›èªç¾©ç†è§£ï¼ˆ40%ï¼‰
2. **ä¸­å±¤**ï¼šäººå·¥æ¨™è¨»å’Œé€£çµæä¾›çµæ§‹ä¿¡æ¯ï¼ˆ30% + 20%ï¼‰
3. **ä¸Šå±¤**ï¼šé ˜åŸŸå’Œä¸Šä¸‹æ–‡æä¾›åˆ†é¡ä¿¡æ¯ï¼ˆ10%ï¼‰

é€™ç¨®**æ··åˆæ–¹æ³•**çµåˆäº†è‡ªå‹•åŒ–çš„å„ªå‹¢ï¼ˆå‘é‡ï¼‰å’Œäººå·¥æ™ºæ…§çš„å„ªå‹¢ï¼ˆæ¨™è¨»ã€é€£çµï¼‰ï¼Œæ˜¯æ§‹å»ºé«˜è³ªé‡çŸ¥è­˜ç¶²çµ¡çš„é—œéµã€‚

---

**æ–‡ä»¶æ›´æ–°è¨˜éŒ„**

| ç‰ˆæœ¬ | æ—¥æœŸ | è®Šæ›´ |
|------|------|------|
| 1.0 | 2025-11-05 | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´èªªæ˜å‘é‡ç›¸ä¼¼åº¦å’Œå¤šç¶­åº¦ä¿¡åº¦è©•åˆ†ç³»çµ± |

