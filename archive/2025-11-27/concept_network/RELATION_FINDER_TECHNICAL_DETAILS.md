# RelationFinder å’Œ ConceptMapper æŠ€è¡“æ·±åº¦è§£æ

**æ–‡ä»¶ç‰ˆæœ¬**: 1.0
**ç”Ÿæˆæ—¥æœŸ**: 2025-11-05
**é©ç”¨æ¨¡çµ„**: relation_finder.py (Phase 2.1) å’Œ concept_mapper.py (Phase 2.2)

---

## ğŸ“‹ ç›®éŒ„

1. [ä»£ç¢¼æ¶æ§‹æ¦‚è¦½](#ä»£ç¢¼æ¶æ§‹æ¦‚è¦½)
2. [RelationFinder è©³ç´°å¯¦ç¾](#relationfinder-è©³ç´°å¯¦ç¾)
3. [ConceptMapper è©³ç´°å¯¦ç¾](#conceptmapper-è©³ç´°å¯¦ç¾)
4. [å‘é‡æ•¸æ“šåº«é›†æˆ](#å‘é‡æ•¸æ“šåº«é›†æˆ)
5. [å¸¸è¦‹é™·é˜±å’Œè§£æ±ºæ–¹æ¡ˆ](#å¸¸è¦‹é™·é˜±å’Œè§£æ±ºæ–¹æ¡ˆ)
6. [æ€§èƒ½å„ªåŒ–æŠ€å·§](#æ€§èƒ½å„ªåŒ–æŠ€å·§)

---

## ä»£ç¢¼æ¶æ§‹æ¦‚è¦½

### é¡å’Œæ–¹æ³•çš„ç¹¼æ‰¿é—œä¿‚

```
RelationFinder (relation_finder.py:86-1041)
â”œâ”€â”€ __init__(kb_path, config)
â”œâ”€â”€ è«–æ–‡é—œä¿‚åˆ†æï¼ˆPhase 1ï¼‰
â”‚   â”œâ”€â”€ find_citations_by_title_similarity()
â”‚   â”œâ”€â”€ find_co_authors()
â”‚   â”œâ”€â”€ find_co_occurrence()
â”‚   â””â”€â”€ export_to_mermaid()
â”œâ”€â”€ Zettelkasten æ¦‚å¿µé—œä¿‚ï¼ˆPhase 2.1ï¼‰
â”‚   â”œâ”€â”€ find_concept_relations()         â­ æ ¸å¿ƒ
â”‚   â”œâ”€â”€ _classify_relation_type()        â­ é—œä¿‚åˆ¤å®š
â”‚   â”œâ”€â”€ _calculate_confidence()          â­ ä¿¡åº¦è©•åˆ†
â”‚   â”œâ”€â”€ _check_explicit_link()
â”‚   â”œâ”€â”€ _extract_shared_concepts_from_cards()
â”‚   â””â”€â”€ build_concept_network()
â”œâ”€â”€ å ±å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ generate_report()
â””â”€â”€ ZettelLinker (nested class)          (Phase 2.5)
    â””â”€â”€ link_zettel_to_papers()

ConceptMapper (concept_mapper.py:1020-1256)
â”œâ”€â”€ __init__(kb_path)
â”œâ”€â”€ build_network()
â”œâ”€â”€ analyze_all()                        â­ ä¸»åˆ†ææ–¹æ³•
â””â”€â”€ _generate_report()

è¼”åŠ©é¡:
â”œâ”€â”€ ConceptNetwork (concept_mapper.py:59-117)
â”œâ”€â”€ CommunityDetector (concept_mapper.py:120-284)
â”œâ”€â”€ PathAnalyzer (concept_mapper.py:287-410)
â”œâ”€â”€ CentralityAnalyzer (concept_mapper.py:413-593)
â””â”€â”€ NetworkVisualizer (concept_mapper.py:596-1017)
```

### æ•¸æ“šæµ

```
çŸ¥è­˜åº« (SQLite)
    â†“
RelationFinder.find_concept_relations()
    â†“
    â”œâ”€â†’ VectorDatabase.find_similar_zettel()  (å‘é‡ç›¸ä¼¼åº¦)
    â”œâ”€â†’ _classify_relation_type()              (é—œä¿‚é¡å‹)
    â”œâ”€â†’ _calculate_confidence()                (ä¿¡åº¦è©•åˆ†)
    â””â”€â†’ List[ConceptRelation]
        â†“
RelationFinder.build_concept_network()
    â†“
    ConceptNetwork (nodes, edges, relations)
    â†“
ConceptMapper.analyze_all()
    â”œâ”€â†’ CommunityDetector
    â”œâ”€â†’ PathAnalyzer
    â”œâ”€â†’ CentralityAnalyzer
    â””â”€â†’ NetworkVisualizer
        â†“
    è¼¸å‡º: HTML + DOT + JSON + Markdown
```

---

## RelationFinder è©³ç´°å¯¦ç¾

### åˆå§‹åŒ–æµç¨‹

**ä»£ç¢¼ä½ç½®**: `relation_finder.py:86-108`

```python
class RelationFinder:
    def __init__(self, kb_path: str = "knowledge_base", config: Optional[Dict] = None):
        # 1. åˆå§‹åŒ–çŸ¥è­˜åº«ç®¡ç†å™¨
        self.kb = KnowledgeBaseManager(kb_root=kb_path)

        # 2. åŠ è¼‰é…ç½®ï¼ˆé»˜èªå€¼æˆ–è‡ªå®šç¾©ï¼‰
        self.config = config or self._default_config()

        # 3. è¨­ç½®æ•¸æ“šåº«è·¯å¾‘
        self.db_path = Path(kb_path) / "index.db"

        # 4. åˆå§‹åŒ– Zettel åˆ†æå™¨ï¼ˆPhase 2.1 ä¾è³´ï¼‰
        self.zettel_analyzer = ZettelConceptAnalyzer(kb_path=kb_path)

        # 5. åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«ï¼ˆå¯èƒ½å¤±æ•—ï¼Œéœ€è¦ chromadbï¼‰
        try:
            self.vector_db = VectorDatabase(persist_directory="chroma_db")
        except Exception as e:
            print(f"Warning: Could not initialize vector database: {e}")
            self.vector_db = None  # Graceful degradation

    def _default_config(self) -> Dict:
        """é»˜èªé…ç½®å€¼"""
        return {
            'title_similarity_threshold': 0.65,    # å¼•ç”¨é—œä¿‚çš„æ¨™é¡Œç›¸ä¼¼åº¦é–¾å€¼
            'co_author_min_papers': 2,             # å…±åŒä½œè€…æœ€å°‘è«–æ–‡æ•¸
            'concept_min_frequency': 2,            # æ¦‚å¿µå…±ç¾æœ€å°‘è«–æ–‡æ•¸
            'year_range': 5,                       # å¹´ä»½ç¯„åœï¼ˆæœªä½¿ç”¨ï¼‰
        }
```

**åˆå§‹åŒ–æª¢æŸ¥æ¸…å–®**:
- âœ… çŸ¥è­˜åº«å­˜åœ¨ä¸”å¯è¨ªå•
- âœ… `chroma_db/` ç›®éŒ„å­˜åœ¨æˆ–å¯å‰µå»º
- âœ… `knowledge_base/index.db` å­˜åœ¨
- âš ï¸ ChromaDB å¯èƒ½åˆå§‹åŒ–å¤±æ•—ï¼ˆéœ€è¦ pip install chromadbï¼‰

### find_concept_relations æ ¸å¿ƒç®—æ³•

**ä»£ç¢¼ä½ç½®**: `relation_finder.py:396-560`

#### ç¬¬ä¸€æ­¥ï¼šè®€å–æ‰€æœ‰ Zettelkasten å¡ç‰‡

```python
def find_concept_relations(
    self,
    min_similarity: float = 0.4,
    relation_types: Optional[List[str]] = None,
    limit: int = 100
) -> List[ConceptRelation]:
    """è­˜åˆ¥ Zettelkasten å¡ç‰‡é–“çš„èªç¾©é—œä¿‚

    åƒæ•¸:
        min_similarity: æœ€å°å‘é‡ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆ0.0-1.0ï¼‰
        relation_types: é—œä¿‚é¡å‹éæ¿¾ï¼ˆNone = å…¨éƒ¨ï¼‰
        limit: æ¯å¼µå¡ç‰‡çš„æœ€å¤§ç›¸ä¼¼å¡ç‰‡æª¢æŸ¥æ•¸

    è¿”å›:
        List[ConceptRelation]: æŒ‰ä¿¡åº¦æ’åºçš„é—œä¿‚åˆ—è¡¨
    """
    if not self.vector_db:
        print("Error: Vector database not initialized")
        return []

    print("\n" + "="*70)
    print("[Phase 2.1] Zettelkasten æ¦‚å¿µé—œä¿‚è­˜åˆ¥")
    print("="*70)

    # æ­¥é©Ÿ 1ï¼šè®€å–æ‰€æœ‰å¡ç‰‡
    print("\n[1] è®€å– Zettelkasten å¡ç‰‡...")
    try:
        conn = sqlite3.connect(str(self.db_path))
        cursor = conn.cursor()

        # æŸ¥è©¢æ‰€æœ‰å¿…è¦çš„æ¬„ä½
        cursor.execute("""
            SELECT zettel_id, title, core_concept, tags, domain, paper_id, content, ai_notes, human_notes
            FROM zettel_cards
            ORDER BY zettel_id
        """)

        cards = []
        for row in cursor.fetchall():
            cards.append({
                'zettel_id': row[0],      # å¡ç‰‡ IDï¼Œå¦‚ "CogSci-20251028-001"
                'title': row[1],          # å¡ç‰‡æ¨™é¡Œ
                'core_concept': row[2],   # æ ¸å¿ƒæ¦‚å¿µï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦ï¼‰
                'tags': row[3],           # æ¨™ç±¤ï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦ï¼‰
                'domain': row[4],         # é ˜åŸŸ
                'paper_id': row[5],       # é—œè¯è«–æ–‡
                'content': row[6],        # å®Œæ•´å…§å®¹
                'ai_notes': row[7],       # AI ç­†è¨˜ï¼ˆå„ªå…ˆæª¢æŸ¥é€£çµï¼‰
                'human_notes': row[8]     # äººé¡ç­†è¨˜
            })
        conn.close()
    except Exception as e:
        print(f"Error reading cards: {e}")
        return []

    print(f"   æ‰¾åˆ° {len(cards)} å¼µå¡ç‰‡")
```

**æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥**:
```python
# æª¢æŸ¥æ˜¯å¦æœ‰å¡ç‰‡è®€å–æˆåŠŸ
if not cards:
    print("Warning: æœªæ‰¾åˆ°ä»»ä½• Zettelkasten å¡ç‰‡ï¼")
    return []

# æª¢æŸ¥å‘é‡æ•¸æ“šåº«æ˜¯å¦æœ‰å°æ‡‰å‘é‡
if len(cards) > 0:
    try:
        test_result = self.vector_db.find_similar_zettel(cards[0]['zettel_id'], n_results=1)
        if not test_result or 'ids' not in test_result:
            print("Warning: å‘é‡æ•¸æ“šåº«ç‚ºç©ºæˆ–æœªåˆå§‹åŒ–ï¼")
            return []
    except Exception as e:
        print(f"Error testing vector DB: {e}")
        return []
```

#### ç¬¬äºŒæ­¥ï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢

**ä»£ç¢¼ä½ç½®**: `relation_finder.py:463-509`

```python
# æ­¥é©Ÿ 2ï¼šå°æ¯å¼µå¡ç‰‡æ‰¾ç›¸ä¼¼å¡ç‰‡
print(f"\n[2] ä½¿ç”¨å‘é‡æœç´¢å°‹æ‰¾ç›¸ä¼¼å¡ç‰‡...")
relations = []
processed_pairs = set()  # é¿å…é‡è¤‡è™•ç† (A, B) å’Œ (B, A)

for i, card in enumerate(cards):
    # é€²åº¦å ±å‘Šï¼ˆæ¯ 50 å¼µï¼‰
    if (i + 1) % 50 == 0:
        print(f"   é€²åº¦: {i+1}/{len(cards)} å¡ç‰‡")

    card_id = card['zettel_id']

    # é—œéµæ­¥é©Ÿï¼šå‘é‡ç›¸ä¼¼åº¦æœç´¢
    try:
        similar_results = self.vector_db.find_similar_zettel(
            zettel_id=card_id,
            n_results=min(limit, len(cards) - 1),  # æœ€å¤šæª¢æŸ¥ limit å€‹
            exclude_self=True  # æ’é™¤è‡ªå·±
        )
    except Exception as e:
        print(f"   Warning: Failed to find similar cards for {card_id}: {e}")
        continue

    # è™•ç†çµæœæ ¼å¼
    if not similar_results or 'ids' not in similar_results:
        continue

    # æª¢æŸ¥çµæœæ˜¯å¦ç‚ºç©º
    if not similar_results['ids'] or len(similar_results['ids']) == 0:
        continue
    if not similar_results['ids'][0] or len(similar_results['ids'][0]) == 0:
        continue

    # è™•ç†æ¯å€‹ç›¸ä¼¼å¡ç‰‡
    for j, similar_id in enumerate(similar_results['ids'][0]):
        # è·³éè‡ªå·±
        if similar_id == card_id:
            continue

        # é¿å…é‡è¤‡è™•ç†
        pair = tuple(sorted([card_id, similar_id]))
        if pair in processed_pairs:
            continue
        processed_pairs.add(pair)

        # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆChromaDB è¿”å›è·é›¢ï¼‰
        # è·é›¢ distance å’Œç›¸ä¼¼åº¦çš„é—œä¿‚ï¼š
        # å‘é‡ç›¸ä¼¼åº¦ (cosine) = 1 - distance
        similarity = 1.0 - similar_results['distances'][0][j]

        # æ‡‰ç”¨é–¾å€¼éæ¿¾
        if similarity < min_similarity:
            continue

        # æ‰¾åˆ°å°æ‡‰çš„å¡ç‰‡æ•¸æ“š
        similar_card = next((c for c in cards if c['zettel_id'] == similar_id), None)
        if not similar_card:
            continue

        # ===== ä»¥ä¸‹é€²å…¥é—œä¿‚åˆ†ææµç¨‹ =====
```

**å‘é‡æœç´¢çµæœæ ¼å¼**:
```python
# VectorDatabase.find_similar_zettel() è¿”å›æ ¼å¼
similar_results = {
    'ids': [[id1, id2, id3, ...]],        # ç›¸ä¼¼å¡ç‰‡ ID åˆ—è¡¨
    'documents': [[doc1, doc2, ...]],     # å°æ‡‰æ–‡æª”å…§å®¹
    'distances': [[dist1, dist2, ...]],   # è·é›¢å€¼ï¼ˆ0=ç›¸åŒï¼Œ1=å®Œå…¨ä¸åŒï¼‰
    'metadatas': [[meta1, meta2, ...]]    # å…ƒæ•¸æ“š
}

# ChromaDB è·é›¢èˆ‡ç›¸ä¼¼åº¦çš„è½‰æ›
distance = 0.3       # ChromaDB è¿”å›
similarity = 1.0 - distance  # = 0.7
```

#### ç¬¬ä¸‰æ­¥ï¼šé—œä¿‚é¡å‹åˆ†é¡

**ä»£ç¢¼ä½ç½®**: `relation_finder.py:562-623`

```python
# åˆ¤å®šé—œä¿‚é¡å‹
relation_type = self._classify_relation_type(
    card, similar_card, similarity
)
```

**å®Œæ•´åˆ¤å®šé‚è¼¯**:

```python
def _classify_relation_type(
    self,
    card1: Dict,
    card2: Dict,
    similarity: float
) -> str:
    """åˆ¤å®šå…©å¼µå¡ç‰‡é–“çš„é—œä¿‚é¡å‹

    å„ªå…ˆé †åº:
    1. æª¢æŸ¥æ˜ç¢ºé€£çµï¼ˆæœ€å¯é ï¼‰
    2. æª¢æŸ¥å°æ¯”é—œéµè©
    3. æª¢æŸ¥ä¸Šä¸‹ä½é—œéµè©
    4. åŸºæ–¼ç›¸ä¼¼åº¦åˆ¤å®š
    """
    content1 = card1.get('content', '').lower()
    content2 = card2.get('content', '').lower()
    card2_id = card2.get('zettel_id', '')

    # ===== å„ªå…ˆç´š 1ï¼šæª¢æŸ¥æ˜ç¢ºé€£çµ =====
    if f'[[{card2_id}]]' in card1.get('content', ''):
        # æª¢æŸ¥é€£çµå‘¨åœçš„ä¸Šä¸‹æ–‡
        if '-->' in content1 or 'å°å‘' in content1 or 'leads to' in content1:
            return 'leads_to'
        elif '<--' in content1 or 'åŸºæ–¼' in content1 or 'based on' in content1:
            return 'based_on'

    # ===== å„ªå…ˆç´š 2ï¼šæª¢æŸ¥å°æ¯”é—œéµè© =====
    contrast_keywords = ['ä½†', 'ç„¶è€Œ', 'ç›¸å', 'å°æ¯”', 'however', 'but', 'contrast', 'differ']
    if any(kw in content1 or kw in content2 for kw in contrast_keywords):
        return 'contrasts_with'

    # ===== å„ªå…ˆç´š 3ï¼šæª¢æŸ¥ä¸Šä¸‹ä½é—œéµè© =====
    superclass_keywords = ['åŒ…å«', 'æŠ½è±¡', 'æ³›æŒ‡', 'include', 'general', 'abstract', 'superclass']
    subclass_keywords = ['å…·é«”', 'ç‰¹ä¾‹', 'å¯¦ä¾‹', 'specific', 'instance', 'example', 'subclass']

    if any(kw in content1 for kw in superclass_keywords):
        return 'superclass_of'
    if any(kw in content1 for kw in subclass_keywords):
        return 'subclass_of'

    # ===== å„ªå…ˆç´š 4ï¼šåŸºæ–¼ç›¸ä¼¼åº¦åˆ¤å®š =====
    if similarity >= 0.7:
        # é«˜ç›¸ä¼¼åº¦ â†’ ç›¸é—œæ¦‚å¿µ
        return 'related_to'
    elif similarity >= 0.5:
        # ä¸­ç­‰ç›¸ä¼¼åº¦ â†’ æª¢æŸ¥æ–¹å‘æ€§é—œéµè©
        directional_keywords = ['å› æ­¤', 'æ‰€ä»¥', 'å°è‡´', 'therefore', 'thus', 'result']
        if any(kw in content1 for kw in directional_keywords):
            return 'leads_to'
        return 'related_to'
    else:
        # ä½ç›¸ä¼¼åº¦ â†’ é»˜èªç›¸é—œ
        return 'related_to'
```

**é—œéµè©åŒ¹é…çš„ç¼ºé™·**:

âš ï¸ **å•é¡Œ 1ï¼šç„¡ NLP åˆ†æ**
```python
# ç•¶å‰åšæ³•ï¼šç°¡å–®å­—ç¬¦ä¸²åŒ¹é…
if 'å°å‘' in content1:
    return 'leads_to'

# å¯èƒ½çš„èª¤åˆ¤ï¼š
content = "å°å‘å…‰ç·šçš„åè½‰"  # ç‰©ç†è©å½™ï¼Œèª¤è­˜åˆ¥ç‚ºã€Œå°å‘ã€é—œéµè©
```

âœ… **æ”¹é€²æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨æ›´ç²¾ç¢ºçš„é—œéµè©
leads_to_patterns = [
    r'å°å‘\w+æ¦‚å¿µ',       # å°å‘[æŸæ¦‚å¿µ]
    r'(?<![^â†’])\s*â†’\s*',  # ç®­é ­ç¬¦è™Ÿ
    r'(?:leads|leads to)',  # è‹±æ–‡çŸ­èª
]

def _check_relation_keyword(text, patterns):
    import re
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False
```

âš ï¸ **å•é¡Œ 2ï¼šå¤šé‡æ¦‚å¿µçš„æ­§ç¾©**
```python
# ç•¶å‰åšæ³•
content = "ç¥ç¶“ç¶²çµ¡åŒ…å«å·ç©å’Œéè¿´å…©ç¨®é¡å‹"
if 'åŒ…å«' in content:
    return 'superclass_of'

# ä½†å¯¦éš›ä¸Š card1 å¯èƒ½ä¸æ˜¯ card2 çš„ä¸Šä½æ¦‚å¿µ
# åªæ˜¯è¨è«–äº†ã€ŒåŒ…å«ã€çš„æ¦‚å¿µ
```

âœ… **æ”¹é€²æ–¹æ¡ˆ**:
```python
# æª¢æŸ¥é—œéµè©æ˜¯å¦èˆ‡ card2 ç›¸é—œ
if 'åŒ…å«' in content1:
    # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æåˆ° card2
    card2_title = card2.get('title', '').lower()
    if card2_title in content1:  # å…§å®¹ç¢ºå¯¦è¨è«–äº† card2
        # æª¢æŸ¥ card2 æ˜¯å¦åœ¨ã€ŒåŒ…å«ã€ä¹‹å¾Œ
        match = re.search(r'åŒ…å«[^ã€‚]*' + re.escape(card2_title), content1)
        if match:
            return 'superclass_of'
```

#### ç¬¬å››æ­¥ï¼šä¿¡åº¦è©•åˆ†è¨ˆç®—

**ä»£ç¢¼ä½ç½®**: `relation_finder.py:625-672`

```python
# è¨ˆç®—ä¿¡åº¦
confidence = self._calculate_confidence(
    card, similar_card, similarity, relation_type
)
```

**å®Œæ•´å¯¦ç¾**:

```python
def _calculate_confidence(
    self,
    card1: Dict,
    card2: Dict,
    similarity: float,
    relation_type: str
) -> float:
    """è¨ˆç®—é—œä¿‚çš„å¤šç¶­åº¦ä¿¡åº¦è©•åˆ†

    è©•åˆ†ç¶­åº¦:
    1. semantic_similarity (40%): å‘é‡ç›¸ä¼¼åº¦
    2. link_explicit (30%): æ˜ç¢ºé€£çµå­˜åœ¨
    3. co_occurrence (20%): å…±åŒæ¦‚å¿µæ•¸é‡
    4. domain_consistency (10%): é ˜åŸŸä¸€è‡´æ€§

    è¿”å›å€¼: 0.0-1.0ï¼Œè¶Šé«˜è¶Šå¯ä¿¡
    """
    scores = {}

    # ===== ç¶­åº¦ 1ï¼šèªç¾©ç›¸ä¼¼åº¦ (40%) =====
    scores['semantic_similarity'] = similarity * 0.4
    # ç›´æ¥ä½¿ç”¨å‘é‡æ¨¡å‹çš„ç›¸ä¼¼åº¦
    # ç¯„åœï¼š0-0.4ï¼ˆæœ€å¤šè²¢ç» 40%ï¼‰

    # ===== ç¶­åº¦ 2ï¼šæ˜ç¢ºé€£çµ (30%) =====
    has_explicit_link = self._check_explicit_link(card1, card2.get('zettel_id', ''))
    scores['link_explicit'] = 0.3 if has_explicit_link else 0.0
    # äºŒå€¼é¸æ“‡ï¼šæœ‰é€£çµ 0.3ï¼Œç„¡é€£çµ 0.0

    # ===== ç¶­åº¦ 3ï¼šå…±åŒæ¦‚å¿µ (20%) =====
    shared = self._extract_shared_concepts_from_cards(card1, card2)
    # æ­£è¦åŒ–ï¼š5 å€‹ä»¥ä¸Šå…±åŒæ¦‚å¿µå¾—æ»¿åˆ†
    shared_score = min(len(shared) / 5.0, 1.0) * 0.2
    scores['co_occurrence'] = shared_score
    # ç¯„åœï¼š0-0.2ï¼ˆæœ€å¤šè²¢ç» 20%ï¼‰
    # ä¾‹ï¼š3 å€‹å…±åŒæ¦‚å¿µ â†’ 3/5 * 0.2 = 0.12

    # ===== ç¶­åº¦ 4ï¼šé ˜åŸŸä¸€è‡´æ€§ (10%) =====
    domain1 = card1.get('domain', '')
    domain2 = card2.get('domain', '')
    domain_consistent = (domain1 == domain2) if domain1 and domain2 else False
    scores['domain_consistency'] = 0.1 if domain_consistent else 0.05
    # åŒé ˜åŸŸ 0.1ï¼Œä¸åŒ/ç¼ºå¤± 0.05

    # ===== ç¸½åˆ†è¨ˆç®— =====
    total_score = sum(scores.values())

    # èª¿è©¦è¼¸å‡ºï¼ˆå¯é¸ï¼‰
    if total_score > 0.8:  # é«˜ä¿¡åº¦æ™‚è©³ç´°æ‰“å°
        print(f"[é«˜ä¿¡åº¦] {card1['zettel_id']} â†” {card2['zettel_id']}")
        for dim, score in scores.items():
            print(f"  {dim}: {score:.3f}")
        print(f"  ç¸½åˆ†: {total_score:.3f}\n")

    return round(total_score, 3)
```

**å…±åŒæ¦‚å¿µæå–è©³è§£**:

```python
def _extract_shared_concepts_from_cards(self, card1: Dict, card2: Dict) -> List[str]:
    """æå–å…©å¼µå¡ç‰‡çš„å…±åŒæ¦‚å¿µ

    ä¾†æºå„ªå…ˆé †åº:
    1. tagsï¼ˆæ¨™ç±¤ï¼Œæœ€æº–ç¢ºï¼‰
    2. core_conceptï¼ˆæ ¸å¿ƒæ¦‚å¿µï¼Œæ¬¡æº–ç¢ºï¼‰
    3. titleï¼ˆæ¨™é¡Œï¼Œæœ€ç°¡æ½”ï¼‰
    """

    def extract_concepts(card: Dict) -> Set[str]:
        concepts = set()

        # ===== ä¾†æº 1ï¼šå¾æ¨™ç±¤æå– =====
        tags = card.get('tags', '')
        if tags:
            try:
                if isinstance(tags, str):
                    if tags.startswith('['):
                        # JSON æ ¼å¼: ["tag1", "tag2"]
                        tag_list = json.loads(tags)
                    else:
                        # CSV æ ¼å¼: "tag1, tag2"
                        tag_list = [t.strip() for t in tags.split(',')]

                    # ç›´æ¥ç´å…¥ï¼ˆå·²æ˜¯å®Œæ•´è©å½™ï¼‰
                    concepts.update(tag_list)
                elif isinstance(tags, list):
                    concepts.update(tags)
            except:
                pass

        # ===== ä¾†æº 2ï¼šå¾æ ¸å¿ƒæ¦‚å¿µæå–é—œéµè© =====
        core = card.get('core_concept', '')
        if core:
            # åˆ†è©ï¼šç§»é™¤æ¨™é»ï¼ŒæŒ‰ç©ºæ ¼åˆ†å‰²
            words = re.findall(r'\w+', core.lower())

            # éæ¿¾åœç”¨è©ï¼šåªä¿ç•™é•·åº¦ >= 3 çš„è©
            keywords = [w for w in words if len(w) >= 3]

            # ä¾‹ï¼šã€ŒèªçŸ¥ç§‘å­¸ä¸­çš„è¦–è¦ºè™•ç†ã€
            # â†’ åˆ†è©ï¼š['èªçŸ¥', 'ç§‘å­¸', 'ä¸­çš„', 'è¦–è¦º', 'è™•ç†']
            # â†’ éæ¿¾ï¼š['èªçŸ¥', 'ç§‘å­¸', 'è¦–è¦º', 'è™•ç†']ï¼ˆæ’é™¤ã€Œä¸­çš„ã€ï¼‰

            concepts.update(keywords)

        # ===== ä¾†æº 3ï¼šå¾æ¨™é¡Œæå–é—œéµè© =====
        title = card.get('title', '')
        if title:
            words = re.findall(r'\w+', title.lower())
            keywords = [w for w in words if len(w) >= 3]
            concepts.update(keywords)

        return concepts

    # è¨ˆç®—äº¤é›†
    concepts1 = extract_concepts(card1)
    concepts2 = extract_concepts(card2)
    shared = concepts1 & concepts2  # é›†åˆäº¤é›†

    return sorted(list(shared))
```

**ä¸­è‹±æ–‡åˆ†è©çš„å•é¡Œ**:

âš ï¸ **å•é¡Œï¼šç°¡å–®æ­£å‰‡åˆ†è©ç„¡æ³•æ­£ç¢ºè™•ç†ä¸­æ–‡**

```python
# ç•¶å‰å¯¦ç¾
words = re.findall(r'\w+', "è¦–è¦ºç³»çµ±çš„çµæ§‹èˆ‡åŠŸèƒ½")
# çµæœï¼š['è¦–', 'è¦º', 'ç³»', 'çµ±', 'çš„', 'çµ', 'æ§‹', 'èˆ‡', 'åŠŸ', 'èƒ½']
# âŒ éŒ¯èª¤ï¼šé€å­—ç¬¦åˆ†å‰²ï¼Œä¸Ÿå¤±è©ç¾©

# æ‡‰ç‚º
# âœ… æ­£ç¢ºï¼š['è¦–è¦º', 'ç³»çµ±', 'çµæ§‹', 'åŠŸèƒ½']
```

âœ… **æ”¹é€²æ–¹æ¡ˆ**:
```python
def extract_concepts_advanced(card: Dict) -> Set[str]:
    """æ”¹é€²çš„æ¦‚å¿µæå–ï¼Œæ”¯æ´æ›´å¥½çš„ä¸­è‹±æ–‡åˆ†è©"""
    concepts = set()

    # æ–¹æ¡ˆ Aï¼šä½¿ç”¨é å®šç¾©çš„è©å½™åº«ï¼ˆè¼•é‡ç´šï¼‰
    important_keywords = {
        'è¦–è¦º': ['è¦–è¦º', 'è¦–è¦ºç³»çµ±', 'è¦–è¦ºçš®å±¤'],
        'èªçŸ¥': ['èªçŸ¥', 'èªçŸ¥ç§‘å­¸', 'èªçŸ¥æ¨¡å‹'],
        # ... æ›´å¤šé å®šç¾©è©å½™
    }

    # æ–¹æ¡ˆ Bï¼šä½¿ç”¨ jieba åˆ†è©ï¼ˆå¦‚æœè£æœ‰ï¼‰
    try:
        import jieba
        core = card.get('core_concept', '')
        words = list(jieba.cut(core))
        keywords = [w for w in words if len(w) >= 2 and w not in ['çš„', 'èˆ‡', 'åŠ', 'æˆ–']]
        concepts.update(keywords)
    except ImportError:
        pass  # Fallback åˆ°ç°¡å–®åˆ†è©

    return concepts
```

#### ç¬¬äº”æ­¥ï¼šå…±åŒæ¦‚å¿µé©—è­‰èˆ‡å…¶ä»–æª¢æŸ¥

```python
# ç²å–å…±åŒæ¦‚å¿µ
shared_concepts = self._extract_shared_concepts_from_cards(card, similar_card)

# å¯é¸çš„é€²ä¸€æ­¥éæ¿¾
# ä¾‹ï¼šå¦‚æœå…±åŒæ¦‚å¿µå¤ªå¤šï¼Œå¯èƒ½æ˜¯è¤‡è£½å…§å®¹
if len(shared_concepts) > 20:
    # å¯èƒ½æ˜¯é‡è¤‡æˆ–éå¸¸ç›¸é—œçš„å¡ç‰‡
    # æ ¹æ“šéœ€è¦èª¿æ•´ä¿¡åº¦æˆ–æ¨™è¨˜ç‚ºéœ€è¦äººå·¥å¯©æŸ¥
    pass

# æª¢æŸ¥æ˜ç¢ºé€£çµï¼ˆç”¨æ–¼ä¿¡åº¦è¨ˆç®—ï¼‰
link_explicit = self._check_explicit_link(card, similar_id)
```

**é€£çµæª¢æŸ¥çš„å¯¦ç¾**:

```python
def _check_explicit_link(self, card: Dict, target_id: str) -> bool:
    """æª¢æŸ¥å¡ç‰‡ä¸­æ˜¯å¦æœ‰æŒ‡å‘ç›®æ¨™å¡ç‰‡çš„æ˜ç¢ºé€£çµ

    å„ªå…ˆç´š:
    1. ai_notesï¼ˆå·²æ·¨åŒ–çš„ AI å…§å®¹ï¼‰
    2. contentï¼ˆå®Œæ•´å…§å®¹ï¼Œéœ€éæ¿¾äººé¡ç­†è¨˜ï¼‰

    é€£çµæ ¼å¼: [[target_id]]
    """

    # å„ªå…ˆä½¿ç”¨ ai_notes
    ai_notes = card.get('ai_notes')
    if ai_notes:
        # ai_notes å·²ç¶“æ˜¯ç´” AI å…§å®¹ï¼Œç›´æ¥ä½¿ç”¨
        ai_content = ai_notes
    else:
        # Fallbackï¼šå¾ content æå– AI å…§å®¹
        content = card.get('content', '')

        # æå– AI å…§å®¹ï¼ˆéæ¿¾äººé¡ç­†è¨˜ï¼‰
        # äººé¡ç­†è¨˜æ ¼å¼ï¼š**[Human]**: (TODO) ...
        # AI å…§å®¹æ ¼å¼ï¼š**[AI Agent]**: ...
        ai_content = extract_ai_content(content)
        # æ­¤å‡½æ•¸éœ€è¦è‡ªè¡Œå¯¦ç¾ï¼Œä¾‹å¦‚ï¼š
        #   ai_lines = [line for line in content.split('\n')
        #               if '[AI Agent]' in line]
        #   ai_content = '\n'.join(ai_lines)

    # æª¢æŸ¥ Obsidian Wiki Links
    # æ ¼å¼ï¼š[[target_id]]
    return f'[[{target_id}]]' in ai_content
```

#### ç¬¬å…­æ­¥ï¼šå‰µå»º ConceptRelation å°è±¡

```python
# å‰µå»ºé—œä¿‚å°è±¡
relation = ConceptRelation(
    card_id_1=card_id,
    card_id_2=similar_id,
    card_title_1=card['title'],
    card_title_2=similar_card['title'],
    relation_type=relation_type,
    confidence_score=confidence,
    semantic_similarity=similarity,
    link_explicit=link_explicit,
    shared_concepts=shared_concepts,
    paper_ids=[card['paper_id'], similar_card['paper_id']]
)
relations.append(relation)
```

**ConceptRelation æ•¸æ“šçµæ§‹**:

```python
@dataclass
class ConceptRelation:
    """Zettelkasten æ¦‚å¿µé—œä¿‚"""
    card_id_1: str                 # å¡ç‰‡ A ID
    card_id_2: str                 # å¡ç‰‡ B ID
    card_title_1: str              # å¡ç‰‡ A æ¨™é¡Œ
    card_title_2: str              # å¡ç‰‡ B æ¨™é¡Œ
    relation_type: str             # 6 ç¨®é—œä¿‚ä¹‹ä¸€
    confidence_score: float        # ä¿¡åº¦ 0.0-1.0
    semantic_similarity: float     # å‘é‡ç›¸ä¼¼åº¦
    link_explicit: bool            # æ˜¯å¦æœ‰æ˜ç¢ºé€£çµ
    shared_concepts: List[str]     # å…±åŒæ¦‚å¿µåˆ—è¡¨
    paper_ids: List[int]           # é—œè¯è«–æ–‡ ID

    def __repr__(self) -> str:
        return (f"ConceptRelation({self.card_id_1} "
                f"--{self.relation_type}--> {self.card_id_2}, "
                f"conf={self.confidence_score:.2f})")
```

---

## ConceptMapper è©³ç´°å¯¦ç¾

### ç¶²çµ¡æ§‹å»ºèˆ‡ç´¢å¼•

**ä»£ç¢¼ä½ç½®**: `concept_mapper.py:59-117` (`ConceptNetwork` é¡)

```python
class ConceptNetwork:
    """æ¦‚å¿µç¶²çµ¡æ ¸å¿ƒé¡ï¼Œæä¾›é«˜æ•ˆæŸ¥è©¢"""

    def __init__(self, network_data: Dict):
        # 1. å­˜å„²åŸå§‹æ•¸æ“š
        self.nodes = network_data.get('nodes', [])
        self.edges = network_data.get('edges', [])
        self.statistics = network_data.get('statistics', {})
        self.hub_nodes = network_data.get('hub_nodes', [])
        self.relations = network_data.get('relations', [])

        # 2. å»ºç«‹ç´¢å¼•ï¼ˆåŠ é€ŸæŸ¥è©¢ï¼‰
        self._build_indices()

    def _build_indices(self):
        """å»ºç«‹ä¸‰ç¨®ç´¢å¼•çµæ§‹"""

        # ç´¢å¼• 1ï¼šç¯€é»å­—å…¸ï¼ˆO(1) æŸ¥è©¢ï¼‰
        self.node_dict = {node['card_id']: node for node in self.nodes}
        # ä¾‹ï¼šnode_dict['CogSci-001'] = {card_id, title, degree, ...}

        # ç´¢å¼• 2ï¼šé„°æ¥è¡¨ï¼ˆç„¡å‘åœ–ï¼‰
        self.adjacency = defaultdict(list)
        for edge in self.edges:
            self.adjacency[edge['source']].append(edge['target'])
            self.adjacency[edge['target']].append(edge['source'])  # å°ç¨±
        # ä¾‹ï¼šadjacency['CogSci-001'] = ['CogSci-002', 'CogSci-005', ...]

        # ç´¢å¼• 3ï¼šé‚Šå­—å…¸ï¼ˆO(1) é‚ŠæŸ¥è©¢ï¼‰
        self.edge_dict = {}
        for edge in self.edges:
            key1 = (edge['source'], edge['target'])
            key2 = (edge['target'], edge['source'])
            self.edge_dict[key1] = edge
            self.edge_dict[key2] = edge  # å°ç¨±
        # ä¾‹ï¼šedge_dict[('CogSci-001', 'CogSci-002')] = {source, target, ...}
```

### PageRank çš„æ•¸å€¼ç©©å®šæ€§

**ä»£ç¢¼ä½ç½®**: `concept_mapper.py:488-527`

```python
def _calculate_pagerank(
    self,
    damping: float = 0.85,        # é˜»å°¼å› å­ï¼ˆGoogle æ¨è–¦ï¼‰
    max_iterations: int = 100,    # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    tolerance: float = 1e-6       # æ”¶æ–‚å®¹å·®
) -> Dict[str, float]:
    """è¿­ä»£è¨ˆç®— PageRank

    å…¬å¼:
    PR(A) = (1-d)/N + d Ã— Î£(PR(B)/out_degree(B))

    å…¶ä¸­:
    - d = damping (0.85)
    - N = ç¯€é»æ•¸
    - B = æŒ‡å‘ A çš„ç¯€é»
    """
    nodes = list(self.network.node_dict.keys())
    n = len(nodes)

    # åˆå§‹åŒ–
    ranks = {node: 1.0 / n for node in nodes}

    # è¿­ä»£å„ªåŒ–
    for iteration in range(max_iterations):
        new_ranks = {}
        max_diff = 0.0

        for node in nodes:
            rank_sum = 0.0

            # è¨ˆç®—ä¾†è‡ªé„°å±…çš„è²¢ç»
            for neighbor in self.network.get_neighbors(node):
                neighbor_degree = self.network.node_dict[neighbor]['degree']
                if neighbor_degree > 0:
                    # å¹³å‡åˆ†é…é„°å±…çš„ PageRank
                    rank_sum += ranks[neighbor] / neighbor_degree

            # PageRank å…¬å¼
            new_rank = (1 - damping) / n + damping * rank_sum
            new_ranks[node] = new_rank

            # æª¢æŸ¥æ”¶æ–‚
            diff = abs(new_rank - ranks[node])
            max_diff = max(max_diff, diff)

        # æ›´æ–°æ’å
        ranks = new_ranks

        # æå‰çµ‚æ­¢æ¢ä»¶
        if max_diff < tolerance:
            print(f"   PageRank åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£å¾Œæ”¶æ–‚")
            break

    return ranks
```

**æ•¸å€¼ç©©å®šæ€§å•é¡Œ**:

âš ï¸ **å•é¡Œï¼šå­¤ç«‹ç¯€é»çš„ PageRank**

```python
# ç•¶å‰å¯¦ç¾
# å¦‚æœç¯€é»ç„¡é„°å±…ï¼Œrank_sum = 0
new_rank = (1 - 0.85) / n + 0.85 * 0  = 0.15 / n

# ä¾‹ï¼š704 å€‹ç¯€é»ï¼Œå­¤ç«‹ç¯€é» PR = 0.15 / 704 â‰ˆ 0.0002
# å³ä½¿å­¤ç«‹ï¼Œä¹Ÿä¸æœƒè¢«å®Œå…¨å¿½è¦–ï¼ˆé‡è¦ï¼ï¼‰
```

âœ… **æ­£ç¢ºæ€§é©—è­‰**:
```python
# PageRank æ‡‰è©²æ»¿è¶³æ­¸ä¸€åŒ–æ¢ä»¶
sum(ranks.values()) â‰ˆ 1.0

# é©—è­‰ä»£ç¢¼
total_pr = sum(ranks.values())
if abs(total_pr - 1.0) > 1e-3:
    print(f"è­¦å‘Šï¼šPageRank æ­¸ä¸€åŒ–èª¤å·®: {total_pr}")
```

---

## å‘é‡æ•¸æ“šåº«é›†æˆ

### ChromaDB çš„åˆå§‹åŒ–å’ŒæŸ¥è©¢

**ä»£ç¢¼ä½ç½®**: `src/embeddings/vector_db.py` (not shown, but used by RelationFinder)

```python
class VectorDatabase:
    """ChromaDB çš„å°è£é¡"""

    def __init__(self, persist_directory="chroma_db"):
        """åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«"""
        import chromadb

        # æŒä¹…åŒ–æ¨¡å¼ï¼ˆæ•¸æ“šä¿å­˜åˆ°ç£ç›¤ï¼‰
        self.client = chromadb.Client(
            chromadb.config.Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=persist_directory,
                anonymized_telemetry=False
            )
        )

        # æˆ–ç°¡å–®æ¨¡å¼ï¼ˆå…§å­˜ï¼‰
        # self.client = chromadb.Client()

    def find_similar_zettel(
        self,
        zettel_id: str,
        n_results: int = 10,
        exclude_self: bool = True
    ) -> Dict:
        """å°‹æ‰¾ç›¸ä¼¼çš„ Zettelkasten å¡ç‰‡"""

        # ç²å– zettelkasten é›†åˆ
        collection = self.client.get_collection("zettelkasten")

        # æŸ¥è©¢
        results = collection.query(
            query_where={"zettel_id": {"$eq": zettel_id}},  # å…ˆæ‰¾è‡ªå·±
            n_results=1  # å–å‡ºè‡ªå·±çš„å‘é‡
        )

        if not results or not results['embeddings']:
            return None

        # ç²å–è‡ªå·±çš„å‘é‡
        query_embedding = results['embeddings'][0]

        # åŸ·è¡Œç›¸ä¼¼åº¦æœç´¢
        similar_results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results + (1 if exclude_self else 0),
            where_document={"$ne": zettel_id} if exclude_self else None
        )

        return similar_results
```

**è·é›¢åº¦é‡**:

```python
# ChromaDB é»˜èªä½¿ç”¨ L2 Euclidean è·é›¢
# è·é›¢ d âˆˆ [0, 2] for å–®ä½å‘é‡
# distance = 0    â†’ å®Œå…¨ç›¸åŒï¼ˆç›¸ä¼¼åº¦ 1.0ï¼‰
# distance = 1.0  â†’ æ­£äº¤ï¼ˆç›¸ä¼¼åº¦ 0.0ï¼‰
# distance = 2.0  â†’ ç›¸åæ–¹å‘ï¼ˆç›¸ä¼¼åº¦ -1.0ï¼Œä½†å¯¦éš›ä¸æœƒå‡ºç¾ï¼‰

# è½‰æ›åˆ° [0, 1] çš„ç›¸ä¼¼åº¦
similarity = 1.0 - (distance / 2.0)

# æˆ–ç›´æ¥ä½¿ç”¨ 1 - distanceï¼ˆå‡è¨­è·é›¢å·²æ­£è¦åŒ–ï¼‰
similarity = 1.0 - distance
```

---

## å¸¸è¦‹é™·é˜±å’Œè§£æ±ºæ–¹æ¡ˆ

### é™·é˜± 1ï¼šå‘é‡æ•¸æ“šåº«æœªåˆå§‹åŒ–

**ç—‡ç‹€**:
```
Error: Vector database not initialized
è¿”å›ç©ºåˆ—è¡¨ï¼Œç„¡é—œä¿‚è­˜åˆ¥
```

**åŸå› **:
```python
# find_concept_relations ä¸­çš„æª¢æŸ¥
if not self.vector_db:
    print("Error: Vector database not initialized")
    return []
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. å®‰è£ chromadb
pip install chromadb

# 2. ç”Ÿæˆå‘é‡åµŒå…¥
python generate_embeddings.py --provider gemini

# 3. é©—è­‰å‘é‡æ•¸æ“šåº«
python -c "
from src.embeddings.vector_db import VectorDatabase
db = VectorDatabase()
stats = db.get_stats()
print(f'Zettel å‘é‡æ•¸: {stats[\"zettel_count\"]}')
"
```

### é™·é˜± 2ï¼šå…±åŒæ¦‚å¿µè¨ˆç®—ä¸­çš„è©å½™éæ¿¾éåº¦

**ç—‡ç‹€**:
```
shared_concepts å§‹çµ‚ç‚ºç©ºæˆ–éå°‘
ä¿¡åº¦è©•åˆ†ä¸­å…±åŒæ¦‚å¿µç¶­åº¦ç¸½æ˜¯ 0
```

**åŸå› **:
```python
# è©å½™é•·åº¦éæ¿¾
keywords = [w for w in words if len(w) >= 3]

# ä¸­è‹±æ–‡æ··åˆæ™‚å•é¡Œæ˜é¡¯
# "èªçŸ¥" (2 å­—) è¢«æ’é™¤
# "learning" (8 å­—) è¢«ä¿ç•™
# å°è‡´ä¸­æ–‡è©å½™è¢«å¤§é‡éæ¿¾
```

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def extract_concepts_improved(card: Dict) -> Set[str]:
    """æ”¹é€²çš„æ¦‚å¿µæå–"""
    concepts = set()

    # 1. æ¨™ç±¤ç›´æ¥ç´å…¥ï¼ˆç„¡éœ€éæ¿¾ï¼‰
    tags = card.get('tags', '')
    if tags:
        try:
            if isinstance(tags, str) and tags.startswith('['):
                concept_list = json.loads(tags)
            else:
                concept_list = [t.strip() for t in tags.split(',')]
            concepts.update(concept_list)
        except:
            pass

    # 2. æ ¸å¿ƒæ¦‚å¿µï¼šèª¿æ•´éæ¿¾è¦å‰‡
    core = card.get('core_concept', '')
    if core:
        words = re.findall(r'\w+', core.lower())

        # æ”¹é€²ï¼šæ”¯æ´ä¸­è‹±æ··åˆ
        keywords = []
        for w in words:
            # ä¸­æ–‡ï¼šå…è¨± 1+ å­—
            # è‹±æ–‡ï¼šå…è¨± 2+ å­—
            if len(w) >= 2 or (len(w) >= 1 and ord(w[0]) > 127):
                keywords.append(w)

        concepts.update(keywords)

    # 3. æ¨™é¡ŒåŒæ¨£è™•ç†
    # ...

    return concepts
```

### é™·é˜± 3ï¼šé—œä¿‚é¡å‹åˆ†é¡çš„èª¤åˆ¤

**ç—‡ç‹€**:
```
é—œä¿‚é¡å‹ä¸ç¬¦åˆé æœŸ
ä¾‹ï¼šæ‡‰è©²æ˜¯ "leads_to"ï¼Œå»è¢«åˆ¤ç‚º "related_to"
```

**åŸå› **:
```python
# ç•¶å‰å¯¦ç¾ä¸­ï¼Œå„ªå…ˆç´šä¸å¤ æ˜ç¢º
if similarity >= 0.7:
    return 'related_to'  # éæ–¼å¯¬æ³›

# ä½†å¦‚æœæœ‰æ˜ç¢ºé€£çµæ‡‰è©²å„ªå…ˆåˆ¤å®š
```

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def _classify_relation_type_improved(
    self,
    card1: Dict,
    card2: Dict,
    similarity: float
) -> str:
    """æ”¹é€²çš„é—œä¿‚åˆ†é¡"""

    # ===== æœ€é«˜å„ªå…ˆç´šï¼šæ˜ç¢ºé€£çµ =====
    card2_id = card2.get('zettel_id', '')

    # æª¢æŸ¥å¤šç¨®é€£çµæ ¼å¼
    link_formats = [
        f'[[{card2_id}]]',           # Obsidian Wiki Link
        f'-> {card2_id}',             # ç®­é ­ç¬¦è™Ÿ
        f'=> {card2_id}',             # å‚™é¸ç®­é ­
    ]

    for link_fmt in link_formats:
        if link_fmt in card1.get('content', ''):
            # ç¢ºèªæ‰¾åˆ°é€£çµï¼Œé€²ä¸€æ­¥åˆ¤å®šæ–¹å‘
            return self._determine_link_direction(card1, card2_id)

    # ===== æ¬¡å„ªå…ˆç´šï¼šç‰¹å¾µé—œéµè© =====
    # ...ï¼ˆå…¶ä»–é‚è¼¯ï¼‰

def _determine_link_direction(self, card1: Dict, target_id: str) -> str:
    """æ ¹æ“šé€£çµçš„ä¸Šä¸‹æ–‡ç¢ºå®šæ–¹å‘"""
    content = card1.get('content', '').lower()

    # åœ¨é€£çµé™„è¿‘æŸ¥æ‰¾æ–¹å‘é—œéµè©
    import re
    context_size = 50  # æª¢æŸ¥å‰å¾Œ 50 å€‹å­—ç¬¦

    match = re.search(f'(.{{{context_size}}})\[\[{target_id}\]\](.{{{context_size}}})', content)
    if match:
        context = match.group(1) + match.group(2)

        # åˆ¤å®šæ–¹å‘
        if any(kw in context for kw in ['å°å‘', 'å°è‡´', 'leads', 'result']):
            return 'leads_to'
        elif any(kw in context for kw in ['åŸºæ–¼', 'åŸºç¤', 'based', 'foundation']):
            return 'based_on'

    return 'related_to'  # é»˜èª
```

### é™·é˜± 4ï¼šè¿´åœˆå°è‡´çš„é‡è¤‡è™•ç†

**ç—‡ç‹€**:
```
åŒä¸€å°å¡ç‰‡åœ¨ relations ä¸­å‡ºç¾å¤šæ¬¡
è™•ç†æ•ˆç‡ä½ä¸‹
```

**åŸå› **:
```python
# ç•¶å‰å¯¦ç¾ä½¿ç”¨äº† processed_pairs
processed_pairs = set()

for i, card in enumerate(cards):
    for j, similar_id in enumerate(...):
        # é¿å…é‡è¤‡
        pair = tuple(sorted([card_id, similar_id]))
        if pair in processed_pairs:
            continue
        processed_pairs.add(pair)
```

**é©—è­‰å’Œå„ªåŒ–**:
```python
# æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡
relation_pairs = set()
duplicates = []

for rel in relations:
    pair = tuple(sorted([rel.card_id_1, rel.card_id_2]))
    if pair in relation_pairs:
        duplicates.append(pair)
    relation_pairs.add(pair)

if duplicates:
    print(f"è­¦å‘Šï¼šæ‰¾åˆ° {len(duplicates)} å€‹é‡è¤‡é—œä¿‚å°")
    print(f"ä¾‹ï¼š{duplicates[0]}")

# å»é‡è™•ç†
unique_relations = {}
for rel in relations:
    pair = tuple(sorted([rel.card_id_1, rel.card_id_2]))
    if pair not in unique_relations or rel.confidence_score > unique_relations[pair].confidence_score:
        unique_relations[pair] = rel

relations = list(unique_relations.values())
```

---

## æ€§èƒ½å„ªåŒ–æŠ€å·§

### å„ªåŒ– 1ï¼šå‘é‡æœç´¢çš„æ‰¹é‡æŸ¥è©¢

**ç•¶å‰å¯¦ç¾**ï¼ˆé€å€‹æŸ¥è©¢ï¼‰:
```python
for card in cards:
    similar_results = self.vector_db.find_similar_zettel(card['zettel_id'])
    # ç¸½æ™‚é–“ï¼šO(n) æ¬¡æŸ¥è©¢
```

**å„ªåŒ–æ–¹æ¡ˆ**ï¼ˆæ‰¹é‡æŸ¥è©¢ï¼‰:
```python
def find_all_similar_batch(self, card_ids: List[str], batch_size: int = 50):
    """æ‰¹é‡å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
    all_results = []

    for batch_start in range(0, len(card_ids), batch_size):
        batch_ids = card_ids[batch_start:batch_start + batch_size]

        # ChromaDB æ”¯æ´å¤šæŸ¥è©¢
        embeddings = self.vector_db.get_embeddings(batch_ids)
        results = self.vector_db.collection.query(
            query_embeddings=embeddings,
            n_results=100
        )

        all_results.extend(results)

    return all_results

# ä½¿ç”¨
similar_results_all = finder.find_all_similar_batch(
    [card['zettel_id'] for card in cards],
    batch_size=50
)
```

**æ€§èƒ½æå‡**: ~30-50% æ™‚é–“æ¸›å°‘

### å„ªåŒ– 2ï¼šç¤¾ç¾¤æª¢æ¸¬çš„å¿«é€Ÿè¿‘ä¼¼

**ç•¶å‰å¯¦ç¾**ï¼ˆLouvain ç²¾ç¢ºç®—æ³•ï¼‰:
```python
# æ™‚é–“è¤‡é›œåº¦ï¼šO(n Ã— max_iter)
# 704 ç¯€é» Ã— 10 æ¬¡è¿­ä»£ â‰ˆ å¯æ¥å—
communities = detector.detect_communities(method='louvain')
```

**å¿«é€Ÿè¿‘ä¼¼æ–¹æ¡ˆ**ï¼ˆé€£é€šåˆ†é‡ï¼‰:
```python
# æ™‚é–“è¤‡é›œåº¦ï¼šO(n + m)ï¼Œå¿«é€Ÿå¾—å¤š
communities = detector.detect_communities(method='simple')

# ç”¨æ–¼å¤§è¦æ¨¡ç¶²çµ¡
if len(nodes) > 2000:
    print("ç¯€é»éå¤šï¼Œä½¿ç”¨å¿«é€Ÿè¿‘ä¼¼...")
    communities = self._detect_by_connected_components()
else:
    communities = self._detect_by_louvain()
```

### å„ªåŒ– 3ï¼šæ¡æ¨£åŠ é€Ÿä»‹æ•¸ä¸­å¿ƒæ€§

**ç•¶å‰å¯¦ç¾**ï¼ˆéš¨æ©Ÿæ¡æ¨£ 50 å°ï¼‰:
```python
sample_size = min(50, len(nodes))
# é©åˆä¸­ç­‰è¦æ¨¡ç¶²çµ¡ï¼ˆ<500 ç¯€é»ï¼‰
```

**å‹•æ…‹æ¡æ¨£**:
```python
def _get_sample_size(self, n_nodes: int) -> int:
    """æ ¹æ“šç¶²çµ¡è¦æ¨¡è‡ªé©æ‡‰æ¡æ¨£"""
    if n_nodes < 100:
        return n_nodes * (n_nodes - 1) // 2  # å…¨éƒ¨
    elif n_nodes < 500:
        return min(100, n_nodes * 2)  # æ¡æ¨£å¤šä¸€äº›
    elif n_nodes < 2000:
        return 50
    else:
        return 20  # è¶…å¤§è¦æ¨¡æ™‚é™æ¡æ¨£

sample_size = self._get_sample_size(len(nodes))
```

---

## ç¸½çµ

| çµ„ä»¶ | æ™‚é–“è¤‡é›œåº¦ | ç©ºé–“è¤‡é›œåº¦ | ç“¶é ¸ |
|------|-----------|----------|------|
| **å‘é‡ç›¸ä¼¼åº¦æœç´¢** | O(n) | O(mÃ—d) | m=å¡ç‰‡æ•¸ï¼Œd=å‘é‡ç¶­åº¦ |
| **ä¿¡åº¦è©•åˆ†** | O(mÂ²) | O(m) | m=å¡ç‰‡å°æ•¸ |
| **ç¤¾ç¾¤æª¢æ¸¬** | O(nÃ—iter) | O(n+m) | iter=è¿­ä»£æ¬¡æ•¸ |
| **PageRank** | O(nÃ—iter) | O(n) | iter=è¿­ä»£æ¬¡æ•¸ |
| **å®Œæ•´åˆ†æ** | O(nÂ²) | O(n+m) | n=ç¯€é»ï¼Œm=é‚Š |

**æœ€å„ªå¯¦è¸**:
1. âœ… ä½¿ç”¨å‘é‡æ•¸æ“šåº«ï¼ˆå·²å¯¦ç¾ï¼‰
2. âœ… é¿å…é‡è¤‡è™•ç†ï¼ˆå·²å¯¦ç¾ï¼‰
3. âœ… æ¡æ¨£åŠ é€Ÿæ˜‚è²´è¨ˆç®—ï¼ˆå·²å¯¦ç¾ï¼‰
4. ğŸ”„ æ‰¹é‡æŸ¥è©¢å‘é‡ï¼ˆæœªå¯¦ç¾ï¼‰
5. ğŸ”„ åˆ†ä½ˆå¼è™•ç†ï¼ˆæœªå¯¦ç¾ï¼‰

---

## åŸºæº–æ¸¬è©¦çµæœèˆ‡æ”¹é€²æ–¹æ¡ˆ

**æ¸¬è©¦æ—¥æœŸ**: 2025-11-05
**æ¸¬è©¦å ±å‘Š**: `docs/BASELINE_RELATION_ANALYSIS.md`

### ç•¶å‰ç³»çµ±è¡¨ç¾

**æ¸¬è©¦æ•¸æ“š**ï¼ˆ704å¼µå¡ç‰‡ï¼‰:
- è­˜åˆ¥é—œä¿‚ç¸½æ•¸: 56,436
- é«˜ä¿¡åº¦é—œä¿‚ï¼ˆâ‰¥ 0.4ï¼‰: **0** âŒ
- å¹³å‡ä¿¡åº¦è©•åˆ†: ~0.33ï¼ˆä½æ–¼é–¾å€¼ï¼‰
- æ˜ç¢ºé€£çµè¦†è“‹ç‡: 11.6%

**é—œéµå•é¡Œ**:
1. âŒ **æ‰€æœ‰é—œä¿‚ä¿¡åº¦ä½æ–¼ 0.4**ï¼Œç„¡æ³•ç”¢ç”Ÿä»»ä½•å»ºè­°é€£çµ
2. âŒ Obsidian å»ºè­°é€£çµåŠŸèƒ½**å®Œå…¨ç„¡æ³•ä½¿ç”¨**ï¼ˆè¼¸å‡ºç‚ºç©ºï¼‰
3. âŒ ç¶²çµ¡å¯†åº¦éé«˜ï¼ˆ0.228ï¼‰ï¼Œç„¡æ³•å€åˆ†çœŸå¯¦é—œä¿‚
4. âŒ æ˜ç¢ºé€£çµåˆ©ç”¨ä¸è¶³ï¼ˆåƒ… 11.6% å¡ç‰‡æœ‰é€£çµï¼‰

### æ”¹é€²æ–¹æ¡ˆæ¦‚è¦½

**å®Œæ•´è¨­è¨ˆ**: `docs/RELATION_FINDER_IMPROVEMENTS.md`ï¼ˆ1200+ è¡Œï¼‰

#### æ”¹é€² 1: å¤šå±¤æ¬¡æ˜ç¢ºé€£çµæª¢æ¸¬ï¼ˆ30%æ¬Šé‡ï¼‰

**ç•¶å‰å•é¡Œ**: åªæª¢æŸ¥ `[[zettel_id]]` Wiki Links

**æ”¹é€²æ–¹æ¡ˆ**:
```python
def _check_explicit_link_enhanced(card, target_id) -> Tuple[bool, float]:
    """
    4å±¤é€£çµæª¢æ¸¬:
    1. AIç­†è¨˜ä¸­çš„Wiki Linksï¼ˆèªå¢ƒåˆ†æï¼‰    â†’ 0.5-1.0
    2. é€£çµç¶²çµ¡å€å¡Š                        â†’ 0.6-0.8
    3. ä¾†æºè„ˆçµ¡æåŠ                        â†’ 0.4
    4. å…§å®¹è‡ªç„¶æåŠ                        â†’ 0.3

    Returns:
        (has_link, link_strength)  # link_strength: 0.0-1.0
    """
    # å¯¦ä½œç´°ç¯€è¦‹ RELATION_FINDER_IMPROVEMENTS.md
```

**é æœŸæ•ˆæœ**:
- å¾äºŒå…ƒï¼ˆ0/0.3ï¼‰â†’ é€£çºŒï¼ˆ0-0.3ï¼‰
- è€ƒæ…®é€£çµèªå¢ƒå’Œæ–¹å‘æ€§
- æ˜ç¢ºé€£çµè²¢ç»: 0.035 â†’ **0.15+**ï¼ˆ+329%ï¼‰

#### æ”¹é€² 2: æ“´å±•å…±åŒæ¦‚å¿µæå–ï¼ˆ20%æ¬Šé‡ï¼‰

**ç•¶å‰å•é¡Œ**: åªå¾ tagsã€core_conceptã€title æå–

**æ”¹é€²æ–¹æ¡ˆ**:
```python
def _extract_shared_concepts_enhanced(card1, card2) -> Tuple[List[str], Dict]:
    """
    5å€‹ä¾†æºï¼ˆåŠ æ¬Šï¼‰:
    - tags (1.0)              # æœ€æº–ç¢º
    - core_concept (0.9)      # æ¬¡æº–ç¢º
    - description (0.8)       # æ–°å¢ï¼é¦–æ®µèªªæ˜
    - title (0.7)             # è¼ƒç°¡çŸ­
    - ai_notes (0.6)          # è¼ƒç™¼æ•£

    Returns:
        (shared_concepts, details_by_source)
    """
    # æ”¯æ´ jieba ä¸­æ–‡åˆ†è©æˆ–é å®šç¾©è©åº«
```

**é æœŸæ•ˆæœ**:
- å…±åŒæ¦‚å¿µæ•¸é‡ +50%
- ä¸­æ–‡åˆ†è©æ”¹å–„
- å…±åŒæ¦‚å¿µè²¢ç»: 0.08 â†’ **0.12+**ï¼ˆ+50%ï¼‰

#### æ”¹é€² 3: é ˜åŸŸç›¸é—œæ€§çŸ©é™£ï¼ˆ10%æ¬Šé‡ï¼‰

**ç•¶å‰å•é¡Œ**: äºŒå…ƒåˆ¤æ–·ï¼ˆåŒé ˜åŸŸ=0.1ï¼Œä¸åŒ=0.05ï¼‰

**æ”¹é€²æ–¹æ¡ˆ**:
```python
domain_similarity_matrix = {
    # é«˜åº¦ç›¸é—œ (0.8)
    ('CogSci', 'AI'): 0.8,
    ('CogSci', 'Linguistics'): 0.8,

    # ä¸­åº¦ç›¸é—œ (0.6)
    ('AI', 'Linguistics'): 0.6,

    # å¼±ç›¸é—œ (0.3ï¼Œé»˜èª)
    # æœªå®šç¾©çµ„åˆ: 0.3
}

def _calculate_multi_domain_similarity(domains1, domains2):
    # æ”¯æ´å¤šé ˜åŸŸ: "CogSci, AI"
    # å–æ‰€æœ‰çµ„åˆçš„æœ€å¤§ç›¸ä¼¼åº¦
```

**é æœŸæ•ˆæœ**:
- æ”¯æ´è·¨é ˜åŸŸç ”ç©¶
- ç´°ç·»è©•åˆ†ï¼ˆ0.03-0.10ï¼‰
- é ˜åŸŸè²¢ç»: 0.075 â†’ **0.09+**ï¼ˆ+20%ï¼‰

#### æ”¹é€² 4: AI Notes é€£çµç”Ÿæˆ

**å•é¡Œ**: LLM è¼¸å‡ºçš„ AI note ç¼ºå°‘å¡ç‰‡é–“é€£çµ

**è§£æ±ºæ–¹æ¡ˆ**:
- æ›´æ–° `templates/prompts/zettelkasten_template.jinja2`
- æ˜ç¢ºè¦æ±‚ã€Œå¿…é ˆå»ºç«‹ 2-3 å€‹å¡ç‰‡é€£çµã€
- Few-shot ç¯„ä¾‹å±•ç¤ºæ­£ç¢ºæ ¼å¼

**é æœŸæ•ˆæœ**:
- æ˜ç¢ºé€£çµè¦†è“‹ç‡: 11.6% â†’ **50%+**

#### æ”¹é€² 5: æ°¸ä¹…ç­†è¨˜ç”Ÿæˆå™¨ï¼ˆé•·æœŸï¼‰

**åŠŸèƒ½**: å¾ AI notes + Human notes åˆæˆæ°¸ä¹…ç­†è¨˜

```python
class PermanentNoteGenerator:
    def generate_permanent_note(topic, related_zettel_ids, output_path):
        # æ”¶é›† AI åæ€ + äººé¡ç­†è¨˜
        # LLM åˆæˆé€£è²«çš„æ°¸ä¹…ç­†è¨˜
        # ä¿ç•™ä¾†æºå¼•ç”¨
```

**CLI**:
```bash
python kb_manage.py synthesize-permanent-note \
    --topic "è¦–è¦ºæ³¨æ„èˆ‡å·¥ä½œè¨˜æ†¶" \
    --zettel-ids CogSci-001 CogSci-003 CogSci-007
```

### æ”¹é€²æ•ˆæœé ä¼°

#### ä¿¡åº¦è©•åˆ†æå‡ï¼ˆè·¨é ˜åŸŸå¡ç‰‡ç¯„ä¾‹ï¼‰

| ç¶­åº¦ | ç•¶å‰ | æ”¹é€²å¾Œ | æå‡ |
|------|-----|-------|------|
| èªç¾©ç›¸ä¼¼åº¦ (40%) | 0.26 | 0.26 | - |
| æ˜ç¢ºé€£çµ (30%) | 0.00 | **0.12** | +40% |
| å…±åŒæ¦‚å¿µ (20%) | 0.04 | **0.10** | +150% |
| é ˜åŸŸä¸€è‡´æ€§ (10%) | 0.05 | **0.08** | +60% |
| **ç¸½ä¿¡åº¦** | **0.35** | **0.56** | **+60%** |

#### æ•´é«”ç³»çµ±æ”¹å–„

| æŒ‡æ¨™ | ç•¶å‰ | ç›®æ¨™ï¼ˆPhase 1ï¼‰ | æ”¹é€²å¹…åº¦ |
|------|-----|----------------|----------|
| é«˜ä¿¡åº¦é—œä¿‚æ•¸ï¼ˆâ‰¥ 0.4ï¼‰ | 0 | 5,000+ | +âˆ |
| å¹³å‡ä¿¡åº¦è©•åˆ† | 0.33 | 0.50+ | +51.5% |
| å»ºè­°é€£çµå¯ç”¨æ€§ | 0% | å¯ç”¨ | âœ… |
| ç¶²çµ¡çµæ§‹æ¸…æ™°åº¦ | å·® | è‰¯å¥½ | â¬†ï¸ |

### å¯¦æ–½è¨ˆç•«

#### Phase 1: æ ¸å¿ƒæ”¹é€²ï¼ˆP0ï¼Œ1-2å¤©ï¼‰

1. âœ… æ”¹é€² 2ï¼šæ“´å±•å…±åŒæ¦‚å¿µæå–
   - åŠ å…¥ description æ¬„ä½
   - å¯¦ä½œä¸­æ–‡åˆ†è©ï¼ˆé å®šç¾©è©åº«ï¼‰
   - åŠ æ¬Šè©•åˆ†æ©Ÿåˆ¶

2. âœ… æ”¹é€² 3ï¼šé ˜åŸŸç›¸é—œæ€§çŸ©é™£
   - å®šç¾©ç›¸é—œæ€§çŸ©é™£
   - æ”¯æ´å¤šé ˜åŸŸè§£æ
   - æ›´æ–°ä¿¡åº¦è¨ˆç®—

**é©—æ”¶æ¨™æº–**:
- [ ] å…±åŒæ¦‚å¿µæ•¸é‡å¹³å‡å¢åŠ  50%+
- [ ] è·¨é ˜åŸŸå¡ç‰‡ä¿¡åº¦æå‡ 20%+
- [ ] ä¸ç ´å£ç¾æœ‰åŠŸèƒ½

#### Phase 2: é€£çµå¢å¼·ï¼ˆP1ï¼Œ2-3å¤©ï¼‰

3. âœ… æ”¹é€² 1ï¼šå¤šå±¤æ¬¡é€£çµæª¢æ¸¬
   - 4å±¤é€£çµæª¢æ¸¬é‚è¼¯
   - Markdown å€å¡Šè§£æ
   - é€£çµèªå¢ƒåˆ†æ

4. âœ… æ”¹é€² 4ï¼šæ”¹é€² Zettelkasten Prompt
   - æ›´æ–° prompt æ¨¡æ¿
   - æ–°å¢é€£çµç”ŸæˆæŒ‡å¼•
   - Few-shot ç¯„ä¾‹å„ªåŒ–

**é©—æ”¶æ¨™æº–**:
- [ ] AI notes å¹³å‡åŒ…å« 2-3 å€‹é€£çµ
- [ ] é€£çµèªå¢ƒè­˜åˆ¥æº–ç¢ºç‡ > 80%
- [ ] æ˜ç¢ºé€£çµè¦†è“‹ç‡ > 30%

#### Phase 3: æ°¸ä¹…ç­†è¨˜ï¼ˆP2ï¼Œ3-4å¤©ï¼‰

5. âœ… æ”¹é€² 5ï¼šæ°¸ä¹…ç­†è¨˜ç”Ÿæˆå™¨
   - å¯¦ä½œ PermanentNoteGenerator é¡
   - CLI å‘½ä»¤æ•´åˆ
   - è¼¸å‡ºæ ¼å¼å„ªåŒ–

**é©—æ”¶æ¨™æº–**:
- [ ] èƒ½å¾ 3-5 å¼µå¡ç‰‡åˆæˆæ°¸ä¹…ç­†è¨˜
- [ ] ä¿ç•™ä¾†æºå¼•ç”¨
- [ ] å…§å®¹é€£è²«ä¸”æ·±å…¥

### é…ç½®æ›´æ–°

**æ–°å¢é…ç½®** (`config/settings.yaml`):

```yaml
# Relation Finder é…ç½®
relation_finder:
  # ä¿¡åº¦è©•åˆ†æ¬Šé‡ï¼ˆç¸½å’Œæ‡‰ç‚º1.0ï¼‰
  confidence_weights:
    semantic_similarity: 0.40
    link_explicit: 0.30
    co_occurrence: 0.20
    domain_consistency: 0.10

  # å…±åŒæ¦‚å¿µä¾†æºæ¬Šé‡
  concept_source_weights:
    tags: 1.0
    core_concept: 0.9
    description: 0.8
    title: 0.7
    ai_notes: 0.6

  # é ˜åŸŸç›¸é—œæ€§çŸ©é™£ï¼ˆå¯è‡ªè¨‚ï¼‰
  domain_similarity:
    # é«˜åº¦ç›¸é—œ
    - [CogSci, AI, 0.8]
    - [CogSci, Linguistics, 0.8]
    # ä¸­åº¦ç›¸é—œ
    - [AI, Linguistics, 0.6]
    # å¼±ç›¸é—œï¼ˆé»˜èªï¼‰
    default: 0.3

  # é€£çµæª¢æ¸¬é…ç½®
  link_detection:
    enable_multi_layer: true
    context_window: 50  # é€£çµèªå¢ƒå­—ç¬¦æ•¸
    link_strength_threshold: 0.3

  # ä¸­æ–‡åˆ†è©é…ç½®
  chinese_segmentation:
    method: "predefined"  # predefined | jieba
    min_keyword_length: 2
    top_keywords: 10
```

### æ¸¬è©¦ç­–ç•¥

#### å›æ­¸æ¸¬è©¦

1. **ä¿å­˜åŸºæº–æ•¸æ“š**
   - âœ… `output/concept_analysis/`ï¼ˆåŸºæº–ç‰ˆæœ¬ï¼‰
   - âœ… `docs/BASELINE_RELATION_ANALYSIS.md`ï¼ˆå ±å‘Šï¼‰

2. **æ”¹é€²å¾Œé‡æ–°æ¸¬è©¦**
   ```bash
   python kb_manage.py visualize-network --obsidian \
       --output output/concept_analysis_v2
   ```

3. **æ¯”è¼ƒé—œéµæŒ‡æ¨™**
   - é«˜ä¿¡åº¦é—œä¿‚æ•¸ï¼š0 â†’ 5,000+
   - å¹³å‡ä¿¡åº¦ï¼š0.33 â†’ 0.50+
   - å»ºè­°é€£çµæ•¸é‡ï¼š0 â†’ 50+

#### äººå·¥é©—è­‰

- éš¨æ©ŸæŠ½å– 20 æ¢é«˜ä¿¡åº¦é—œä¿‚
- äººå·¥è©•ä¼°æº–ç¢ºç‡
- ç›®æ¨™æº–ç¢ºç‡ï¼š> 80%

### å‘å¾Œç›¸å®¹æ€§

- âœ… **å®Œå…¨ç›¸å®¹**ï¼šAPI ä¸è®Šï¼Œå…§éƒ¨é‚è¼¯æ”¹é€²
- âœ… **é…ç½®å¯é¸**ï¼šæ–°å¢é…ç½®æœ‰é»˜èªå€¼
- âœ… **å¹³æ»‘å‡ç´š**ï¼šå¯é€æ­¥å•Ÿç”¨æ–°åŠŸèƒ½

### æ•ˆèƒ½å½±éŸ¿

| æ“ä½œ | ç•¶å‰ | æ”¹é€²å¾Œ | è®ŠåŒ– |
|------|-----|-------|------|
| å–®å¡ç‰‡é—œä¿‚è¨ˆç®— | ~0.5ç§’ | ~0.7ç§’ | +40% |
| å®Œæ•´ç¶²çµ¡ï¼ˆ704å¼µï¼‰ | 2-3åˆ†é˜ | 3-4åˆ†é˜ | +33% |

**å¯æ¥å—**ï¼šæ•ˆæœæå‡é å¤§æ–¼æ€§èƒ½æå¤±

---

## åƒè€ƒæ–‡æª”

- **æ”¹é€²æ–¹æ¡ˆè©³ç´°è¨­è¨ˆ**: `docs/RELATION_FINDER_IMPROVEMENTS.md`ï¼ˆ1200è¡Œï¼‰
- **åŸºæº–æ¸¬è©¦å ±å‘Š**: `docs/BASELINE_RELATION_ANALYSIS.md`
- **å¯¦æ–½æª¢æŸ¥æ¸…å–®**: è¦‹ RELATION_FINDER_IMPROVEMENTS.md æœ«å°¾

---

**æ–‡ä»¶ç‰ˆæœ¬**: 1.1
**æœ€å¾Œæ›´æ–°**: 2025-11-06
**ç‹€æ…‹**: âœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼Œæ”¹é€²æ–¹æ¡ˆè¨­è¨ˆå®Œæˆï¼Œå¾…å¯¦æ–½

