% Phase 3A Test Batch - 18 Papers
% Generated for rebuild test

@article{Adams-2020,
  title = {A {{Replication}} of {{Beyond}} the {{Turk}}: {{Alternative Platforms}} for {{Crowdsourcing Behavioral Research}} -- {{Sometimes Preferable}} to {{Student Groups}}},
  shorttitle = {A {{Replication}} of {{Beyond}} the {{Turk}}},
  author = {Adams, Troy and Li, Yuanxia and Liu, Hao},
  year = 2020,
  month = oct,
  journal = {AIS Transactions on Replication Research},
  volume = {6},
  number = {1},
  issn = {2473-3458},
  doi = {10.17705/1atrr.00058},
  file = {D:\core\Research\Program_verse\+\pdf\Adams-2020.pdf}
}

@article{Baruch-2016,
  title = {The Motivations, Enablers and Barriers for Voluntary Participation in an Online Crowdsourcing Platform},
  author = {Baruch, Avinoam and May, Andrew and Yu, Dapeng},
  year = 2016,
  month = nov,
  journal = {Computers in Human Behavior},
  volume = {64},
  pages = {923--931},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2016.07.039},
  urldate = {2025-07-03},
  abstract = {This paper examines the phenomena of online crowdsourcing from the perspectives of both volunteers and the campaign coordinator of Tomnod -- an online mapping project that uses crowdsourcing to identify objects and places in satellite images. A mixed-methods approach was used to study the enablers and barriers to participation, taking into consideration the whole spectrum of volunteers. The results show broad diversity in online volunteers, both in their demographics and the factors affecting their voluntary participation. The majority are older than 50 years and many -- particularly the most active volunteers -- have disabilities or long term health problems. The personal circumstances of participants are highlighted as a major factor affecting involvement in campaigns. Like many other platforms, altruism is a key motivator, yet many participants are more interested in the quality of their data and the impact it has on the ground. For many participants of online crowdsourcing campaigns, their involvement is strongly linked to the level of contact they have with campaign coordinators, both in the design of the platform and in providing feedback on the impact of their contributions.},
  file = {D:\core\Research\Program_verse\+\pdf\Baruch-2016.pdf}
}

@article{Crequit-2018,
  title = {Mapping of {{Crowdsourcing}} in {{Health}}: {{Systematic Review}}},
  shorttitle = {Mapping of {{Crowdsourcing}} in {{Health}}},
  author = {Cr{\'e}quit, Perrine and Mansouri, Ghizl{\`e}ne and Benchoufi, Mehdi and Vivot, Alexandre and Ravaud, Philippe},
  year = 2018,
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {20},
  number = {5},
  pages = {e9330},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/jmir.9330},
  urldate = {2025-08-21},
  abstract = {: Crowdsourcing involves obtaining ideas, needed services, or content by soliciting Web-based contributions from a crowd. The 4 types of crowdsourced tasks (problem solving, data processing, surveillance or monitoring, and surveying) can be applied in the 3 categories of health (promotion, research, and care). : This study aimed to map the different applications of crowdsourcing in health to assess the fields of health that are using crowdsourcing and the crowdsourced tasks used. We also describe the logistics of crowdsourcing and the characteristics of crowd workers. : MEDLINE, EMBASE, and ClinicalTrials.gov were searched for available reports from inception to March 30, 2016, with no restriction on language or publication status. : We identified 202 relevant studies that used crowdsourcing, including 9 randomized controlled trials, of which only one had posted results at ClinicalTrials.gov. Crowdsourcing was used in health promotion (91/202, 45.0\%), research (73/202, 36.1\%), and care (38/202, 18.8\%). The 4 most frequent areas of application were public health (67/202, 33.2\%), psychiatry (32/202, 15.8\%), surgery (22/202, 10.9\%), and oncology (14/202, 6.9\%). Half of the reports (99/202, 49.0\%) referred to data processing, 34.6\% (70/202) referred to surveying, 10.4\% (21/202) referred to surveillance or monitoring, and 5.9\% (12/202) referred to problem-solving. Labor market platforms (eg, Amazon Mechanical Turk) were used in most studies (190/202, 94\%). The crowd workers' characteristics were poorly reported, and crowdsourcing logistics were missing from two-thirds of the reports. When reported, the median size of the crowd was 424 (first and third quartiles: 167-802); crowd workers' median age was 34 years (32-36). Crowd workers were mainly recruited nationally, particularly in the United States. For many studies (58.9\%, 119/202), previous experience in crowdsourcing was required, and passing a qualification test or training was seldom needed (11.9\% of studies; 24/202). For half of the studies, monetary incentives were mentioned, with mainly less than US \$1 to perform the task. The time needed to perform the task was mostly less than 10 min (58.9\% of studies; 119/202). Data quality validation was used in 54/202 studies (26.7\%), mainly by attention check questions or by replicating the task with several crowd workers. : The use of crowdsourcing, which allows access to a large pool of participants as well as saving time in data collection, lowering costs, and speeding up innovations, is increasing in health promotion, research, and care. However, the description of crowdsourcing logistics and crowd workers' characteristics is frequently missing in study reports and needs to be precisely reported to better interpret the study findings and replicate them.},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\CrÃ©quit-2018.pdf}
}

@article{Crockett-2025,
  title = {{{AI Surrogates}} and Illusions of Generalizability in Cognitive Science},
  author = {Crockett, M. J. and Messeri, Lisa},
  year = 2025,
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {0},
  number = {0},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2025.09.012},
  urldate = {2025-10-24},
  langid = {english},
  pmid = {41125476},
  file = {D:\core\Research\Program_verse\+\pdf\Crockett-2025.pdf}
}

@misc{Guest-2025,
  title = {Against the {{Uncritical Adoption}} of '{{AI}}' {{Technologies}} in {{Academia}}},
  author = {Guest, Olivia and Suarez, Marcela and M{\"u}ller, Barbara and {\noopsort{meerkerk}}{van Meerkerk}, Edwin and Oude Groote Beverborg, Arnoud and {\noopsort{haan}}{de Haan}, Ronald and Reyes Elizondo, Andrea and Blokpoel, Mark and Scharfenberg, Natalia and Kleinherenbrink, Annelies and Camerino, Ileana and Woensdregt, Marieke and Monett, Dagmar and Brown, Jed and Avraamidou, Lucy and {Alenda-Demoutiez}, Juliette and Hermans, Felienne and {\noopsort{rooij}}{van Rooij}, Iris},
  year = 2025,
  month = sep,
  publisher = {Zenodo},
  urldate = {2025-09-09},
  abstract = {Under the banner of progress, products have been uncritically adopted or even imposed on users --- in past centuries with tobacco and combustion engines, and in the 21st with social media. For these collective blunders, we now regret our involvement or apathy as scientists, and society struggles to put the genie back in the bottle. Currently, we are similarly entangled with artificial intelligence (AI) technology. For example, software updates are rolled out seamlessly and non-consensually, Microsoft Office is bundled with chatbots, and we, our students, and our employers have had no say, as it is not considered a valid position to reject AI technologies in our teaching and research. This is why in June 2025, we co-authored an Open Letter calling on our employers to reverse and rethink their stance on uncritically adopting AI technologies. In this position piece, we expound on why universities must take their role seriously to a) counter the technology industry's marketing, hype, and harm; and to b) safeguard higher education, critical thinking, expertise, academic freedom, and scientific integrity. We include pointers to relevant work to further inform our colleagues.},
  archiveprefix = {Zenodo},
  file = {D:\core\Research\Program_verse\+\pdf\Guest-2025.pdf}
}

@misc{Guest-2025a,
  title = {Critical {{Artificial Intelligence Literacy}} for {{Psychologists}}},
  author = {Guest, Olivia and {\noopsort{rooij}}van Rooij, Iris},
  year = 2025,
  month = oct,
  publisher = {OSF},
  urldate = {2025-10-12},
  abstract = {Psychologists --- from computational modellers to social and personality researchers to cognitive neuroscientists and from experimentalists to methodologists to theoreticians --- can fall prey to exaggerated claims about artificial intelligence (AI). In social psychology, as in psychology generally, we see arguments taken at face value for: a) the displacement of experimental participants with opaque AI products; the outsourcing of b) programming, c) writing, and even d) scientific theorising to such models; and the notion that e) human-technology interactions could be on the same footing as human-human (e.g., client-therapist, student-teacher, patient-doctor, friendship, or romantic) relationships. But if our colleagues are, accidentally or otherwise, promoting such ideas in exchange for salary, grants, or citations, how are we as academic psychologists meant to react? Formal models, from statistics and computational methods broadly, have a potential obfuscatory power that is weaponisable, laying serious traps for the uncritical adopters, with even the term 'AI' having murky referents. Herein, we concretise the term AI and counter the five related proposals above --- from the clearly insidious to those whose ethical neutrality is skin-deep and whose functionality is a mirage. Ultimately, contemporary AI is research misconduct.},
  archiveprefix = {OSF},
  langid = {american},
  file = {D:\core\Research\Program_verse\+\pdf\Guest-2025a.pdf}
}

@misc{Gunther-2025a,
  title = {Interpretability Norms for Novel Words and Nonwords},
  author = {G{\"u}nther, Fritz and Raveling, Laura},
  year = 2025,
  month = mar,
  publisher = {OSF},
  doi = {10.31234/osf.io/t3krs_v1},
  urldate = {2025-03-23},
  abstract = {The lexicon of a language is subject to constant change, and new words constantly enter the lexicon. In principle, any word form that is not currently in the lexicon but adheres to the orthotactic rules of a language can be a novel word, including morphologically complex words but also pseudowords. However, such novel words differ in their semantic interpretability -- how easily speakers can come up with an interpretation for them -- which is of interest as both an independent and dependent variable in theory-building, computational modelling, and empirical studies. Here, we provide an overview of studies that make available (large) norms of semantic interpretability ratings and judgments, which will serve as a useful resource for such studies.},
  archiveprefix = {OSF},
  langid = {american},
  file = {D:\core\Research\Program_verse\+\pdf\GÃ¼nther-2025.pdf}
}

@article{Hosseini-2015,
  title = {Crowdsourcing: {{A}} Taxonomy and Systematic Mapping Study},
  shorttitle = {Crowdsourcing},
  author = {Hosseini, Mahmood and Shahri, Alimohammad and Phalp, Keith and Taylor, Jacqui and Ali, Raian},
  year = 2015,
  month = aug,
  journal = {Computer Science Review},
  volume = {17},
  pages = {43--69},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2015.05.001},
  urldate = {2025-08-21},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\Hosseini-2015.pdf}
}

@article{Leckel-2025,
  title = {Interaction Design for Open Innovation Platforms: {{A}} Social Exchange Perspective},
  shorttitle = {Interaction Design for Open Innovation Platforms},
  author = {Leckel, Anja and Randhawa, Krithika and Piller, Frank T.},
  year = 2025,
  month = jul,
  journal = {Journal of Product Innovation Management},
  volume = {42},
  number = {4},
  pages = {641--678},
  issn = {0737-6782, 1540-5885},
  doi = {10.1111/jpim.12787},
  urldate = {2025-08-21},
  abstract = {Abstract             We investigate the interaction design preferences of solution seekers and problem solvers on open innovation (crowdsourcing) platforms. Drawing on social exchange theory (SET), we hypothesize that seekers and solvers have different preferences for the configuration of four central interaction design features of a crowdsourcing platform: communication channels, collaboration options, selection of winning submissions, and feedback mechanisms. Based on a conjoint study with 842 respondents, we show conflicting preferences for the configuration of these features, but also find a surprisingly consistent ``best'' configuration that can balance the individual preferences of both seekers and solvers. In addition, we identify social trust, risk aversion, and the need for cognition as three personal characteristics of individuals in seeker organizations and solvers that influence their preferred configuration of platform design. Our findings help intermediaries operating a crowdsourcing platform to offer nuanced platform interactions that align how individuals in seeker organizations (e.g., project managers) and individual solvers create and capture value in crowdsourcing. Furthermore, we contribute to the micro-foundations of open innovation by proposing SET as a novel perspective to examine how the expectations and value drivers of all parties involved in a crowdsourcing project can be balanced.},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\Leckel-2025.pdf}
}

@article{Liao-2021,
  title = {{{GRBMC}}: {{An}} Effective Crowdsourcing Recommendation for Workers Groups},
  shorttitle = {{{GRBMC}}},
  author = {Liao, Zhifang and Xu, Xin and Fan, Xiaoping and Zhang, Yan and Yu, Song},
  year = 2021,
  month = oct,
  journal = {Expert Systems with Applications},
  volume = {179},
  pages = {115039},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.115039},
  urldate = {2025-08-21},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\Liao-2021.pdf}
}

@article{Peer-2017,
  title = {Beyond the {{Turk}}: {{Alternative}} Platforms for Crowdsourcing Behavioral Research},
  shorttitle = {Beyond the {{Turk}}},
  author = {Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
  year = 2017,
  month = may,
  journal = {Journal of Experimental Social Psychology},
  volume = {70},
  pages = {153--163},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.01.006},
  urldate = {2017-02-07},
  abstract = {The success of Amazon Mechanical Turk (MTurk) as an online research platform has come at a price: MTurk has suffered from slowing rates of population replenishment, and growing participant non-naivety. Recently, a number of alternative platforms have emerged, offering capabilities similar to MTurk but providing access to new and more na\"ive populations. After surveying several options, we empirically examined two such platforms, CrowdFlower (CF) and Prolific Academic (ProA). In two studies, we found that participants on both platforms were more na\"ive and less dishonest compared to MTurk participants. Across the three platforms, CF provided the best response rate, but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk. Moreover, ProA participants produced data quality that was higher than CF's and comparable to MTurk's. ProA and CF participants were also much more diverse than participants from MTurk.},
  file = {D\:\\core\\Research\\Program_verse\\+\\pdf\\Peer-2017.pdf;D\:\\core\\Version_Controls\\zotero_data\\storage\\HMNDUFTB\\S0022103116303201.html}
}

@article{Shapiro-2013,
  title = {Using {{Mechanical Turk}} to {{Study Clinical Populations}}},
  author = {Shapiro, Danielle N. and Chandler, Jesse and Mueller, Pam A.},
  year = 2013,
  month = apr,
  journal = {Clinical Psychological Science},
  volume = {1},
  number = {2},
  pages = {213--220},
  publisher = {SAGE Publications Inc},
  issn = {2167-7026},
  doi = {10.1177/2167702612469015},
  urldate = {2025-08-21},
  abstract = {Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon's Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online.},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\Shapiro-2013.pdf}
}

@article{Stewart-2017,
  title = {Crowdsourcing {{Samples}} in {{Cognitive Science}}},
  author = {Stewart, Neil and Chandler, Jesse and Paolacci, Gabriele},
  year = 2017,
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {21},
  number = {10},
  pages = {736--748},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2017.06.007},
  urldate = {2025-08-21},
  abstract = {Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.},
  file = {D\:\\core\\Research\\Program_verse\\+\\pdf\\Stewart-2017.pdf;D\:\\core\\Version_Controls\\zotero_data\\storage\\P67DT2NE\\Stewart ç­‰ã€‚ - 2017 - Crowdsourcing Samples in Cognitive Science.pdf;D\:\\core\\Version_Controls\\zotero_data\\storage\\Q8ZXTKSV\\S1364661317301316.html}
}

@article{Strickland-2019,
  title = {The Use of Crowdsourcing in Addiction Science Research: {{Amazon Mechanical Turk}}},
  shorttitle = {The Use of Crowdsourcing in Addiction Science Research},
  author = {Strickland, Justin C. and Stoops, William W.},
  year = 2019,
  journal = {Experimental and Clinical Psychopharmacology},
  volume = {27},
  number = {1},
  pages = {1--18},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1936-2293},
  doi = {10.1037/pha0000235},
  abstract = {Crowdsourcing, the use of the Internet to outsource work to a large number of people, has witnessed a dramatic growth over the past decade. One popular crowdsourcing option, Amazon Mechanical Turk (MTurk), is now commonly used to sample participants for psychological research. Addiction science is positioned to benefit greatly from crowdsourced sampling due to the ability to efficiently and effectively tap into populations with specific behavioral and health histories. The primary objective of this review is to describe the utility of crowdsourcing, broadly, and MTurk, specifically, for conducting research relevant to substance use and misuse. Studies in psychological and other health science have supported the reliability and validity of data gathered using crowdsourced samples. Promising research relevant to addiction science has also been conducted, including studies using cross-sectional designs and those for measure development purposes. Preliminary work using longitudinal methods and for interventions development has also revealed the potential of MTurk for studying alcohol and other drug use through these designs. Additional studies are needed to better understand the benefits, as well as the limits and constraints, of research conducted through crowdsourced online platforms. Crowdsourcing, such as on MTurk, can ultimately provide an important complement to existing methods used in human laboratory, clinical trial, community intervention, and epidemiological research. The combinations of these methodological approaches could help improve the rigor, reproducibility, and overall scope of research conducted in addiction science. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  file = {D:\core\Research\Program_verse\+\pdf\Strickland-2019.pdf}
}

@article{Strickland-2022,
  title = {Crowdsourcing Methods in Addiction Science: {{Emerging}} Research and Best Practices},
  shorttitle = {Crowdsourcing Methods in Addiction Science},
  author = {Strickland, Justin C. and Amlung, Michael and Reed, Derek D.},
  year = 2022,
  journal = {Experimental and Clinical Psychopharmacology},
  volume = {30},
  number = {4},
  pages = {379--380},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1936-2293},
  doi = {10.1037/pha0000582},
  abstract = {Crowdsourcing platforms such as Amazon Mechanical Turk, Prolific, and Qualtrics Panels have become a dominant form of sampling in recent years. Crowdsourcing enables researchers to effectively and efficiently sample research participants with greater geographic variability, access to hard-to-reach populations, and reduced costs. These methods have been increasingly used across varied areas of psychological science and essential for research during the COVID-19 pandemic due to their facilitation of remote research. Recent work documents methods for improving data quality, emerging crowdsourcing platforms, and how crowdsourcing data fit within broader research programs. Addiction scientists will benefit from the adoption of best practice guidelines in crowdsourcing as well as developing novel approaches, venues, and applications to advance the field. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  file = {D:\core\Research\Program_verse\+\pdf\Strickland-2022.pdf}
}

@misc{vanRooij-2025,
  title = {Combining {{Psychology}} with {{Artificial Intelligence}}: {{What}} Could Possibly Go Wrong?},
  shorttitle = {Combining {{Psychology}} with {{Artificial Intelligence}}},
  author = {{\noopsort{rooij}}{van Rooij}, Iris and Guest, Olivia},
  year = 2025,
  month = may,
  publisher = {OSF},
  doi = {10.31234/osf.io/aue4m_v1},
  urldate = {2025-05-15},
  abstract = {The current AI hype cycle combined with Psychology's various crises make for a perfect storm. Psychology, on the one hand, has a history of weak theoretical foundations, a neglect for computational and formal skills, and a hyperempiricist privileging of experimental tasks and testing for effects. Artificial Intelligence, on the other hand, has a history of conflating artifacts for theories of cognition, or even minds themselves, and its engineering offspring likes to move fast and break things. Many of our contemporaries now want to combine the worst of these two worlds. What could possibly go wrong? Quite a lot. Does this mean that Psychology and Artificial Intelligence can best part ways? Not at all. There are very fruitful ways in which the two disciplines can interact and theoretically inform the interdisciplinary study of cognition. But to reap the fruits one needs to understand how to steer clear of potential traps.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {\_ðŸª¨imported},
  file = {D:\core\Research\Program_verse\+\pdf\van Rooij-2025.pdf}
}

@misc{Vigly-2025,
  title = {Comprehension Effort as the Cost of Inference},
  author = {Vigly, Jacob Hoover and Qian, Peng and Sonderegger, Morgan and O'Donnell, Timothy J},
  year = 2025,
  month = jun,
  abstract = {As you read this text, word by word, you build an understanding of its meaning. What cognitive mechanisms underlie this ability? An influential approach to answering this question comes from viewing comprehension as probabilistic inference over potential interpretations given linguistic input. Motivated within this perspective, a wealth of previous literature in psycholinguistics has focused on an important empirical relationship made precise by surprisal theory (Hale, 2001; Levy, 2008a), the hypothesis that the effort required to process a word scales in its negative log probability, in context. However, the standard derivation of surprisal within the inference framework relies on a crucial assumption: that there is a deterministic relationship between the latent interpretations targeted by inference and the observable input. In this work we propose relaxing this assumption and formalize inference cost directly as the amount of change in probabilistic beliefs. This proposal forms a nontrivial generalization of standard surprisal theory, which provides a more direct connection to algorithmic theories, and naturally explains phenomena where unpredictable input requires little processing effort. To test this framework against surprisal theory, we conduct a self-paced reading time study targeting words with orthographic errors, a specific setting where our approach predicts substantially different patterns. We find that processing effort follows the predictions of belief-update rather than surprisal, in a noisy-channel model of comprehension as inference about intended words. These results demonstrate a clear case where surface surprisal cannot explain human processing cost, and provide further support for models of language comprehension as rational inference.},
  langid = {english},
  file = {D:\core\Research\Program_verse\+\pdf\Vigly-2025.pdf}
}

@misc{Woodley-2025,
  title = {No {{Evidence}} of {{Experimenter Demand Effects}} in {{Three Online Psychology Experiments}}},
  author = {Woodley, Lucas and {Roberts-Gaal}, Xavier and Calcott, Rachel and Cushman, Fiery},
  year = 2025,
  month = sep,
  publisher = {OSF},
  doi = {10.31234/osf.io/g6xhf_v1},
  urldate = {2025-09-17},
  abstract = {Experimenter demand effects occur when participants alter their behavior to align with perceived study hypotheses, threatening internal validity. Concern about demand effects is pervasive in psychology. Experimenter demand may be especially acute in studies relying on experienced participants recruited online (e.g., via Prolific), who may readily guess hypotheses, or using common paradigms (e.g., vignette studies and interventions) where study goals are transparent. We conducted three preregistered experiments (N = 2,254) examining whether explicit demand cues influence online participants' behavior across three paradigms commonly used in psychology: a dictator game, replicating prior work on demand effects (Experiment 1); a moral dilemma vignette (Experiment 2); and an intervention on group attitudes (Experiment 3). We randomly assigned participants on Prolific to receive information about the study's hypothesis or to a no-information control. As expected, we find that receiving such information significantly shifts participants' beliefs about the study's hypothesis, creating an experimenter demand. Yet we find no evidence that learning any study's hypothesis alters participants' behavior, judgments, or attitudes, suggesting that demand effects may be elusive in online samples. These findings offer important insights for the design and interpretation of modern psychology experiments.},
  archiveprefix = {OSF},
  langid = {american},
  file = {D:\core\Research\Program_verse\+\pdf\Woodley-2025.pdf}
}