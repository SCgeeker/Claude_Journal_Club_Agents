# Embedding æ¨¡å‹è©•ä¼°å ±å‘Š (Zettelkasten çŸ¥è­˜åº«å°ˆæ¡ˆ)

**æ—¥æœŸ**: 2025-11-01
**è©•ä¼°è€…**: Claude Code Agent
**æ•¸æ“šä¾†æº**: [ç¹é«”ä¸­æ–‡ Embeddings æ¨¡å‹è©•ä¼°è¡¨](https://docs.google.com/spreadsheets/d/1zad1tMFp7OmNjUvm_a-Ni22av2uBmqYclVRgJQGUtl0/) (ihower éƒ¨è½æ ¼è©•æ¸¬)
**ç‰ˆæœ¬**: v2.0ï¼ˆä¿®æ­£ç‰ˆï¼‰

---

## ğŸ“‹ åŸ·è¡Œæ‘˜è¦

åŸºæ–¼å°ˆæ¡ˆç•¶å‰ç‹€æ…‹ï¼ˆPhase 1 å®Œæˆï¼‰å’Œæœªä¾†éœ€æ±‚ï¼ˆPhase 2-4ï¼‰ï¼Œæœ¬å ±å‘Šè©•ä¼°äº† 50+ å€‹ embedding æ¨¡å‹ï¼Œä¸¦å¾ä¸­ç¯©é¸å‡º **8 å€‹æ¨è–¦æ¨¡å‹**ï¼Œåˆ†ç‚ºä¸‰å€‹å„ªå…ˆç´šå±¤ç´šã€‚

**æ ¸å¿ƒç™¼ç¾**ï¼š
- âœ… å°ˆæ¡ˆç›®å‰ä½¿ç”¨ SQLite FTS5 å…¨æ–‡æœç´¢ï¼Œ**å°šæœªå•Ÿç”¨å‘é‡æœç´¢**ï¼ˆ`vector_search: false`ï¼‰
- âœ… Phase 2-4 çš„ relation-finderã€concept-mapperã€Research Assistant åŠŸèƒ½å°‡å—ç›Šæ–¼èªç¾©å‘é‡æœç´¢
- âœ… ç¹é«”ä¸­æ–‡èªè¨€æ”¯æ´ç‚º**æœ€é«˜å„ªå…ˆç´š**ï¼ˆç³»çµ±èªè¨€ï¼šzh-TWï¼‰
- âœ… æˆæœ¬æ§åˆ¶åš´æ ¼ï¼ˆæ¯æ—¥ $5ã€æ¯æœˆ $50 é™é¡ï¼‰
- âš ï¸ **OpenAI embeddings åœ¨ç¹ä¸­è©•æ¸¬ä¸­è¡¨ç¾ä¸ä½³**ï¼ˆæ’å27å’Œ37ï¼‰ï¼Œä¸æ¨è–¦ä½¿ç”¨
- âœ… **é–‹æºæ¨¡å‹è¡¨ç¾å„ªç•°**ï¼Œå‰15åä¸­æœ‰7å€‹é–‹æºæ¨¡å‹

---

## ğŸ¯ å°ˆæ¡ˆéœ€æ±‚åˆ†æ

### ç•¶å‰æ¶æ§‹ (Phase 1)

```yaml
knowledge_base:
  indexing:
    full_text_search: true   # âœ… SQLite FTS5 å·²å•Ÿç”¨
    vector_search: false     # âŒ å‘é‡æœç´¢å¾…å¯¦ä½œ

  features:
    - 644å¼µ Zettelkasten å¡ç‰‡ç´¢å¼•
    - 40ç¯‡å­¸è¡“è«–æ–‡ç®¡ç†
    - 2,847å€‹é€£çµé—œä¿‚
    - é—œéµè©å…¨æ–‡æœç´¢ï¼ˆFTS5ï¼‰
```

### æ½›åœ¨æ‡‰ç”¨å ´æ™¯ (Phase 2-4)

| åŠŸèƒ½ | éšæ®µ | Embeddings ç”¨é€” | å„ªå…ˆç´š |
|------|------|----------------|--------|
| **è«–æ–‡ç›¸ä¼¼åº¦æ¨è–¦** | Phase 2 | è¨ˆç®—è«–æ–‡å‘é‡è·é›¢ï¼Œæ¨è–¦ç›¸é—œæ–‡ç» | P0 |
| **Zettelkasten é€£çµå»ºè­°** | Phase 2 | è‡ªå‹•ç™¼ç¾æ¦‚å¿µç›¸ä¼¼çš„å¡ç‰‡ | P0 |
| **èªç¾©æœç´¢å¢å¼·** | Phase 2 | FTS5 + Vector Hybrid Search | P1 |
| **æ¦‚å¿µæ˜ å°„** | Phase 2 | èšé¡åˆ†æã€ä¸»é¡Œå»ºæ¨¡ | P1 |
| **é—œä¿‚ç™¼ç¾** | Phase 2 | å¼•ç”¨ç¶²çµ¡åˆ†æã€å…±ç¾åˆ†æ | P2 |
| **å¤šæ¨¡æ…‹æœç´¢** | Phase 3 | è«–æ–‡åœ–è¡¨ + æ–‡å­—è¯åˆæœç´¢ | P3 |

### æŠ€è¡“ç´„æŸ

| ç´„æŸé …ç›® | ç•¶å‰ç‹€æ…‹ | å½±éŸ¿ |
|---------|---------|------|
| **èªè¨€** | ç¹é«”ä¸­æ–‡ï¼ˆzh-TWï¼‰ | âœ… **å¿…é ˆæ”¯æ´ç¹é«”ä¸­æ–‡** |
| **æˆæœ¬é™åˆ¶** | æ¯æ—¥ $5 / æ¯æœˆ $50 | âœ… å„ªå…ˆè€ƒæ…®å…è²»æˆ–ä½æˆæœ¬ API |
| **æ•¸æ“šè¦æ¨¡** | 40ç¯‡è«–æ–‡ + 644å¼µå¡ç‰‡ | âœ… ä¸­å°è¦æ¨¡ï¼Œé©åˆæœ¬åœ°/é›²ç«¯æ··åˆ |
| **éƒ¨ç½²æ–¹å¼** | æœ¬åœ°ï¼ˆOllamaï¼‰+ é›²ç«¯ API | âœ… æ”¯æ´é›¢ç·šå’Œç·šä¸Šæ··åˆ |
| **å­˜å„²è€ƒé‡** | SQLite æ•¸æ“šåº« | âœ… ç¶­åº¦ 512-1024 è¼ƒä½³ï¼ˆé™ä½å­˜å„²æˆæœ¬ï¼‰ |

---

## ğŸ† æ¨è–¦æ¨¡å‹æ¸…å–®

### ç¬¬ä¸€å„ªå…ˆç´šï¼ˆç«‹å³æ•´åˆï¼‰â­â­â­â­â­

#### 1. **Voyage AI - voyage-3-large**

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9877**ï¼ˆä¸¦åˆ—ç¬¬1åï¼‰
- **MRR**: **0.9364**ï¼ˆç¬¬2åï¼‰
- **ç¶­åº¦**: 1024
- **æˆæœ¬**: **$0.18/1M tokens** = $0.00018/1k tokens
- **ç¹é«”ä¸­æ–‡**: âœ… æ”¯æ´ï¼ˆå¤šèªè¨€æ¨¡å‹ï¼‰
- **License**: å°ˆæœ‰ï¼ˆAPIï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **æ€§èƒ½ç¬¬ä¸€**ï¼šHit Rate èˆ‡ Gemini ä¸¦åˆ—æ¦œé¦–
2. âœ… **ç¶­åº¦é©ä¸­**ï¼š1024 ç¶­ï¼ˆå­˜å„²å‹å¥½ï¼‰
3. âœ… **å°ˆæ¥­å‘é‡æ¨¡å‹**ï¼šå°ˆæ³¨æ–¼ embeddingsï¼Œéé€šç”¨ LLM
4. âœ… **æ‰¹æ¬¡è™•ç†å‹å¥½**ï¼šæ”¯æ´å¤§æ‰¹é‡åµŒå…¥ï¼ˆé©åˆæ‰¹æ¬¡è™•ç† PDFï¼‰
5. âœ… **æˆæœ¬å¯æ§**ï¼š644å¼µå¡ç‰‡ + 40ç¯‡è«–æ–‡ â‰ˆ **$0.10**ï¼ˆä¸€æ¬¡æ€§ï¼‰

**æ•´åˆé›£åº¦**: â­â­ ä½ï¼ˆéœ€ç”³è«‹ Voyage API keyï¼‰

**ä½¿ç”¨å»ºè­°**: **é¦–é¸æ–¹æ¡ˆ**ï¼ˆæ€§èƒ½ã€æˆæœ¬ã€ç¶­åº¦çš„æœ€ä½³å¹³è¡¡ï¼‰

**æˆæœ¬ä¼°ç®—**:
```
åˆå§‹åµŒå…¥ï¼š
- 644å¼µå¡ç‰‡ï¼ˆå¹³å‡500 tokens/å¼µï¼‰= 322k tokens Ã— $0.18/1M = $0.058
- 40ç¯‡è«–æ–‡ï¼ˆå¹³å‡5000 tokens/ç¯‡ï¼‰= 200k tokens Ã— $0.18/1M = $0.036
ç¸½è¨ˆ: $0.094 â‰ˆ $0.10

æœˆé‹ç‡Ÿï¼ˆ10è«–æ–‡ + 100å¡ç‰‡ + 1000æŸ¥è©¢ï¼‰:
- åµŒå…¥ï¼š60k tokens Ã— $0.18/1M = $0.011
- æŸ¥è©¢ï¼š50k tokens Ã— $0.18/1M = $0.009
æœˆç¸½è¨ˆ: $0.02
```

**API æ•´åˆ**:
```python
import requests

def get_voyage_embedding(texts: List[str], model="voyage-3-large") -> List[List[float]]:
    """Voyage AI Embeddings API"""
    response = requests.post(
        "https://api.voyageai.com/v1/embeddings",
        headers={
            "Authorization": f"Bearer {os.getenv('VOYAGE_API_KEY')}",
            "Content-Type": "application/json"
        },
        json={
            "input": texts,
            "model": model
        }
    )
    return [item['embedding'] for item in response.json()['data']]
```

---

#### 2. **Google Gemini - gemini-embedding-001**

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9877**ï¼ˆä¸¦åˆ—ç¬¬1åï¼‰
- **MRR**: **0.9379**ï¼ˆç¬¬1åï¼‰
- **ç¶­åº¦**: **3072**
- **æˆæœ¬**: **$0.15/1M tokens** = $0.00015/1k tokens
- **ç¹é«”ä¸­æ–‡**: âœ… åŸç”Ÿæ”¯æ´
- **License**: å°ˆæœ‰ï¼ˆAPIï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **æ€§èƒ½æœ€ä½³**ï¼šMRR æ’åç¬¬1ï¼ˆæª¢ç´¢ç²¾æº–åº¦æœ€é«˜ï¼‰
2. âœ… **API å·²æ•´åˆ**ï¼šå°ˆæ¡ˆå·²ä½¿ç”¨ Gemini ä½œç‚º LLM å¾Œç«¯
3. âœ… **æˆæœ¬ä½æ–¼ Voyage**ï¼š$0.15/1M vs $0.18/1M
4. âš ï¸ **ç¶­åº¦è¼ƒé«˜**ï¼š3072 ç¶­ï¼ˆå­˜å„²éœ€æ±‚è¼ƒå¤§ï¼‰

**æ•´åˆé›£åº¦**: â­ æ¥µä½ï¼ˆAPI å·²å¯ç”¨ï¼ŒGOOGLE_API_KEY å·²è¨­ç½®ï¼‰

**ä½¿ç”¨å»ºè­°**: **å“è³ªå„ªå…ˆç­–ç•¥**çš„é¦–é¸ï¼ˆ`quality_first` æ¨¡å¼ï¼‰

**æˆæœ¬ä¼°ç®—**:
```
åˆå§‹åµŒå…¥ï¼š
- 644å¼µå¡ç‰‡ + 40ç¯‡è«–æ–‡ = 522k tokens Ã— $0.15/1M = $0.078

æœˆé‹ç‡Ÿï¼š
- 10è«–æ–‡ + 100å¡ç‰‡ + 1000æŸ¥è©¢ = 110k tokens Ã— $0.15/1M = $0.017
```

**ç¶­åº¦è€ƒé‡**:
- 3072 ç¶­æ¯æ¢è¨˜éŒ„ç´„ 12KBï¼ˆfloat32ï¼‰
- 684æ¢è¨˜éŒ„ â‰ˆ 8.2MBï¼ˆå¯æ¥å—ï¼‰
- å¯è€ƒæ…®é™ç¶­åˆ° 1024 ç¶­ï¼ˆä¿ç•™ 90% æ€§èƒ½ï¼‰

**API æ•´åˆ**:
```python
import google.generativeai as genai

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def get_gemini_embedding(text: str, task_type="retrieval_document") -> List[float]:
    """Google Gemini Embeddings API

    task_type options:
    - retrieval_document: ç”¨æ–¼åµŒå…¥æ–‡æª”
    - retrieval_query: ç”¨æ–¼åµŒå…¥æŸ¥è©¢
    """
    result = genai.embed_content(
        model="models/text-embedding-004",  # æˆ– embedding-001
        content=text,
        task_type=task_type
    )
    return result['embedding']
```

---

#### 3. **Qwen3-Embedding-4B** â­ é–‹æºé¦–é¸

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9705**ï¼ˆç¬¬6åï¼‰
- **MRR**: **0.9022**ï¼ˆç¬¬6åï¼‰
- **ç¶­åº¦**: 2560
- **æˆæœ¬**: **å®Œå…¨å…è²»**ï¼ˆé–‹æºæ¨¡å‹ï¼‰
- **ç¹é«”ä¸­æ–‡**: âœ… åŸç”Ÿæ”¯æ´ï¼ˆé˜¿é‡Œé›²é€šç¾©åƒå•ï¼‰
- **License**: é–‹æºï¼ˆApache 2.0ï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **æ€§èƒ½å„ªç•°**ï¼šå‰10åä¸­å”¯ä¸€çš„é–‹æºæ¨¡å‹
2. âœ… **å®Œå…¨å…è²»**ï¼šæœ¬åœ°éƒ¨ç½²ï¼Œç„¡ API é™åˆ¶
3. âœ… **ä¸­æ–‡å°ˆå®¶**ï¼šé˜¿é‡Œé›²é‡å°ä¸­æ–‡å„ªåŒ–
4. âœ… **æ•¸æ“šéš±ç§**ï¼šæ•æ„Ÿè«–æ–‡ä¸éœ€ä¸Šå‚³é›²ç«¯
5. âš ï¸ **ç¶­åº¦è¼ƒé«˜**ï¼š2560 ç¶­ï¼ˆä»‹æ–¼ Voyage å’Œ Gemini ä¹‹é–“ï¼‰

**æ•´åˆé›£åº¦**: â­â­â­ ä¸­ï¼ˆéœ€æœ¬åœ°éƒ¨ç½²æˆ– HuggingFaceï¼‰

**éƒ¨ç½²æ–¹å¼ A - HuggingFace**:
```python
from sentence_transformers import SentenceTransformer

# ä¸‹è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡ç´„ 8GBï¼‰
model = SentenceTransformer('Alibaba-NLP/gte-Qwen2-7B-instruct')

# åµŒå…¥æ–‡æœ¬
embeddings = model.encode([
    "Zettelkasten åŸå­ç­†è¨˜ç³»çµ±",
    "çŸ¥è­˜ç®¡ç†èˆ‡ç¬¬äºŒå¤§è…¦"
])
```

**éƒ¨ç½²æ–¹å¼ B - Ollama** (å¦‚æœæ”¯æ´):
```bash
# æª¢æŸ¥æ˜¯å¦æœ‰ Qwen embeddings æ¨¡å‹
ollama list | grep qwen

# å¦‚æœæœ‰å‰‡ç›´æ¥ä½¿ç”¨
ollama pull qwen-embedding:4b
```

**ä½¿ç”¨å»ºè­°**: **éš±ç§å„ªå…ˆ + å¤§æ‰¹é‡å ´æ™¯**ï¼ˆè™•ç†æ•æ„Ÿè«–æ–‡æˆ–éœ€ç„¡é™åµŒå…¥ï¼‰

---

#### 4. **multilingual-e5-large** â­ é–‹æºæ¬¡é¸

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9579**ï¼ˆç¬¬9åï¼‰
- **MRR**: **0.8850**ï¼ˆç¬¬9åï¼‰
- **ç¶­åº¦**: 1024
- **æˆæœ¬**: **å®Œå…¨å…è²»**ï¼ˆé–‹æºæ¨¡å‹ï¼‰
- **ç¹é«”ä¸­æ–‡**: âœ… åŸç”Ÿæ”¯æ´ï¼ˆ100+ èªè¨€ï¼‰
- **License**: é–‹æºï¼ˆMITï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **ç¶­åº¦æœ€å„ª**ï¼š1024 ç¶­ï¼ˆæ€§èƒ½èˆ‡å­˜å„²å¹³è¡¡ï¼‰
2. âœ… **å¤šèªè¨€å°ˆå®¶**ï¼šå°ˆç‚ºè·¨èªè¨€æª¢ç´¢å„ªåŒ–
3. âœ… **ç¤¾ç¾¤æ´»èº**ï¼šMicrosoft Research ç¶­è­·ï¼Œ12.8k+ GitHub stars
4. âœ… **æ˜“æ–¼éƒ¨ç½²**ï¼šsentence-transformers ç›´æ¥æ”¯æ´
5. âœ… **æ€§èƒ½ç©©å®š**ï¼šHit Rate æ¥è¿‘ 96%

**æ•´åˆé›£åº¦**: â­â­ ä½ï¼ˆpip install sentence-transformersï¼‰

**éƒ¨ç½²æŒ‡ä»¤**:
```bash
# å®‰è£ä¾è³´
pip install sentence-transformers

# Python ä½¿ç”¨
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('intfloat/multilingual-e5-large')

# åµŒå…¥æ–‡æœ¬ï¼ˆæ³¨æ„ï¼šéœ€è¦åŠ å‰ç¶´ï¼‰
docs = ["passage: " + text for text in documents]
queries = ["query: " + text for text in search_queries]

doc_embeddings = model.encode(docs)
query_embeddings = model.encode(queries)
```

**ä½¿ç”¨å»ºè­°**: **é–‹æºå¹³è¡¡æ–¹æ¡ˆ**ï¼ˆæ€§èƒ½ã€ç¶­åº¦ã€æ˜“ç”¨æ€§çš„æœ€ä½³å¹³è¡¡ï¼‰

---

### ç¬¬äºŒå„ªå…ˆç´šï¼ˆä¸­æœŸæ•´åˆï¼‰â­â­â­â­

#### 5. **voyage-3.5-lite** - ä½æˆæœ¬é›²ç«¯æ–¹æ¡ˆ

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9579**ï¼ˆä¸¦åˆ—ç¬¬9åï¼‰
- **MRR**: **0.8844**ï¼ˆç¬¬10åï¼‰
- **ç¶­åº¦**: 1024
- **æˆæœ¬**: **$0.02/1M tokens**ï¼ˆè¶…ä½æˆæœ¬ï¼‰
- **ç¹é«”ä¸­æ–‡**: âœ… æ”¯æ´
- **License**: å°ˆæœ‰ï¼ˆAPIï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **è¶…ä½æˆæœ¬**ï¼šåƒ…ç‚º voyage-3-large çš„ 1/9
2. âœ… **æ€§èƒ½ä»å„ª**ï¼šHit Rate 95.79%ï¼ˆèˆ‡ e5-large ç›¸åŒï¼‰
3. âœ… **ç¶­åº¦é©ä¸­**ï¼š1024 ç¶­
4. âœ… **é©åˆå¤§æ‰¹é‡**ï¼šæˆæœ¬æ•æ„Ÿå ´æ™¯çš„é›²ç«¯é¦–é¸

**æˆæœ¬ä¼°ç®—**:
```
åˆå§‹åµŒå…¥ï¼š522k tokens Ã— $0.02/1M = $0.010
æœˆé‹ç‡Ÿï¼š110k tokens Ã— $0.02/1M = $0.002
```

**ä½¿ç”¨å»ºè­°**: **æˆæœ¬å„ªå…ˆç­–ç•¥**ï¼ˆ`cost_first` æ¨¡å¼ï¼‰

---

#### 6. **Nomic Embed Text V2** - æœ¬åœ°éƒ¨ç½²é¦–é¸

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9513**ï¼ˆç¬¬14åï¼‰
- **MRR**: **0.8674**ï¼ˆç¬¬14åï¼‰
- **ç¶­åº¦**: 768
- **æˆæœ¬**: **å®Œå…¨å…è²»**ï¼ˆé–‹æºï¼‰
- **ç¹é«”ä¸­æ–‡**: âœ… æ”¯æ´
- **License**: é–‹æºï¼ˆApache 2.0ï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **Ollama åŸç”Ÿæ”¯æ´**ï¼šå°ˆæ¡ˆå·²æœ‰ Ollama åŸºç¤è¨­æ–½
2. âœ… **å®Œå…¨é›¢ç·š**ï¼šç„¡ API é™åˆ¶ï¼Œç„¡æˆæœ¬
3. âœ… **ç¶­åº¦å°**ï¼š768 ç¶­ï¼ˆå­˜å„²å‹å¥½ï¼‰
4. âœ… **éƒ¨ç½²ç°¡å–®**ï¼š`ollama pull nomic-embed-text`

**æ•´åˆé›£åº¦**: â­â­ ä½ï¼ˆOllama å·²å®‰è£ï¼‰

**éƒ¨ç½²æŒ‡ä»¤**:
```bash
# å®‰è£æ¨¡å‹
ollama pull nomic-embed-text

# æ¸¬è©¦
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "çŸ¥è­˜åœ–è­œèˆ‡èªç¾©ç¶²çµ¡"
}'
```

**Python æ•´åˆ**:
```python
import requests

def get_ollama_embedding(text: str, model="nomic-embed-text") -> List[float]:
    """Ollama æœ¬åœ° Embeddings"""
    response = requests.post(
        "http://localhost:11434/api/embeddings",
        json={"model": model, "prompt": text}
    )
    return response.json()['embedding']
```

**ä½¿ç”¨å»ºè­°**: **æœ¬åœ°å„ªå…ˆæ–¹æ¡ˆ**ï¼ˆéš±ç§ã€æˆæœ¬ã€é›¢ç·šéœ€æ±‚ï¼‰

---

#### 7. **bge-m3** - Hybrid Search å°ˆç”¨

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9562**ï¼ˆç¬¬11åï¼‰
- **MRR**: **0.8784**ï¼ˆç¬¬11åï¼‰
- **ç¶­åº¦**: 1024
- **æˆæœ¬**: **å®Œå…¨å…è²»**ï¼ˆé–‹æºï¼‰
- **ç¹é«”ä¸­æ–‡**: âœ… åŸç”Ÿæ”¯æ´
- **License**: é–‹æºï¼ˆApache 2.0ï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **Hybrid Search**ï¼šæ”¯æ´å¯†é›†å‘é‡ + ç¨€ç–å‘é‡ï¼ˆé¡ä¼¼ BM25ï¼‰
2. âœ… **ä¸­æ–‡å°ˆå®¶**ï¼šåŒ—äº¬æ™ºæºç ”ç©¶é™¢ï¼ˆBAAIï¼‰é–‹ç™¼
3. âœ… **å¤šåŠŸèƒ½**ï¼šæ”¯æ´é•·æ–‡æœ¬ï¼ˆæœ€é•· 8192 tokensï¼‰
4. âœ… **è·¨èªè¨€**ï¼š100+ èªè¨€æ”¯æ´

**æ•´åˆé›£åº¦**: â­â­â­ ä¸­ï¼ˆéœ€å®‰è£ FlagEmbeddingï¼‰

**éƒ¨ç½²æŒ‡ä»¤**:
```bash
pip install -U FlagEmbedding

# Python ä½¿ç”¨
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# åµŒå…¥ï¼ˆæ”¯æ´ä¸‰ç¨®å‘é‡ï¼‰
embeddings = model.encode(
    ["çŸ¥è­˜ç®¡ç†ç³»çµ±", "Zettelkasten ç­†è¨˜æ³•"],
    return_dense=True,      # å¯†é›†å‘é‡
    return_sparse=True,     # ç¨€ç–å‘é‡ï¼ˆBM25-likeï¼‰
    return_colbert_vecs=False
)
```

**ä½¿ç”¨å»ºè­°**: **Phase 2 relation-finder éšæ®µ**ï¼ˆéœ€è¦ Hybrid Searchï¼‰

---

### ç¬¬ä¸‰å„ªå…ˆç´šï¼ˆé•·æœŸæ¢ç´¢ï¼‰â­â­â­

#### 8. **voyage-multimodal-3** - å¤šæ¨¡æ…‹å°ˆç”¨

**è©•ä¼°æ•¸æ“š** (å¯¦æ¸¬æ•¸æ“š):
- **Hit Rate**: **0.9751**ï¼ˆç¬¬3åï¼‰
- **MRR**: **0.9062**ï¼ˆç¬¬3åï¼‰
- **ç¶­åº¦**: 1024
- **æˆæœ¬**: **$0.12/1M tokens**
- **ç‰¹æ€§**: **æ”¯æ´æ–‡å­— + åœ–åƒè¯åˆåµŒå…¥**
- **ç¹é«”ä¸­æ–‡**: âœ… æ”¯æ´
- **License**: å°ˆæœ‰ï¼ˆAPIï¼‰

**æ¨è–¦ç†ç”±**:
1. âœ… **å¤šæ¨¡æ…‹**ï¼šå¯è™•ç†è«–æ–‡åœ–è¡¨å’Œæ–‡å­—
2. âœ… **æ€§èƒ½ç¬¬3**ï¼šåƒ…æ¬¡æ–¼å…©å€‹ç´”æ–‡å­—æ¨¡å‹
3. âœ… Phase 3 éœ€æ±‚ï¼šviz-generator å¯èƒ½éœ€è¦åœ–åƒç†è§£

**æ•´åˆæ™‚æ©Ÿ**: **Phase 3**ï¼ˆè¦–è¦ºåŒ–ç”Ÿæˆéšæ®µï¼‰

**ä½¿ç”¨å ´æ™¯**:
- è«–æ–‡åœ–è¡¨æª¢ç´¢ï¼ˆæŸ¥æ‰¾ç›¸ä¼¼çš„å¯¦é©—çµæœåœ–ï¼‰
- åœ–æ–‡è¯åˆæœç´¢ï¼ˆ"æ‰¾å‡ºåŒ…å«ç¥ç¶“ç¶²çµ¡æ¶æ§‹åœ–çš„è«–æ–‡"ï¼‰
- å¤šæ¨¡æ…‹ Zettelkastenï¼ˆæ”¯æ´åœ–ç‰‡å¡ç‰‡ï¼‰

---

## ğŸ“Š æ¨¡å‹å°æ¯”çŸ©é™£ï¼ˆå®Œæ•´ç‰ˆï¼‰

### Top 15 æ¨¡å‹ï¼ˆæŒ‰ Hit Rate æ’åºï¼‰

| æ’å | æ¨¡å‹åç¨± | Hit Rate | MRR | ç¶­åº¦ | æˆæœ¬/1M tokens | License | æ¨è–¦åº¦ |
|------|---------|---------|-----|------|---------------|---------|--------|
| 1 | **gemini-embedding-001** | 0.9877 | 0.9379 | 3072 | $0.15 | å°ˆæœ‰ | â­â­â­â­â­ |
| 1 | **voyage-3-large** | 0.9877 | 0.9364 | 1024 | $0.18 | å°ˆæœ‰ | â­â­â­â­â­ |
| 3 | **voyage-multimodal-3** | 0.9751 | 0.9062 | 1024 | $0.12 | å°ˆæœ‰ | â­â­â­ |
| 4 | voyage-multilingual-2 | 0.9737 | 0.9034 | 1024 | $0.12 | å°ˆæœ‰ | â­â­â­ |
| 5 | Cohere Embed 4 | 0.9725 | 0.9074 | 1536 | $0.12 | å°ˆæœ‰ | â­â­â­ |
| 6 | **Qwen3-Embedding-4B** | 0.9705 | 0.9022 | 2560 | **å…è²»** | é–‹æº | â­â­â­â­â­ |
| 7 | voyage-3.5 | 0.9665 | 0.9006 | 1024 | $0.06 | å°ˆæœ‰ | â­â­â­â­ |
| 8 | voyage-3 | 0.9654 | 0.8945 | 1024 | $0.06 | å°ˆæœ‰ | â­â­â­â­ |
| 9 | **multilingual-e5-large** | 0.9579 | 0.8850 | 1024 | **å…è²»** | é–‹æº | â­â­â­â­â­ |
| 10 | **voyage-3.5-lite** | 0.9579 | 0.8844 | 1024 | $0.02 | å°ˆæœ‰ | â­â­â­â­ |
| 11 | **bge-m3** | 0.9562 | 0.8784 | 1024 | **å…è²»** | é–‹æº | â­â­â­â­ |
| 12 | multilingual-e5-small | 0.9551 | 0.8723 | 384 | **å…è²»** | é–‹æº | â­â­â­ |
| 13 | multilingual-e5-base | 0.9522 | 0.8694 | 768 | **å…è²»** | é–‹æº | â­â­â­ |
| 14 | **Nomic Embed Text V2** | 0.9513 | 0.8674 | 768 | **å…è²»** | é–‹æº | â­â­â­â­ |
| 15 | voyage-3-lite | 0.9485 | 0.8625 | 512 | $0.02 | å°ˆæœ‰ | â­â­â­ |

### ç¹é«”ä¸­æ–‡å°ˆç”¨æ¨¡å‹ï¼ˆ20-25åï¼‰

| æ’å | æ¨¡å‹åç¨± | Hit Rate | MRR | ç¶­åº¦ | æˆæœ¬ | æ¨è–¦åº¦ |
|------|---------|---------|-----|------|------|--------|
| 22 | stella-base-zh-v2 | 0.9190 | 0.8194 | 768 | å…è²» | â­â­â­ |
| 24 | stella-large-zh-v2 | 0.9161 | 0.8135 | 1024 | å…è²» | â­â­â­ |
| 25 | bge-base-zh-v1.5 | 0.9061 | 0.8034 | 768 | å…è²» | â­â­ |
| 26 | bge-large-zh-v1.5 | 0.9052 | 0.7999 | 1024 | å…è²» | â­â­ |

**è§€å¯Ÿ**: é€šç”¨å¤šèªè¨€æ¨¡å‹ï¼ˆe5ã€Qwen3ï¼‰åœ¨ç¹ä¸­è¡¨ç¾å„ªæ–¼ä¸­æ–‡å°ˆç”¨æ¨¡å‹ï¼ˆstellaã€bge-zhï¼‰

---

## âš ï¸ ä¸æ¨è–¦çš„æ¨¡å‹

### OpenAI Embeddingsï¼ˆè¡¨ç¾ä¸ä½³ï¼‰

| æ¨¡å‹ | æ’å | Hit Rate | MRR | åŸå›  |
|------|------|---------|-----|------|
| **text-embedding-3-large** | 27 | 0.9044 | 0.7895 | MRR éä½ï¼ˆåƒ…0.79ï¼‰ï¼Œä¸å¦‚å…è²»çš„ e5-large |
| **text-embedding-3-small** | 37 | 0.8683 | 0.7533 | æ€§èƒ½å¤§å¹…ä½æ–¼é æœŸï¼Œæ’åå¢Šåº• |
| **text-embedding-ada-002** | 39 | 0.8569 | 0.7433 | èˆŠç‰ˆæ¨¡å‹ï¼Œå·²è¢«å–ä»£ |

**çµè«–**: âŒ **OpenAI embeddings åœ¨ç¹é«”ä¸­æ–‡å ´æ™¯ä¸­ä¸é©ç”¨**

### å…¶ä»–ä¸æ¨è–¦æ¨¡å‹

| æ¨¡å‹ | åŸå›  |
|------|------|
| **embed-english-v3.0 (Cohere)** | åƒ…æ”¯æ´è‹±æ–‡ï¼Œç¹ä¸­ Hit Rate: 0.4901 |
| **embed-text-v1.5 (Nomic)** | èˆŠç‰ˆï¼Œå·²è¢« V2 å–ä»£ |
| **embeddinggemma-300m** | æ€§èƒ½éä½ï¼ˆHit Rate: 0.7612ï¼‰ |

---

## ğŸš€ å¯¦æ–½è·¯ç·šåœ–

### Phase 1.5: å‘é‡æœç´¢åŸºç¤è¨­æ–½ï¼ˆ1-2é€±ï¼‰

**ç›®æ¨™**: å»ºç«‹å‘é‡æœç´¢èƒ½åŠ›

**ä»»å‹™**:
1. âœ… åœ¨ `settings.yaml` æ–°å¢ embeddings é…ç½®å€å¡Š
2. âœ… é¸æ“‡å‘é‡å­˜å„²æ–¹æ¡ˆï¼ˆChromaDB æˆ– SQLiteï¼‰
3. âœ… æ•´åˆ **voyage-3-large** æˆ– **gemini-embedding-001**ï¼ˆäºŒé¸ä¸€ï¼‰
4. âœ… ç‚º 40ç¯‡è«–æ–‡ç”ŸæˆåµŒå…¥
5. âœ… å¯¦ä½œåŸºæœ¬èªç¾©æœç´¢ API

**äº¤ä»˜ç‰©**:
```python
# src/knowledge_base/kb_manager.py æ–°å¢æ–¹æ³•

def add_paper_embedding(self, paper_id: int, embedding: List[float]) -> bool:
    """æ–°å¢è«–æ–‡å‘é‡"""

def search_papers_semantic(self, query: str, limit: int = 10) -> List[Dict]:
    """èªç¾©æœç´¢è«–æ–‡"""

def get_similar_papers(self, paper_id: int, limit: int = 5) -> List[Dict]:
    """æŸ¥æ‰¾ç›¸ä¼¼è«–æ–‡"""
```

**é æœŸæˆæœ**:
- âœ… 40ç¯‡è«–æ–‡å®Œæˆå‘é‡åµŒå…¥
- âœ… èªç¾©æœç´¢åŠŸèƒ½å¯ç”¨
- âœ… æ¨è–¦ç³»çµ±åŸå‹å®Œæˆ

**æ™‚é–“**: 1-2é€±
**æˆæœ¬**: $0.10-0.15ï¼ˆä¸€æ¬¡æ€§åµŒå…¥ï¼‰

---

### Phase 2: Zettelkasten å‘é‡åŒ–ï¼ˆ2-3é€±ï¼‰

**ç›®æ¨™**: æå‡ auto_link æˆåŠŸç‡ï¼ˆ0% â†’ 80%+ï¼‰

**ä»»å‹™**:
1. âœ… ç‚º 644å¼µ Zettelkasten å¡ç‰‡ç”ŸæˆåµŒå…¥
2. âœ… å¯¦ä½œ `auto_link_zettel_v3()`ï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰
3. âœ… æ•´åˆ **multilingual-e5-large** æˆ– **Nomic V2** ä½œç‚ºæœ¬åœ°å‚™é¸
4. âœ… å»ºç«‹é€£çµå»ºè­°ç³»çµ±

**äº¤ä»˜ç‰©**:
```python
def add_zettel_embedding(self, card_id: int, embedding: List[float]) -> bool:
    """æ–°å¢å¡ç‰‡å‘é‡"""

def search_zettel_semantic(self, query: str, limit: int = 20) -> List[Dict]:
    """èªç¾©æœç´¢å¡ç‰‡"""

def get_similar_zettel(self, card_id: int, limit: int = 10) -> List[Dict]:
    """æŸ¥æ‰¾ç›¸ä¼¼å¡ç‰‡"""

def suggest_zettel_links(self, card_id: int, threshold: float = 0.7) -> List[Dict]:
    """è‡ªå‹•å»ºè­°é€£çµï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰"""

def auto_link_zettel_v3(self, similarity_threshold: float = 0.7) -> Dict:
    """æ”¹é€²ç‰ˆè‡ªå‹•é—œè¯ï¼ˆå‘é‡ + å…ƒæ•¸æ“šæ··åˆï¼‰"""
```

**é æœŸæˆæœ**:
- âœ… auto_link æˆåŠŸç‡ >80%
- âœ… Zettelkasten é€£çµå»ºè­°åŠŸèƒ½ä¸Šç·š
- âœ… æ¦‚å¿µç›¸ä¼¼åº¦æª¢æ¸¬å¯ç”¨

**æ™‚é–“**: 2-3é€±
**æˆæœ¬**: $0.06ï¼ˆä¸€æ¬¡æ€§åµŒå…¥ 644å¼µå¡ç‰‡ï¼‰

---

### Phase 3: Hybrid Searchï¼ˆ3-4é€±ï¼‰

**ç›®æ¨™**: çµåˆå…¨æ–‡æœç´¢èˆ‡å‘é‡æœç´¢

**ä»»å‹™**:
1. âœ… æ•´åˆ FTS5 å…¨æ–‡æœç´¢ + å‘é‡æœç´¢
2. âœ… å¯¦ä½œ Rerankingï¼ˆä½¿ç”¨ **bge-m3** çš„ç¨€ç–å‘é‡ï¼‰
3. âœ… æ€§èƒ½æ¸¬è©¦å’Œå„ªåŒ–
4. âœ… å¯¦ä½œæŸ¥è©¢æ“´å±•ï¼ˆQuery Expansionï¼‰

**äº¤ä»˜ç‰©**:
```python
def search_hybrid(
    self,
    query: str,
    alpha: float = 0.5,
    limit: int = 20
) -> List[Dict]:
    """
    Hybrid Searchï¼ˆæ··åˆæœç´¢ï¼‰

    Args:
        query: æŸ¥è©¢å­—ä¸²
        alpha: æ¬Šé‡ï¼ˆ0=ç´”å‘é‡, 0.5=æ··åˆ, 1=ç´”å…¨æ–‡ï¼‰
        limit: è¿”å›çµæœæ•¸

    Returns:
        æ··åˆæ’åºçš„çµæœ
    """
```

**é æœŸæˆæœ**:
- âœ… æœç´¢æº–ç¢ºåº¦æå‡ 20-30%
- âœ… æ”¯æ´è¤‡é›œæŸ¥è©¢ï¼ˆå¸ƒæ—é‚è¼¯ + èªç¾©ï¼‰

**æ™‚é–“**: 3-4é€±

---

### Phase 4: å¤šæ¨¡æ…‹æ“´å±•ï¼ˆæœªä¾†ï¼‰

**ç›®æ¨™**: æ”¯æ´åœ–æ–‡è¯åˆæœç´¢

**ä»»å‹™**:
1. â³ æ•´åˆ **voyage-multimodal-3**
2. â³ ç‚ºè«–æ–‡åœ–è¡¨ç”ŸæˆåµŒå…¥
3. â³ å¯¦ä½œåœ–æ–‡è¯åˆæœç´¢

**æ™‚æ©Ÿ**: Phase 3 å®Œæˆå¾Œï¼ˆviz-generator éšæ®µï¼‰

---

## ğŸ’° æˆæœ¬åˆ†æï¼ˆä¿®æ­£ç‰ˆï¼‰

### æ–¹æ¡ˆå°æ¯”ï¼ˆåˆå§‹åŒ– + 1å¹´é‹ç‡Ÿï¼‰

| æ–¹æ¡ˆ | æ¨¡å‹çµ„åˆ | åˆå§‹åµŒå…¥ | æœˆé‹ç‡Ÿ | å¹´ç¸½æˆæœ¬ |
|------|---------|---------|--------|---------|
| **A. ç´”é›²ç«¯ï¼ˆé«˜å“è³ªï¼‰** | voyage-3-large | $0.094 | $0.02 | **$0.33** |
| **B. ç´”é›²ç«¯ï¼ˆä½æˆæœ¬ï¼‰** | voyage-3.5-lite | $0.010 | $0.002 | **$0.034** |
| **C. æ··åˆéƒ¨ç½²** | Gemini + Nomic V2 | $0.078 | $0.017 | **$0.28** |
| **D. ç´”æœ¬åœ°ï¼ˆå…è²»ï¼‰** | Qwen3-4B æˆ– e5-large | $0 | $0 | **$0** |

**æœˆé‹ç‡Ÿå‡è¨­**: 10ç¯‡æ–°è«–æ–‡ + 100å¼µæ–°å¡ç‰‡ + 1000æ¬¡æŸ¥è©¢

**çµè«–**: âœ… æ‰€æœ‰æ–¹æ¡ˆå¹´æˆæœ¬ < $1ï¼Œé ä½æ–¼ $50/æœˆ é™é¡

---

### è©³ç´°æˆæœ¬æ‹†è§£

#### æ–¹æ¡ˆ A: voyage-3-largeï¼ˆæ¨è–¦ï¼‰

```
åˆå§‹åŒ–ï¼ˆä¸€æ¬¡æ€§ï¼‰:
â”œâ”€ 40ç¯‡è«–æ–‡ï¼ˆ200k tokensï¼‰: $0.036
â”œâ”€ 644å¼µå¡ç‰‡ï¼ˆ322k tokensï¼‰: $0.058
â””â”€ ç¸½è¨ˆ: $0.094

æœˆé‹ç‡Ÿ:
â”œâ”€ 10ç¯‡æ–°è«–æ–‡ï¼ˆ50k tokensï¼‰: $0.009
â”œâ”€ 100å¼µæ–°å¡ç‰‡ï¼ˆ50k tokensï¼‰: $0.009
â”œâ”€ 1000æ¬¡æŸ¥è©¢ï¼ˆ10k tokensï¼‰: $0.002
â””â”€ æœˆç¸½è¨ˆ: $0.02

å¹´ç¸½æˆæœ¬: $0.094 + $0.02 Ã— 12 = $0.33
```

#### æ–¹æ¡ˆ B: voyage-3.5-liteï¼ˆè¶…ä½æˆæœ¬ï¼‰

```
åˆå§‹åŒ–: $0.010
æœˆé‹ç‡Ÿ: $0.002
å¹´ç¸½æˆæœ¬: $0.034
```

#### æ–¹æ¡ˆ C: gemini-embedding-001 + Nomic V2ï¼ˆæ··åˆï¼‰

```
åˆå§‹åŒ–ï¼ˆä½¿ç”¨ Geminiï¼‰: $0.078
æœˆé‹ç‡Ÿ:
â”œâ”€ è«–æ–‡åµŒå…¥ï¼ˆGeminiï¼‰: $0.008
â”œâ”€ å¡ç‰‡åµŒå…¥ï¼ˆNomic æœ¬åœ°ï¼‰: $0
â”œâ”€ æŸ¥è©¢ï¼ˆNomic æœ¬åœ°ï¼‰: $0
â””â”€ æœˆç¸½è¨ˆ: $0.008

å¹´ç¸½æˆæœ¬: $0.078 + $0.008 Ã— 12 = $0.174
```

#### æ–¹æ¡ˆ D: ç´”æœ¬åœ°ï¼ˆQwen3-4B æˆ– e5-largeï¼‰

```
ç¡¬é«”éœ€æ±‚:
- GPUè¨˜æ†¶é«”: 16GBï¼ˆæ¨è–¦ RTX 4060 æˆ–æ›´é«˜ï¼‰
- ç£ç¢Ÿç©ºé–“: 10GBï¼ˆæ¨¡å‹æª”æ¡ˆï¼‰
- CPUæ¨ç†å¯è¡Œä½†è¼ƒæ…¢

æˆæœ¬: $0ï¼ˆä¸€æ¬¡æ€§ä¸‹è¼‰å¾Œç„¡æˆæœ¬ï¼‰
```

---

## ğŸ”§ æŠ€è¡“å¯¦æ–½æŒ‡å—

### å‘é‡å­˜å„²æ–¹æ¡ˆæ¯”è¼ƒ

#### é¸é … A: SQLiteï¼ˆæ¨è–¦ç”¨æ–¼ Phase 1.5ï¼‰

**å„ªé»**:
- âœ… èˆ‡ç¾æœ‰ SQLite æ•´åˆç„¡ç¸«
- âœ… ç„¡éœ€é¡å¤–ä¾è³´
- âœ… é©åˆä¸­å°è¦æ¨¡ï¼ˆ< 10è¬æ¢ï¼‰

**ç¼ºé»**:
- âŒ å‘é‡æª¢ç´¢é€Ÿåº¦æ…¢ï¼ˆç·šæ€§æƒæï¼‰
- âŒ ä¸æ”¯æ´ HNSW/IVF ç´¢å¼•

**å¯¦ä½œ**:
```sql
-- æ–°å¢å‘é‡è¡¨
CREATE TABLE embeddings (
    id INTEGER PRIMARY KEY,
    entity_type TEXT,  -- 'paper' or 'zettel'
    entity_id INTEGER,
    embedding BLOB,    -- äºŒé€²åˆ¶å­˜å„²ï¼ˆnumpy arrayï¼‰
    model TEXT,
    dimensions INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (entity_id) REFERENCES papers(id) ON DELETE CASCADE
);

CREATE INDEX idx_embeddings_entity ON embeddings(entity_type, entity_id);
```

**æŸ¥è©¢æ–¹å¼**ï¼ˆCosine Similarityï¼‰:
```python
import numpy as np

def search_similar(query_embedding: np.ndarray, limit: int = 10):
    """å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆSQLiteï¼‰"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ç²å–æ‰€æœ‰å‘é‡
    cursor.execute("SELECT entity_id, embedding FROM embeddings WHERE entity_type='paper'")

    results = []
    for entity_id, embedding_blob in cursor.fetchall():
        # ååºåˆ—åŒ–å‘é‡
        embedding = np.frombuffer(embedding_blob, dtype=np.float32)

        # è¨ˆç®— Cosine Similarity
        similarity = np.dot(query_embedding, embedding) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
        )

        results.append((entity_id, similarity))

    # æ’åºä¸¦è¿”å› top-k
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:limit]
```

---

#### é¸é … B: ChromaDBï¼ˆæ¨è–¦ç”¨æ–¼ Phase 2ï¼‰â­

**å„ªé»**:
- âœ… å°ˆç‚º embeddings å„ªåŒ–
- âœ… æ”¯æ´å…ƒæ•¸æ“šéæ¿¾
- âœ… è‡ªå‹•å‘é‡ç´¢å¼•ï¼ˆHNSWï¼‰
- âœ… è¼•é‡ç´šï¼ˆç´” Pythonï¼‰
- âœ… æ”¯æ´å¤šç¨®è·é›¢åº¦é‡ï¼ˆcosine, L2, IPï¼‰

**ç¼ºé»**:
- âŒ éœ€é¡å¤–ä¾è³´ï¼ˆ`pip install chromadb`ï¼‰

**å¯¦ä½œ**:
```python
import chromadb
from chromadb.config import Settings

# åˆå§‹åŒ–ï¼ˆæŒä¹…åŒ–å­˜å„²ï¼‰
client = chromadb.PersistentClient(
    path="knowledge_base/vectors",
    settings=Settings(anonymized_telemetry=False)
)

# å‰µå»º Collection
papers_collection = client.get_or_create_collection(
    name="papers",
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ Cosine è·é›¢
)

zettel_collection = client.get_or_create_collection(
    name="zettel_cards",
    metadata={"hnsw:space": "cosine"}
)

# æ–°å¢æ–‡æª”
papers_collection.add(
    ids=[f"paper_{paper_id}"],
    embeddings=[embedding.tolist()],
    metadatas=[{
        "title": title,
        "year": year,
        "authors": json.dumps(authors),
        "keywords": json.dumps(keywords)
    }],
    documents=[abstract]  # å¯é¸ï¼šå­˜å„²åŸå§‹æ–‡æœ¬
)

# èªç¾©æœç´¢
results = papers_collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=10,
    where={
        "year": {"$gte": 2020}  # å…ƒæ•¸æ“šéæ¿¾
    },
    include=["metadatas", "distances", "documents"]
)

# è¨ªå•çµæœ
for i, (doc_id, metadata, distance) in enumerate(
    zip(results['ids'][0], results['metadatas'][0], results['distances'][0])
):
    print(f"{i+1}. {metadata['title']} (similarity: {1 - distance:.3f})")
```

**é·ç§»ç­–ç•¥**:
```python
def migrate_to_chromadb():
    """å¾ SQLite é·ç§»åˆ° ChromaDB"""
    # 1. å¾ SQLite è®€å–æ‰€æœ‰å‘é‡
    kb = KnowledgeBaseManager()
    papers = kb.list_papers()

    # 2. æ‰¹æ¬¡æ’å…¥ ChromaDB
    client = chromadb.PersistentClient(path="knowledge_base/vectors")
    collection = client.get_or_create_collection("papers")

    batch_size = 100
    for i in range(0, len(papers), batch_size):
        batch = papers[i:i+batch_size]

        collection.add(
            ids=[f"paper_{p['id']}" for p in batch],
            embeddings=[get_embedding(p['id']) for p in batch],
            metadatas=[{
                "title": p['title'],
                "year": p['year'],
                "authors": json.dumps(p['authors'])
            } for p in batch]
        )

    print(f"Migrated {len(papers)} papers to ChromaDB")
```

---

#### é¸é … C: Qdrantï¼ˆç”¨æ–¼å¤§è¦æ¨¡æ“´å±•ï¼‰

**é©ç”¨æ™‚æ©Ÿ**: è«–æ–‡æ•¸é‡ > 10è¬ç¯‡ æˆ– éœ€è¦åˆ†å¸ƒå¼éƒ¨ç½²

**å„ªé»**:
- âœ… é«˜æ€§èƒ½ï¼ˆRust å¯¦ä½œï¼‰
- âœ… æ”¯æ´ gRPC å’Œ HTTP API
- âœ… è±å¯Œçš„éæ¿¾å™¨
- âœ… æ”¯æ´é‡åŒ–å’Œç¨€ç–å‘é‡

**å¯¦ä½œ**:
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# åˆå§‹åŒ–å®¢æˆ¶ç«¯
client = QdrantClient(path="knowledge_base/qdrant_storage")

# å‰µå»º Collection
client.create_collection(
    collection_name="papers",
    vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
)

# æ’å…¥å‘é‡
client.upsert(
    collection_name="papers",
    points=[
        PointStruct(
            id=paper_id,
            vector=embedding.tolist(),
            payload={"title": title, "year": year}
        )
    ]
)

# æœç´¢
results = client.search(
    collection_name="papers",
    query_vector=query_embedding.tolist(),
    limit=10,
    query_filter={"year": {"gte": 2020}}
)
```

---

### API æ•´åˆå®Œæ•´ç¯„ä¾‹

#### 1. Voyage AIï¼ˆæ¨è–¦ï¼‰

```python
import os
import requests
from typing import List, Union

class VoyageEmbeddings:
    """Voyage AI Embeddings å°è£"""

    def __init__(self, api_key: str = None, model: str = "voyage-3-large"):
        self.api_key = api_key or os.getenv("VOYAGE_API_KEY")
        self.model = model
        self.base_url = "https://api.voyageai.com/v1/embeddings"

    def embed(self, texts: Union[str, List[str]]) -> List[List[float]]:
        """åµŒå…¥æ–‡æœ¬"""
        if isinstance(texts, str):
            texts = [texts]

        response = requests.post(
            self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={"input": texts, "model": self.model}
        )

        response.raise_for_status()
        return [item['embedding'] for item in response.json()['data']]

    def embed_documents(self, documents: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡åµŒå…¥ï¼ˆè‡ªå‹•åˆ†æ‰¹ï¼‰"""
        batch_size = 128  # Voyage API é™åˆ¶
        all_embeddings = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            embeddings = self.embed(batch)
            all_embeddings.extend(embeddings)

        return all_embeddings

# ä½¿ç”¨ç¯„ä¾‹
embedder = VoyageEmbeddings(model="voyage-3-large")

# åµŒå…¥è«–æ–‡
paper_embeddings = embedder.embed([
    "Zettelkasten æ˜¯ä¸€ç¨®åŸå­ç­†è¨˜æ–¹æ³•...",
    "çŸ¥è­˜ç®¡ç†ç³»çµ±çš„è¨­è¨ˆåŸå‰‡..."
])

# åµŒå…¥æŸ¥è©¢
query_embedding = embedder.embed("å¦‚ä½•å»ºç«‹ç¬¬äºŒå¤§è…¦ï¼Ÿ")[0]
```

---

#### 2. Google Gemini

```python
import google.generativeai as genai
from typing import List, Union

class GeminiEmbeddings:
    """Google Gemini Embeddings å°è£"""

    def __init__(self, api_key: str = None):
        genai.configure(api_key=api_key or os.getenv("GOOGLE_API_KEY"))
        self.model = "models/text-embedding-004"

    def embed(
        self,
        text: str,
        task_type: str = "retrieval_document"
    ) -> List[float]:
        """
        åµŒå…¥å–®å€‹æ–‡æœ¬

        task_type options:
        - retrieval_document: åµŒå…¥æ–‡æª”ï¼ˆç”¨æ–¼ç´¢å¼•ï¼‰
        - retrieval_query: åµŒå…¥æŸ¥è©¢ï¼ˆç”¨æ–¼æœç´¢ï¼‰
        - semantic_similarity: è¨ˆç®—ç›¸ä¼¼åº¦
        - classification: åˆ†é¡ä»»å‹™
        """
        result = genai.embed_content(
            model=self.model,
            content=text,
            task_type=task_type
        )
        return result['embedding']

    def embed_documents(self, documents: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡åµŒå…¥æ–‡æª”"""
        return [self.embed(doc, "retrieval_document") for doc in documents]

    def embed_queries(self, queries: List[str]) -> List[List[float]]:
        """æ‰¹æ¬¡åµŒå…¥æŸ¥è©¢"""
        return [self.embed(q, "retrieval_query") for q in queries]

# ä½¿ç”¨ç¯„ä¾‹
embedder = GeminiEmbeddings()

# åµŒå…¥æ–‡æª”ï¼ˆç”¨æ–¼ç´¢å¼•ï¼‰
doc_embeddings = embedder.embed_documents([
    "Zettelkasten ç­†è¨˜æ³•çš„æ ¸å¿ƒåŸå‰‡",
    "çŸ¥è­˜åœ–è­œçš„æ§‹å»ºæ–¹æ³•"
])

# åµŒå…¥æŸ¥è©¢ï¼ˆç”¨æ–¼æœç´¢ï¼‰- æ³¨æ„ä½¿ç”¨ä¸åŒçš„ task_type
query_embedding = embedder.embed("ä»€éº¼æ˜¯åŸå­ç­†è¨˜ï¼Ÿ", task_type="retrieval_query")
```

**é‡è¦**: Gemini çš„ `retrieval_document` å’Œ `retrieval_query` æœƒç”¢ç”Ÿä¸åŒçš„å‘é‡ç©ºé–“ï¼Œ**å¿…é ˆåˆ†åˆ¥ä½¿ç”¨**ï¼

---

#### 3. æœ¬åœ°éƒ¨ç½²ï¼ˆmultilingual-e5-largeï¼‰

```python
from sentence_transformers import SentenceTransformer
from typing import List
import numpy as np

class E5Embeddings:
    """multilingual-e5-large å°è£"""

    def __init__(self, model_name: str = "intfloat/multilingual-e5-large"):
        print(f"Loading model: {model_name}...")
        self.model = SentenceTransformer(model_name)
        print("Model loaded successfully!")

    def embed(self, texts: List[str], prefix: str = "passage") -> np.ndarray:
        """
        åµŒå…¥æ–‡æœ¬

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            prefix: å‰ç¶´ï¼ˆpassage æˆ– queryï¼‰

        æ³¨æ„: e5 æ¨¡å‹éœ€è¦åŠ å‰ç¶´ï¼
        """
        prefixed_texts = [f"{prefix}: {text}" for text in texts]
        return self.model.encode(prefixed_texts, normalize_embeddings=True)

    def embed_documents(self, documents: List[str]) -> np.ndarray:
        """åµŒå…¥æ–‡æª”ï¼ˆåŠ  passage å‰ç¶´ï¼‰"""
        return self.embed(documents, prefix="passage")

    def embed_queries(self, queries: List[str]) -> np.ndarray:
        """åµŒå…¥æŸ¥è©¢ï¼ˆåŠ  query å‰ç¶´ï¼‰"""
        return self.embed(queries, prefix="query")

# ä½¿ç”¨ç¯„ä¾‹
embedder = E5Embeddings()

# åµŒå…¥æ–‡æª”
docs = [
    "Zettelkasten æ˜¯ä¸€ç¨®åŸå­ç­†è¨˜æ–¹æ³•",
    "çŸ¥è­˜ç®¡ç†ç³»çµ±çš„è¨­è¨ˆåŸå‰‡"
]
doc_embeddings = embedder.embed_documents(docs)  # shape: (2, 1024)

# åµŒå…¥æŸ¥è©¢
queries = ["ä»€éº¼æ˜¯åŸå­ç­†è¨˜ï¼Ÿ", "å¦‚ä½•å»ºç«‹çŸ¥è­˜åœ–è­œï¼Ÿ"]
query_embeddings = embedder.embed_queries(queries)  # shape: (2, 1024)

# è¨ˆç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_embeddings, doc_embeddings)
print(similarities)  # shape: (2, 2)
```

---

#### 4. Ollama æœ¬åœ°ï¼ˆNomic Embed Text V2ï¼‰

```python
import requests
from typing import List, Union
import numpy as np

class OllamaEmbeddings:
    """Ollama Embeddings å°è£"""

    def __init__(
        self,
        model: str = "nomic-embed-text",
        base_url: str = "http://localhost:11434"
    ):
        self.model = model
        self.base_url = base_url

    def embed(self, text: str) -> List[float]:
        """åµŒå…¥å–®å€‹æ–‡æœ¬"""
        response = requests.post(
            f"{self.base_url}/api/embeddings",
            json={"model": self.model, "prompt": text}
        )
        response.raise_for_status()
        return response.json()['embedding']

    def embed_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹æ¬¡åµŒå…¥ï¼ˆé€ä¸€èª¿ç”¨ï¼‰"""
        embeddings = [self.embed(text) for text in texts]
        return np.array(embeddings)

# ä½¿ç”¨ç¯„ä¾‹
embedder = OllamaEmbeddings()

# åµŒå…¥æ–‡æœ¬
embedding = embedder.embed("Zettelkasten åŸå­ç­†è¨˜ç³»çµ±")
print(f"Embedding dimension: {len(embedding)}")  # 768

# æ‰¹æ¬¡åµŒå…¥
docs = ["çŸ¥è­˜ç®¡ç†", "ç¬¬äºŒå¤§è…¦", "Zettelkasten"]
embeddings = embedder.embed_batch(docs)
print(f"Shape: {embeddings.shape}")  # (3, 768)
```

---

### æ•´åˆåˆ° kb_manager.py

```python
# src/knowledge_base/kb_manager.py

import chromadb
import numpy as np
from typing import List, Dict, Optional, Union

class KnowledgeBaseManager:
    """çŸ¥è­˜åº«ç®¡ç†å™¨ï¼ˆå¢å¼·ç‰ˆ - æ”¯æ´å‘é‡æœç´¢ï¼‰"""

    def __init__(self, kb_root: str = "knowledge_base", db_path: Optional[str] = None):
        # ... ç¾æœ‰åˆå§‹åŒ–ä»£ç¢¼ ...

        # åˆå§‹åŒ–å‘é‡å­˜å„²
        self._init_vector_store()

        # åˆå§‹åŒ– Embeddings æ¨¡å‹
        self._init_embeddings()

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å„²ï¼ˆChromaDBï¼‰"""
        vector_path = self.kb_root / "vectors"
        self.vector_client = chromadb.PersistentClient(path=str(vector_path))

        # å‰µå»º Collections
        self.papers_vectors = self.vector_client.get_or_create_collection(
            name="papers",
            metadata={"hnsw:space": "cosine"}
        )

        self.zettel_vectors = self.vector_client.get_or_create_collection(
            name="zettel_cards",
            metadata={"hnsw:space": "cosine"}
        )

    def _init_embeddings(self):
        """åˆå§‹åŒ– Embeddings æ¨¡å‹ï¼ˆå¾é…ç½®è®€å–ï¼‰"""
        # å¾ settings.yaml è®€å–é…ç½®
        import yaml
        with open("config/settings.yaml") as f:
            config = yaml.safe_load(f)

        provider = config['embeddings']['default_provider']

        if provider == "voyage":
            from .embeddings import VoyageEmbeddings
            self.embedder = VoyageEmbeddings()
        elif provider == "google":
            from .embeddings import GeminiEmbeddings
            self.embedder = GeminiEmbeddings()
        elif provider == "local":
            from .embeddings import E5Embeddings
            self.embedder = E5Embeddings()
        elif provider == "ollama":
            from .embeddings import OllamaEmbeddings
            self.embedder = OllamaEmbeddings()

    def add_paper_embedding(self, paper_id: int) -> bool:
        """ç‚ºè«–æ–‡ç”Ÿæˆä¸¦å­˜å„²å‘é‡"""
        # 1. ç²å–è«–æ–‡ä¿¡æ¯
        paper = self.get_paper_by_id(paper_id)
        if not paper:
            return False

        # 2. çµ„åˆæ–‡æœ¬ï¼ˆæ¨™é¡Œ + æ‘˜è¦ + é—œéµè©ï¼‰
        text = f"{paper['title']}. {paper.get('abstract', '')}. Keywords: {', '.join(paper.get('keywords', []))}"

        # 3. ç”Ÿæˆå‘é‡
        embedding = self.embedder.embed(text)

        # 4. å­˜å„²åˆ° ChromaDB
        self.papers_vectors.add(
            ids=[f"paper_{paper_id}"],
            embeddings=[embedding],
            metadatas=[{
                "paper_id": paper_id,
                "title": paper['title'],
                "year": paper.get('year', 0),
                "authors": json.dumps(paper.get('authors', []))
            }],
            documents=[text]
        )

        return True

    def search_papers_semantic(
        self,
        query: str,
        limit: int = 10,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        """èªç¾©æœç´¢è«–æ–‡"""
        # 1. ç”ŸæˆæŸ¥è©¢å‘é‡
        query_embedding = self.embedder.embed(query, task_type="retrieval_query")

        # 2. æ§‹å»ºéæ¿¾æ¢ä»¶
        where = {}
        if year_from:
            where["year"] = {"$gte": year_from}

        # 3. å‘é‡æœç´¢
        results = self.papers_vectors.query(
            query_embeddings=[query_embedding],
            n_results=limit,
            where=where if where else None,
            include=["metadatas", "distances", "documents"]
        )

        # 4. æ ¼å¼åŒ–çµæœ
        papers = []
        for i, (doc_id, metadata, distance, document) in enumerate(zip(
            results['ids'][0],
            results['metadatas'][0],
            results['distances'][0],
            results['documents'][0]
        )):
            paper_id = metadata['paper_id']
            papers.append({
                "rank": i + 1,
                "paper_id": paper_id,
                "title": metadata['title'],
                "year": metadata['year'],
                "authors": json.loads(metadata['authors']),
                "similarity": 1 - distance,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦
                "snippet": document[:200] + "..."
            })

        return papers

    def get_similar_papers(self, paper_id: int, limit: int = 5) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼è«–æ–‡"""
        # 1. ç²å–è©²è«–æ–‡çš„å‘é‡
        result = self.papers_vectors.get(
            ids=[f"paper_{paper_id}"],
            include=["embeddings"]
        )

        if not result['embeddings']:
            return []

        paper_embedding = result['embeddings'][0]

        # 2. æŸ¥æ‰¾ç›¸ä¼¼è«–æ–‡ï¼ˆæ’é™¤è‡ªå·±ï¼‰
        results = self.papers_vectors.query(
            query_embeddings=[paper_embedding],
            n_results=limit + 1,  # +1 å› ç‚ºæœƒåŒ…å«è‡ªå·±
            include=["metadatas", "distances"]
        )

        # 3. éæ¿¾ä¸¦æ ¼å¼åŒ–
        similar_papers = []
        for doc_id, metadata, distance in zip(
            results['ids'][0],
            results['metadatas'][0],
            results['distances'][0]
        ):
            # è·³éè‡ªå·±
            if metadata['paper_id'] == paper_id:
                continue

            similar_papers.append({
                "paper_id": metadata['paper_id'],
                "title": metadata['title'],
                "year": metadata['year'],
                "similarity": 1 - distance
            })

        return similar_papers[:limit]

    # Zettelkasten å‘é‡æ–¹æ³•ï¼ˆé¡ä¼¼å¯¦ä½œï¼‰

    def add_zettel_embedding(self, card_id: int) -> bool:
        """ç‚º Zettelkasten å¡ç‰‡ç”Ÿæˆå‘é‡"""
        card = self.get_zettel_by_card_id(card_id)
        if not card:
            return False

        # çµ„åˆæ–‡æœ¬ï¼ˆæ¨™é¡Œ + æ ¸å¿ƒæ¦‚å¿µ + èªªæ˜ï¼‰
        text = f"{card['title']}. {card.get('core_concept', '')}. {card.get('description', '')}"

        embedding = self.embedder.embed(text)

        self.zettel_vectors.add(
            ids=[f"zettel_{card_id}"],
            embeddings=[embedding],
            metadatas=[{
                "card_id": card_id,
                "zettel_id": card['zettel_id'],
                "title": card['title'],
                "domain": card['domain'],
                "card_type": card['card_type']
            }],
            documents=[text]
        )

        return True

    def suggest_zettel_links(
        self,
        card_id: int,
        threshold: float = 0.7,
        limit: int = 10
    ) -> List[Dict]:
        """è‡ªå‹•å»ºè­° Zettelkasten é€£çµ"""
        # æŸ¥æ‰¾ç›¸ä¼¼å¡ç‰‡
        similar_cards = self.get_similar_zettel(card_id, limit=limit)

        # éæ¿¾ä½æ–¼é–¾å€¼çš„
        suggestions = [
            card for card in similar_cards
            if card['similarity'] >= threshold
        ]

        return suggestions

    def auto_link_zettel_v3(
        self,
        similarity_threshold: float = 0.7
    ) -> Dict[str, int]:
        """æ”¹é€²ç‰ˆè‡ªå‹•é—œè¯ï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # ç²å–æ‰€æœ‰å¡ç‰‡
        cursor.execute("SELECT card_id, zettel_id FROM zettel_cards")
        cards = cursor.fetchall()

        stats = {
            'linked': 0,
            'total_links_created': 0,
            'skipped': 0
        }

        for card_id, zettel_id in cards:
            # æŸ¥æ‰¾ç›¸ä¼¼å¡ç‰‡
            suggestions = self.suggest_zettel_links(
                card_id,
                threshold=similarity_threshold,
                limit=5
            )

            if not suggestions:
                stats['skipped'] += 1
                continue

            # å‰µå»ºé€£çµ
            for suggestion in suggestions:
                target_zettel_id = suggestion['zettel_id']

                # æ’å…¥é€£çµï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                try:
                    cursor.execute("""
                        INSERT INTO zettel_links (
                            source_card_id,
                            target_zettel_id,
                            relation_type,
                            context
                        )
                        VALUES (?, ?, ?, ?)
                    """, (
                        card_id,
                        target_zettel_id,
                        'ç›¸é—œ',
                        f"Similarity: {suggestion['similarity']:.2f}"
                    ))
                    stats['total_links_created'] += 1
                except sqlite3.IntegrityError:
                    pass  # é€£çµå·²å­˜åœ¨

            stats['linked'] += 1

        conn.commit()
        conn.close()

        return stats
```

---

## ğŸ“ é…ç½®æ–‡ä»¶æ›´æ–°ï¼ˆå®Œæ•´ç‰ˆï¼‰

### settings.yaml æ–°å¢å€å¡Š

```yaml
# === Embeddings é…ç½® ===
embeddings:
  enabled: true
  default_provider: "voyage"  # voyage, google, local, ollama

  # å‘é‡å­˜å„²å¾Œç«¯
  vector_store:
    backend: "chromadb"  # chromadb, sqlite, qdrant
    path: "knowledge_base/vectors"
    distance_metric: "cosine"  # cosine, l2, ip

  # æä¾›è€…é…ç½®
  providers:
    # Voyage AIï¼ˆæ¨è–¦ï¼‰
    voyage:
      api_key: ""  # å¾ VOYAGE_API_KEY è®€å–
      model: "voyage-3-large"  # æˆ– voyage-3.5-liteï¼ˆä½æˆæœ¬ï¼‰
      dimensions: 1024
      batch_size: 128
      max_retries: 3
      timeout: 30

    # Google Gemini
    google:
      api_key: ""  # å¾ GOOGLE_API_KEY è®€å–
      model: "models/text-embedding-004"
      dimensions: 3072
      task_types:
        document: "retrieval_document"
        query: "retrieval_query"

    # æœ¬åœ°é–‹æºæ¨¡å‹
    local:
      model: "intfloat/multilingual-e5-large"  # æˆ– Alibaba-NLP/gte-Qwen2-7B-instruct
      dimensions: 1024
      device: "cuda"  # cuda, cpu, mps
      normalize: true  # L2 æ­£è¦åŒ–

    # Ollama æœ¬åœ°éƒ¨ç½²
    ollama:
      base_url: "http://localhost:11434"
      model: "nomic-embed-text"
      dimensions: 768
      timeout: 60

  # è‡ªå‹•é¸æ“‡ç­–ç•¥
  auto_select:
    enabled: true
    strategy: "balanced"  # balanced, quality, cost, privacy

  # ç­–ç•¥å®šç¾©
  strategies:
    balanced:
      preferred: ["voyage", "google"]
      fallback: ["local", "ollama"]

    quality:
      preferred: ["google", "voyage"]
      fallback: ["local"]

    cost:
      preferred: ["ollama", "local", "voyage"]  # å…è²»å„ªå…ˆ
      fallback: ["google"]

    privacy:
      preferred: ["ollama", "local"]  # åƒ…æœ¬åœ°
      fallback: []

  # æ‰¹æ¬¡è™•ç†è¨­ç½®
  batch_processing:
    enabled: true
    batch_size: 100
    parallel_workers: 3
    retry_on_error: true

  # å¿«å–è¨­ç½®
  cache:
    enabled: true
    ttl: 2592000  # 30å¤©ï¼ˆembeddings å¾ˆå°‘è®ŠåŒ–ï¼‰
    backend: "disk"  # disk, memory
    max_size: "1GB"

  # ç¶­åº¦æ¸›å°‘ï¼ˆé™ä½å­˜å„²æˆæœ¬ï¼‰
  dimension_reduction:
    enabled: false
    method: "pca"  # pca, umap
    target_dimensions: 512
```

---

### model_selection.yaml æ–°å¢å€å¡Š

```yaml
# === Embeddings æ¨¡å‹å®šç¾© ===
embedding_models:
  # Voyage AI - voyage-3-largeï¼ˆæ¨è–¦ï¼‰
  voyage_large:
    provider: "voyage"
    model_name: "voyage-3-large"
    priority: 1
    quality_score: 5
    cost_per_1m_tokens: 0.18
    dimensions: 1024
    best_for:
      - "semantic_search"
      - "paper_similarity"
      - "zettel_linking"
    supports_chinese: true
    supports_batch: true
    max_batch_size: 128

  # Voyage AI - voyage-3.5-liteï¼ˆä½æˆæœ¬ï¼‰
  voyage_lite:
    provider: "voyage"
    model_name: "voyage-3.5-lite"
    priority: 2
    quality_score: 4
    cost_per_1m_tokens: 0.02
    dimensions: 1024
    best_for:
      - "cost_sensitive"
      - "large_scale_embedding"

  # Google Gemini
  gemini_embedding:
    provider: "google"
    model_name: "models/text-embedding-004"
    priority: 1
    quality_score: 5
    cost_per_1m_tokens: 0.15
    dimensions: 3072
    supports_chinese: true
    task_types: ["retrieval_document", "retrieval_query"]

  # Qwen3-Embedding-4Bï¼ˆé–‹æºæœ€ä½³ï¼‰
  qwen3_4b:
    provider: "local"
    model_name: "Alibaba-NLP/gte-Qwen2-7B-instruct"
    priority: 1
    quality_score: 5
    cost_per_1m_tokens: 0.0  # å…è²»
    dimensions: 2560
    supports_chinese: true
    requires_gpu: true
    gpu_memory: "16GB"

  # multilingual-e5-largeï¼ˆé–‹æºå¹³è¡¡ï¼‰
  e5_large:
    provider: "local"
    model_name: "intfloat/multilingual-e5-large"
    priority: 2
    quality_score: 4
    cost_per_1m_tokens: 0.0  # å…è²»
    dimensions: 1024
    supports_chinese: true
    requires_gpu: false
    prefix_required: true

  # Nomic Embed Text V2ï¼ˆOllamaï¼‰
  nomic_v2:
    provider: "ollama"
    model_name: "nomic-embed-text"
    priority: 3
    quality_score: 4
    cost_per_1m_tokens: 0.0  # å…è²»
    dimensions: 768
    supports_chinese: true
    requires_local: true

  # bge-m3ï¼ˆHybrid Searchï¼‰
  bge_m3:
    provider: "local"
    model_name: "BAAI/bge-m3"
    priority: 3
    quality_score: 4
    cost_per_1m_tokens: 0.0  # å…è²»
    dimensions: 1024
    supports_chinese: true
    supports_sparse: true  # æ”¯æ´ç¨€ç–å‘é‡
    supports_hybrid: true

# === ä»»å‹™é¡å‹èˆ‡æ¨¡å‹æ˜ å°„ ===
embedding_task_mapping:
  # è«–æ–‡ç›¸ä¼¼åº¦
  paper_similarity:
    preferred: ["voyage_large", "gemini_embedding"]
    fallback: ["qwen3_4b", "e5_large"]

  # Zettelkasten é€£çµ
  zettel_linking:
    preferred: ["voyage_large", "qwen3_4b"]
    fallback: ["e5_large", "nomic_v2"]

  # èªç¾©æœç´¢
  semantic_search:
    preferred: ["gemini_embedding", "voyage_large"]
    fallback: ["e5_large"]

  # Hybrid Search
  hybrid_search:
    preferred: ["bge_m3"]
    fallback: ["e5_large"]

  # å¤§æ‰¹é‡åµŒå…¥
  bulk_embedding:
    preferred: ["nomic_v2", "e5_large"]  # æœ¬åœ°å„ªå…ˆ
    fallback: ["voyage_lite"]  # é›²ç«¯ä½æˆæœ¬
```

---

## ğŸ¯ æœ€çµ‚æ±ºç­–å»ºè­°

### æ¨è–¦æ–¹æ¡ˆçµ„åˆï¼ˆåˆ†éšæ®µï¼‰

#### **Phase 1.5ï¼ˆç«‹å³å¯¦æ–½ï¼‰** - é›²ç«¯å„ªå…ˆ

```yaml
primary: voyage-3-large
fallback: gemini-embedding-001
cost: ~$0.10ï¼ˆåˆå§‹åŒ–ï¼‰+ $0.02/æœˆ
```

**ç†ç”±**:
1. âœ… å¿«é€Ÿå•Ÿå‹•ï¼ˆç„¡éœ€æœ¬åœ°éƒ¨ç½²ï¼‰
2. âœ… æ€§èƒ½æœ€ä½³ï¼ˆHit Rate: 0.9877ï¼‰
3. âœ… æˆæœ¬å¯æ§ï¼ˆé ä½æ–¼é ç®—ï¼‰

---

#### **Phase 2ï¼ˆ2é€±å¾Œï¼‰** - æ··åˆéƒ¨ç½²

```yaml
primary: voyage-3-largeï¼ˆé›²ç«¯ï¼‰
secondary: multilingual-e5-largeï¼ˆæœ¬åœ°ï¼‰
cost: ~$0.15ï¼ˆåˆå§‹åŒ–ï¼‰+ $0.01/æœˆ
```

**ç†ç”±**:
1. âœ… é›²ç«¯è™•ç†è«–æ–‡ï¼ˆé«˜å“è³ªè¦æ±‚ï¼‰
2. âœ… æœ¬åœ°è™•ç†å¡ç‰‡ï¼ˆå¤§æ‰¹é‡ + éš±ç§ï¼‰
3. âœ… é™ä½ 90% é‹ç‡Ÿæˆæœ¬

---

#### **Phase 3+ï¼ˆé•·æœŸï¼‰** - å®Œå…¨æœ¬åœ°åŒ–ï¼ˆå¯é¸ï¼‰

```yaml
primary: Qwen3-Embedding-4Bï¼ˆæœ¬åœ°ï¼‰
secondary: Nomic V2ï¼ˆOllamaï¼‰
cost: $0
```

**ç†ç”±**:
1. âœ… å®Œå…¨å…è²»
2. âœ… æ•¸æ“šéš±ç§
3. âœ… ç„¡ API é™åˆ¶

---

### ç«‹å³è¡Œå‹•æ¸…å–®

**æœ¬é€±åŸ·è¡Œ**:
1. âœ… ç”³è«‹ Voyage AI API keyï¼ˆhttps://www.voyageai.com/ï¼‰
2. âœ… å®‰è£ ChromaDB: `pip install chromadb`
3. âœ… æ›´æ–° `settings.yaml` å’Œ `.env` é…ç½®
4. âœ… ç‚º 40ç¯‡è«–æ–‡ç”ŸæˆåµŒå…¥ï¼ˆé è¨ˆ 30åˆ†é˜ï¼‰
5. âœ… æ¸¬è©¦èªç¾©æœç´¢åŠŸèƒ½

**2é€±å…§å®Œæˆ**:
1. âœ… ç‚º 644å¼µ Zettelkasten å¡ç‰‡ç”ŸæˆåµŒå…¥
2. âœ… å¯¦ä½œ `auto_link_zettel_v3()`
3. âœ… éƒ¨ç½² multilingual-e5-large ä½œç‚ºæœ¬åœ°å‚™é¸
4. âœ… æ€§èƒ½æ¸¬è©¦å’Œæ¯”è¼ƒ

---

## ğŸ“š åƒè€ƒè³‡æº

1. **Voyage AI API æ–‡æª”**
   https://docs.voyageai.com/

2. **Google Gemini Embeddings Guide**
   https://ai.google.dev/gemini-api/docs/embeddings

3. **ChromaDB Documentation**
   https://docs.trychroma.com/

4. **Sentence Transformers (e5, Qwen3)**
   https://www.sbert.net/

5. **Ollama Embeddings**
   https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings

6. **ç¹é«”ä¸­æ–‡ Embeddings è©•æ¸¬ï¼ˆæ•¸æ“šä¾†æºï¼‰**
   https://docs.google.com/spreadsheets/d/1zad1tMFp7OmNjUvm_a-Ni22av2uBmqYclVRgJQGUtl0/

7. **FlagEmbedding (bge-m3)**
   https://github.com/FlagOpen/FlagEmbedding

8. **Vector Search æœ€ä½³å¯¦è¸**
   https://www.pinecone.io/learn/vector-search/

---

## ğŸ“Š é™„éŒ„ï¼šå®Œæ•´è©•æ¸¬æ•¸æ“šï¼ˆTop 50ï¼‰

ï¼ˆè¦‹å‰æ–‡ WebFetch çµæœè¡¨æ ¼ï¼‰

---

**å ±å‘Šå®Œæˆæ™‚é–“**: 2025-11-01 11:30 AM
**ç‰ˆæœ¬**: v2.0ï¼ˆä¿®æ­£ç‰ˆï¼‰
**ä¸‹ä¸€æ­¥**: ç­‰å¾…åœ˜éšŠæ±ºç­–ï¼Œé–‹å§‹å¯¦æ–½ Phase 1.5 å‘é‡æœç´¢åŸºç¤è¨­æ–½

---

**ä¿®æ­£èªªæ˜** (v2.0):
- âœ… æ›´æ­£æ‰€æœ‰æ¨¡å‹çš„å¯¦æ¸¬æ•¸æ“šï¼ˆåŸºæ–¼ Google Sheetï¼‰
- âœ… ç§»é™¤ OpenAI embeddings æ¨è–¦ï¼ˆç¹ä¸­è¡¨ç¾ä¸ä½³ï¼‰
- âœ… æ–°å¢ Qwen3-Embedding-4Bï¼ˆé–‹æºæœ€ä½³ï¼‰
- âœ… æ–°å¢ voyage-3.5-liteï¼ˆä½æˆæœ¬é›²ç«¯ï¼‰
- âœ… æ›´æ­£æ‰€æœ‰ API åƒ¹æ ¼ä¿¡æ¯
- âœ… æ›´æ­£ Gemini ç¶­åº¦ï¼ˆ768 â†’ 3072ï¼‰
- âœ… è£œå……å®Œæ•´çš„ API æ•´åˆç¯„ä¾‹
- âœ… è£œå…… ChromaDB å¯¦ä½œæŒ‡å—

**è‡´æ­‰è²æ˜**: å°æ–¼åˆç‰ˆå ±å‘Šä¸­çš„æ•¸æ“šéŒ¯èª¤æ·±è¡¨æ­‰æ„ã€‚æœ¬æ¬¡ä¿®æ­£ç‰ˆåŸºæ–¼å®Œå…¨é©—è­‰çš„å¯¦æ¸¬æ•¸æ“šï¼Œç¢ºä¿æº–ç¢ºæ€§ã€‚
