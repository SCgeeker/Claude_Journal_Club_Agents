#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸‰æ¨¡å‹å°æ¯”æ¸¬è©¦è…³æœ¬
ä½¿ç”¨ä¸‰å€‹å…è²» OpenRouter æ¨¡å‹é‡æ–°ç”Ÿæˆ Zettelkasten å¡ç‰‡ä¸¦å°æ¯”è³ªé‡
"""

import shutil
import sys
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()

from src.generators.zettel_maker import ZettelMaker
from src.generators.slide_maker import SlideMaker


def extract_paper_content(md_path):
    """å¾ MD æ–‡ä»¶æå–è«–æ–‡å…§å®¹"""
    content = md_path.read_text(encoding='utf-8')

    # æå–åŸºæœ¬ä¿¡æ¯
    import re
    title = "Unknown"
    authors = "Unknown"
    year = None

    # å¾ frontmatter æå–
    title_match = re.search(r"title:\s*['\"]?(.+?)['\"]?\s*$", content, re.MULTILINE)
    if title_match:
        title = title_match.group(1)

    authors_match = re.search(r"authors:\s*(.+?)$", content, re.MULTILINE)
    if authors_match:
        authors = authors_match.group(1)

    year_match = re.search(r"year:\s*(\d{4})", content)
    if year_match:
        year = int(year_match.group(1))

    # æå–å®Œæ•´å…§å®¹éƒ¨åˆ†ï¼ˆå»é™¤ frontmatterï¼‰
    content_start = content.find('---', content.find('---') + 3) + 3
    full_content = content[content_start:].strip()

    return {
        'title': title,
        'authors': authors,
        'year': year,
        'content': full_content[:15000],  # é™åˆ¶é•·åº¦
        'cite_key': md_path.stem
    }


def generate_with_model(paper_data, model_id, model_name, output_suffix):
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆå¡ç‰‡"""
    print(f"\n{'='*70}")
    print(f"[{model_name}] ç”Ÿæˆ Zettelkasten: {paper_data['cite_key']}")
    print(f"æ¨¡å‹: {model_id}")
    print(f"æ¨™é¡Œ: {paper_data['title'][:60]}...")
    print(f"{'='*70}\n")

    # 1. åˆå§‹åŒ– SlideMakerï¼ˆæŒ‡å®š OpenRouterï¼‰
    slide_maker = SlideMaker(
        llm_provider='openrouter'
    )

    # 2. æº–å‚™ prompt
    from jinja2 import Template
    template_path = Path("templates/prompts/zettelkasten_template.jinja2")
    template = Template(template_path.read_text(encoding='utf-8'))

    prompt = template.render(
        topic=paper_data['title'],
        card_count=20,
        detail_level="comprehensive",
        paper_content=paper_data['content'],
        cite_key=paper_data['cite_key']
    )

    print(f"Prompt é•·åº¦: {len(prompt)} å­—ç¬¦")
    print(f"ç”Ÿæˆ 20 å¼µå¡ç‰‡ (comprehensive æ¨¡å¼)\n")

    # 3. èª¿ç”¨ LLM
    print(f"æ­£åœ¨èª¿ç”¨ {model_name}...")

    try:
        result = slide_maker.call_llm(
            prompt,
            provider='openrouter',
            model=model_id,
            timeout=600  # 10 åˆ†é˜è¶…æ™‚
        )

        if not result:
            print(f"[ERROR] {model_name} è¿”å›ç©ºéŸ¿æ‡‰")
            return None

        response, provider = result
        print(f"[OK] {model_name} éŸ¿æ‡‰: {len(response)} å­—ç¬¦\n")

    except Exception as e:
        print(f"[ERROR] {model_name} èª¿ç”¨å¤±æ•—: {e}")
        return None

    # 4. ç”Ÿæˆå¡ç‰‡æ–‡ä»¶
    output_dir = Path("output/zettelkasten_notes") / f"zettel_{paper_data['cite_key']}_{datetime.now().strftime('%Y%m%d')}_{output_suffix}"

    # åˆªé™¤èˆŠç‰ˆæœ¬
    if output_dir.exists():
        shutil.rmtree(output_dir)

    print(f"ç”Ÿæˆå¡ç‰‡åˆ°: {output_dir}")

    zettel_maker = ZettelMaker()
    result = zettel_maker.generate_zettelkasten(
        llm_output=response,
        output_dir=output_dir,
        paper_info={
            'cite_key': paper_data['cite_key'],
            'title': paper_data['title'],
            'authors': paper_data['authors'],
            'year': paper_data['year']
        }
    )

    print(f"[OK] ç”Ÿæˆ {result['card_count']} å¼µå¡ç‰‡")
    print(f"\n[SUCCESS] å®Œæˆ: {output_dir}\n")

    return output_dir


def analyze_output(output_dir, model_name):
    """åˆ†æç”Ÿæˆçš„å¡ç‰‡è³ªé‡"""
    if not output_dir or not output_dir.exists():
        return None

    print(f"\n[ANALYZE] {model_name} è¼¸å‡ºåˆ†æ:")
    print("-" * 70)

    # çµ±è¨ˆå¡ç‰‡æ•¸
    cards_dir = output_dir / "zettel_cards"
    if cards_dir.exists():
        card_files = list(cards_dir.glob("*.md"))
        print(f"  å¡ç‰‡æ•¸é‡: {len(card_files)}")

        # åˆ†æ AI notes ä¸­çš„é€£çµ
        total_links = 0
        cards_with_links = 0

        for card_file in card_files:
            content = card_file.read_text(encoding='utf-8')

            # æª¢æŸ¥ AI notes å€å¡Š
            if 'ğŸ¤– **AI**:' in content:
                ai_section = content.split('ğŸ¤– **AI**:')[1].split('âœï¸ **Human**:')[0]
                # çµ±è¨ˆ Wiki Links
                links = ai_section.count('[[')
                if links > 0:
                    cards_with_links += 1
                    total_links += links

        print(f"  AI notes åŒ…å«é€£çµçš„å¡ç‰‡: {cards_with_links}/{len(card_files)} ({cards_with_links/len(card_files)*100:.1f}%)")
        print(f"  AI notes ç¸½é€£çµæ•¸: {total_links}")
        print(f"  å¹³å‡æ¯å¼µå¡ç‰‡é€£çµæ•¸: {total_links/len(card_files):.2f}")

    print("-" * 70)

    return {
        'card_count': len(card_files),
        'cards_with_links': cards_with_links,
        'total_links': total_links,
        'avg_links': total_links/len(card_files) if card_files else 0
    }


def main():
    parser = argparse.ArgumentParser(description='ä¸‰æ¨¡å‹å°æ¯”æ¸¬è©¦')
    parser.add_argument('--cite-key', default='Jones-2024', help='è«–æ–‡ cite key')
    args = parser.parse_args()

    print("\n" + "="*70)
    print("ä¸‰æ¨¡å‹å°æ¯”æ¸¬è©¦ - OpenRouter å…è²»æ¨¡å‹")
    print("="*70 + "\n")

    # 1. è®€å–è«–æ–‡
    md_path = Path(f"knowledge_base/papers/{args.cite_key}.md")
    if not md_path.exists():
        print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {md_path}")
        return

    print(f"[INFO] è®€å–è«–æ–‡: {md_path}")
    paper_data = extract_paper_content(md_path)
    print(f"[OK] æå–æˆåŠŸ\n")

    # 2. ä¸‰å€‹æ¸¬è©¦æ¨¡å‹
    models = [
        {
            'id': 'google/gemini-2.0-flash-exp:free',
            'name': 'Gemini 2.0 Flash',
            'suffix': 'gemini'
        },
        {
            'id': 'deepseek/deepseek-r1:free',
            'name': 'DeepSeek R1',
            'suffix': 'deepseek'
        },
        {
            'id': 'meta-llama/llama-3.3-70b-instruct:free',
            'name': 'Llama 3.3 70B',
            'suffix': 'llama'
        }
    ]

    # 3. ä¾æ¬¡ä½¿ç”¨ä¸‰å€‹æ¨¡å‹ç”Ÿæˆ
    results = {}
    import time

    for i, model in enumerate(models):
        # åœ¨æ¨¡å‹ä¹‹é–“ç­‰å¾…ä»¥é¿å… rate limiting
        if i > 0:
            print(f"\n[INFO] ç­‰å¾… 60 ç§’ä»¥é¿å… rate limiting...")
            time.sleep(60)

        output_dir = generate_with_model(
            paper_data,
            model['id'],
            model['name'],
            model['suffix']
        )

        if output_dir:
            analysis = analyze_output(output_dir, model['name'])
            results[model['name']] = {
                'output_dir': output_dir,
                'analysis': analysis
            }
        else:
            print(f"[WARNING] {model['name']} ç”Ÿæˆå¤±æ•—ï¼Œç­‰å¾… 30 ç§’å¾Œç¹¼çºŒ...")
            time.sleep(30)

    # 4. å°æ¯”çµæœ
    print("\n" + "="*70)
    print("å°æ¯”çµæœç¸½çµ")
    print("="*70 + "\n")

    print(f"{'æ¨¡å‹':<20} {'å¡ç‰‡æ•¸':<8} {'æœ‰é€£çµå¡ç‰‡':<12} {'ç¸½é€£çµæ•¸':<10} {'å¹³å‡é€£çµ/å¡ç‰‡':<15}")
    print("-" * 70)

    for model in models:
        if model['name'] in results and results[model['name']]['analysis']:
            a = results[model['name']]['analysis']
            print(f"{model['name']:<20} {a['card_count']:<8} {a['cards_with_links']:<12} {a['total_links']:<10} {a['avg_links']:<15.2f}")
        else:
            print(f"{model['name']:<20} {'å¤±æ•—':<8}")

    print("\n" + "="*70)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("="*70)

    # 5. è¼¸å‡ºè©³ç´°è·¯å¾‘
    print("\nç”Ÿæˆçš„å¡ç‰‡ç›®éŒ„:")
    for model in models:
        if model['name'] in results:
            print(f"  [{model['name']}] {results[model['name']]['output_dir']}")


if __name__ == '__main__':
    main()
