#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è­˜åº«ç®¡ç†å‘½ä»¤è¡Œå·¥å…·
ä½¿ç”¨æ–¹å¼: python kb_manage.py <command> [options]
"""

import sys
import argparse
from pathlib import Path

# è¨­ç½®UTF-8ç·¨ç¢¼ï¼ˆWindowsç›¸å®¹æ€§ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

sys.path.insert(0, str(Path(__file__).parent))

from src.knowledge_base import KnowledgeBaseManager
from src.embeddings.providers import GeminiEmbedder, OllamaEmbedder
from src.embeddings.vector_db import VectorDatabase

# å°å…¥å…ƒæ•¸æ“šä¿®å¾©å·¥å…·
try:
    from fix_metadata import MetadataFixer
except ImportError:
    MetadataFixer = None

# å°å…¥é—œä¿‚ç™¼ç¾å™¨ (Phase 2.1)
try:
    from src.analyzers import RelationFinder
except ImportError:
    RelationFinder = None

import sqlite3
import json
import yaml
import re
from typing import Dict


def cmd_stats(args):
    """é¡¯ç¤ºçŸ¥è­˜åº«çµ±è¨ˆä¿¡æ¯"""
    kb = KnowledgeBaseManager()
    stats = kb.get_stats()

    print("\n" + "=" * 60)
    print("ğŸ“Š çŸ¥è­˜åº«çµ±è¨ˆ")
    print("=" * 60)
    print(f"è«–æ–‡ç¸½æ•¸: {stats['total_papers']}")
    print(f"ä¸»é¡Œç¸½æ•¸: {stats['total_topics']}")
    print(f"å¼•ç”¨ç¸½æ•¸: {stats['total_citations']}")
    print("=" * 60 + "\n")


def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰è«–æ–‡"""
    kb = KnowledgeBaseManager()
    papers = kb.list_papers(limit=args.limit)

    print("\n" + "=" * 60)
    print(f"ğŸ“„ è«–æ–‡åˆ—è¡¨ (æœ€å¤š {args.limit} ç¯‡)")
    print("=" * 60)

    if papers:
        for paper in papers:
            print(f"\n[ID: {paper['id']}] {paper['title']}")
            print(f"  ä½œè€…: {', '.join(paper['authors'][:3])}")
            if len(paper['authors']) > 3:
                print(f"        (+{len(paper['authors'])-3} ä½)")
            print(f"  å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
            if paper['keywords']:
                print(f"  é—œéµè©: {', '.join(paper['keywords'][:5])}")
            print(f"  æ™‚é–“: {paper['created_at']}")
    else:
        print("\nâš ï¸  çŸ¥è­˜åº«ä¸­é‚„æ²’æœ‰è«–æ–‡")

    print("\n" + "=" * 60 + "\n")


def cmd_search(args):
    """æœç´¢è«–æ–‡"""
    kb = KnowledgeBaseManager()
    results = kb.search_papers(args.query, limit=args.limit)

    print("\n" + "=" * 60)
    print(f"ğŸ” æœç´¢: '{args.query}'")
    print("=" * 60)

    if results:
        print(f"\næ‰¾åˆ° {len(results)} å€‹çµæœ:\n")
        for i, paper in enumerate(results, 1):
            print(f"{i}. [ID: {paper['id']}] {paper['title']}")
            print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
            print(f"   å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
            if paper['abstract']:
                preview = paper['abstract'][:100].replace('\n', ' ')
                print(f"   æ‘˜è¦: {preview}...")
            print()
    else:
        print(f"\nâŒ æœªæ‰¾åˆ°åŒ…å« '{args.query}' çš„è«–æ–‡\n")

    print("=" * 60 + "\n")


def cmd_show(args):
    """é¡¯ç¤ºè«–æ–‡è©³æƒ…"""
    kb = KnowledgeBaseManager()
    paper = kb.get_paper_by_id(args.id)

    print("\n" + "=" * 60)
    print(f"ğŸ“„ è«–æ–‡è©³æƒ… (ID: {args.id})")
    print("=" * 60)

    if paper:
        print(f"\næ¨™é¡Œ: {paper['title']}")
        print(f"ä½œè€…: {', '.join(paper['authors'])}")
        print(f"å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
        print(f"æª”æ¡ˆ: {paper['file_path']}")
        print(f"å‰µå»º: {paper['created_at']}")
        print(f"æ›´æ–°: {paper['updated_at']}")

        if paper['keywords']:
            print(f"é—œéµè©: {', '.join(paper['keywords'])}")

        if paper['abstract']:
            print(f"\næ‘˜è¦:")
            print(f"{paper['abstract'][:500]}")
            if len(paper['abstract']) > 500:
                print("...")
    else:
        print(f"\nâŒ æ‰¾ä¸åˆ°IDç‚º {args.id} çš„è«–æ–‡")

    print("\n" + "=" * 60 + "\n")


def cmd_add_topic(args):
    """å‰µå»ºä¸»é¡Œ"""
    kb = KnowledgeBaseManager()
    topic_id = kb.add_topic(args.name, args.description)

    print("\n" + "=" * 60)
    print("ğŸ·ï¸  å‰µå»ºä¸»é¡Œ")
    print("=" * 60)
    print(f"åç¨±: {args.name}")
    print(f"æè¿°: {args.description}")
    print(f"ID: {topic_id}")
    print("=" * 60 + "\n")


def cmd_link(args):
    """é€£çµè«–æ–‡åˆ°ä¸»é¡Œ"""
    kb = KnowledgeBaseManager()
    kb.link_paper_to_topic(args.paper_id, args.topic_id, args.relevance)

    print("\n" + "=" * 60)
    print("ğŸ”— é€£çµè«–æ–‡èˆ‡ä¸»é¡Œ")
    print("=" * 60)
    print(f"è«–æ–‡ID: {args.paper_id}")
    print(f"ä¸»é¡ŒID: {args.topic_id}")
    print(f"ç›¸é—œåº¦: {args.relevance}")
    print("âœ… é€£çµæˆåŠŸ")
    print("=" * 60 + "\n")


def cmd_topic_papers(args):
    """æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡"""
    kb = KnowledgeBaseManager()
    papers = kb.get_papers_by_topic(args.name)

    print("\n" + "=" * 60)
    print(f"ğŸ·ï¸  ä¸»é¡Œ: {args.name}")
    print("=" * 60)

    if papers:
        print(f"\næ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡:\n")
        for paper in papers:
            print(f"[ID: {paper['id']}] {paper['title']}")
            print(f"  ä½œè€…: {', '.join(paper['authors'][:3])}")
            print(f"  ç›¸é—œåº¦: {paper['relevance']:.2f}")
            print()
    else:
        print(f"\nâš ï¸  ä¸»é¡Œ '{args.name}' ä¸‹æ²’æœ‰è«–æ–‡\n")

    print("=" * 60 + "\n")


def cmd_cite(args):
    """æ·»åŠ å¼•ç”¨é—œä¿‚"""
    kb = KnowledgeBaseManager()
    kb.add_citation(args.source, args.target, args.type)

    print("\n" + "=" * 60)
    print("ğŸ“š æ·»åŠ å¼•ç”¨é—œä¿‚")
    print("=" * 60)
    print(f"ä¾†æºè«–æ–‡ID: {args.source}")
    print(f"ç›®æ¨™è«–æ–‡ID: {args.target}")
    print(f"å¼•ç”¨é¡å‹: {args.type}")
    print("âœ… å¼•ç”¨é—œä¿‚å·²æ·»åŠ ")
    print("=" * 60 + "\n")


def cmd_semantic_search(args):
    """èªç¾©æœç´¢è«–æ–‡æˆ–Zettelkastenå¡ç‰‡"""
    # åˆå§‹åŒ–
    if args.provider == "gemini":
        embedder = GeminiEmbedder()
    else:
        embedder = OllamaEmbedder()

    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)
    print(f"ğŸ” èªç¾©æœç´¢: '{args.query}'")
    print(f"æä¾›è€…: {args.provider.upper()}")
    print("=" * 60)

    # ç”ŸæˆæŸ¥è©¢å‘é‡
    print("\nç”ŸæˆæŸ¥è©¢å‘é‡...")
    query_embedding = embedder.embed(args.query, task_type="retrieval_query")

    # æœç´¢è«–æ–‡
    if args.type in ['papers', 'all']:
        print(f"\nğŸ“„ æœç´¢è«–æ–‡ (top {args.limit}):")
        print("-" * 60)

        results = vector_db.semantic_search_papers(
            query_embedding=query_embedding,
            n_results=args.limit
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            for i, (paper_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                metadata = results['metadatas'][0][i]
                similarity = (1 - distance) * 100  # è½‰æ›ç‚ºç™¾åˆ†æ¯”

                # å¾çŸ¥è­˜åº«ç²å–å®Œæ•´ä¿¡æ¯
                pid = int(paper_id.replace('paper_', ''))
                paper = kb.get_paper_by_id(pid)

                if paper:
                    print(f"\n{i+1}. [{similarity:.1f}%] {paper['title']}")
                    print(f"   ID: {pid}")
                    print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
                    if len(paper['authors']) > 3:
                        print(f"         (+{len(paper['authors'])-3} ä½)")
                    print(f"   å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")

                    if args.verbose and paper['abstract']:
                        preview = paper['abstract'][:150].replace('\n', ' ')
                        print(f"   æ‘˜è¦: {preview}...")
        else:
            print("\næœªæ‰¾åˆ°ç›¸é—œè«–æ–‡")

    # æœç´¢Zettelkasten
    if args.type in ['zettel', 'all']:
        print(f"\nğŸ—‚ï¸  æœç´¢ Zettelkasten å¡ç‰‡ (top {args.limit}):")
        print("-" * 60)

        results = vector_db.semantic_search_zettel(
            query_embedding=query_embedding,
            n_results=args.limit
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            for i, (zettel_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                metadata = results['metadatas'][0][i]
                similarity = (1 - distance) * 100
                title = metadata.get('title', 'Unknown')

                print(f"\n{i+1}. [{similarity:.1f}%] {title}")
                print(f"   ID: {zettel_id}")

                if args.verbose:
                    doc = results['documents'][0][i]
                    preview = doc[:150].replace('\n', ' ')
                    print(f"   å…§å®¹: {preview}...")
        else:
            print("\næœªæ‰¾åˆ°ç›¸é—œå¡ç‰‡")

    print("\n" + "=" * 60 + "\n")


def cmd_similar(args):
    """å°‹æ‰¾ç›¸ä¼¼çš„è«–æ–‡æˆ–Zettelkastenå¡ç‰‡"""
    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)

    # åˆ¤æ–·æ˜¯è«–æ–‡é‚„æ˜¯Zettelkasten
    if args.id.startswith('paper_') or args.id.isdigit():
        # è«–æ–‡
        paper_id = f"paper_{args.id}" if args.id.isdigit() else args.id
        pid = int(paper_id.replace('paper_', ''))

        paper = kb.get_paper_by_id(pid)
        if not paper:
            print(f"âŒ æ‰¾ä¸åˆ°è«–æ–‡ ID: {args.id}")
            print("=" * 60 + "\n")
            return

        print(f"ğŸ” å°‹æ‰¾èˆ‡è«–æ–‡ç›¸ä¼¼çš„å…§å®¹")
        print(f"è«–æ–‡: {paper['title']}")
        print("=" * 60)

        # å°‹æ‰¾ç›¸ä¼¼è«–æ–‡
        results = vector_db.find_similar_papers(
            paper_id=paper_id,
            n_results=args.limit,
            exclude_self=True
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            print(f"\nğŸ“„ ç›¸ä¼¼è«–æ–‡ (top {args.limit}):")
            print("-" * 60)

            for i, (sim_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                similarity = (1 - distance) * 100
                sim_pid = int(sim_id.replace('paper_', ''))
                sim_paper = kb.get_paper_by_id(sim_pid)

                if sim_paper:
                    print(f"\n{i+1}. [{similarity:.1f}%] {sim_paper['title']}")
                    print(f"   ID: {sim_pid}")
                    print(f"   ä½œè€…: {', '.join(sim_paper['authors'][:3])}")
        else:
            print("\næœªæ‰¾åˆ°ç›¸ä¼¼è«–æ–‡")

    else:
        # Zettelkasten
        zettel_id = args.id

        print(f"ğŸ” å°‹æ‰¾èˆ‡å¡ç‰‡ç›¸ä¼¼çš„å…§å®¹")
        print(f"å¡ç‰‡ ID: {zettel_id}")
        print("=" * 60)

        # å°‹æ‰¾ç›¸ä¼¼å¡ç‰‡
        results = vector_db.find_similar_zettel(
            zettel_id=zettel_id,
            n_results=args.limit,
            exclude_self=True
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            print(f"\nğŸ—‚ï¸  ç›¸ä¼¼å¡ç‰‡ (top {args.limit}):")
            print("-" * 60)

            for i, (sim_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                similarity = (1 - distance) * 100
                metadata = results['metadatas'][0][i]
                title = metadata.get('title', 'Unknown')

                print(f"\n{i+1}. [{similarity:.1f}%] {title}")
                print(f"   ID: {sim_id}")
        else:
            print("\næœªæ‰¾åˆ°ç›¸ä¼¼å¡ç‰‡")

    print("\n" + "=" * 60 + "\n")


def cmd_hybrid_search(args):
    """æ··åˆæœç´¢ï¼šçµåˆå…¨æ–‡æœç´¢å’Œèªç¾©æœç´¢"""
    # åˆå§‹åŒ–
    if args.provider == "gemini":
        embedder = GeminiEmbedder()
    else:
        embedder = OllamaEmbedder()

    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)
    print(f"ğŸ” æ··åˆæœç´¢: '{args.query}'")
    print(f"æä¾›è€…: {args.provider.upper()}")
    print("=" * 60)

    # 1. å…¨æ–‡æœç´¢
    print("\nğŸ“ å…¨æ–‡æœç´¢çµæœ:")
    print("-" * 60)
    fts_results = kb.search_papers(args.query, limit=args.limit)
    fts_ids = set()

    if fts_results:
        for i, paper in enumerate(fts_results, 1):
            fts_ids.add(paper['id'])
            print(f"{i}. [FTS] {paper['title']}")
            print(f"   ID: {paper['id']}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    # 2. èªç¾©æœç´¢
    print(f"\nğŸ” èªç¾©æœç´¢çµæœ:")
    print("-" * 60)
    print("ç”ŸæˆæŸ¥è©¢å‘é‡...")
    query_embedding = embedder.embed(args.query, task_type="retrieval_query")

    sem_results = vector_db.semantic_search_papers(
        query_embedding=query_embedding,
        n_results=args.limit
    )

    sem_ids = set()
    sem_scores = {}

    if sem_results['ids'] and len(sem_results['ids'][0]) > 0:
        for i, (paper_id, distance) in enumerate(zip(sem_results['ids'][0], sem_results['distances'][0])):
            pid = int(paper_id.replace('paper_', ''))
            similarity = (1 - distance) * 100
            sem_ids.add(pid)
            sem_scores[pid] = similarity

            paper = kb.get_paper_by_id(pid)
            if paper:
                print(f"{i+1}. [{similarity:.1f}%] {paper['title']}")
                print(f"   ID: {pid}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    # 3. æ··åˆçµæœ
    print(f"\nâœ¨ æ··åˆçµæœ (å…©ç¨®æ–¹æ³•çš„è¯é›†):")
    print("-" * 60)

    all_ids = fts_ids | sem_ids
    both_ids = fts_ids & sem_ids

    if all_ids:
        # æŒ‰èªç¾©ç›¸ä¼¼åº¦æ’åº
        sorted_ids = sorted(all_ids,
                          key=lambda x: sem_scores.get(x, 0),
                          reverse=True)[:args.limit]

        for i, pid in enumerate(sorted_ids, 1):
            paper = kb.get_paper_by_id(pid)
            if paper:
                tags = []
                if pid in fts_ids:
                    tags.append("FTS")
                if pid in sem_ids:
                    tags.append(f"SEM {sem_scores[pid]:.1f}%")

                tag_str = " + ".join(tags)

                print(f"\n{i}. [{tag_str}] {paper['title']}")
                print(f"   ID: {pid}")
                print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    print(f"\nçµ±è¨ˆ:")
    print(f"  å…¨æ–‡æœç´¢: {len(fts_ids)} ç¯‡")
    print(f"  èªç¾©æœç´¢: {len(sem_ids)} ç¯‡")
    print(f"  å…±åŒçµæœ: {len(both_ids)} ç¯‡")
    print(f"  ç¸½è¨ˆ: {len(all_ids)} ç¯‡")

    print("\n" + "=" * 60 + "\n")


def cmd_auto_link(args):
    """ç‚ºè«–æ–‡è‡ªå‹•å»ºç«‹èˆ‡Zettelkastençš„é€£çµï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
    from src.knowledge_base.auto_link import auto_link_v2

    print("\n" + "=" * 60)
    print(f"ğŸ”— è‡ªå‹•é€£çµè«–æ–‡ {args.paper_id}")
    print("=" * 60)
    print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.threshold}")
    print(f"æœ€å¤šé€£çµ: {args.max_links}")

    try:
        result = auto_link_v2(
            paper_id=args.paper_id,
            threshold=args.threshold,
            max_links=args.max_links
        )

        print(f"\nâœ… å®Œæˆï¼")
        print(f"è«–æ–‡: {result['paper_title']}")
        print(f"å»ºç«‹é€£çµ: {result['links_created']} å€‹")
        print(f"å€™é¸ç¸½æ•¸: {result['candidates_found']} å€‹ (>= {args.threshold} ç›¸ä¼¼åº¦)")

        if result['links']:
            print("\né€£çµè©³æƒ…:")
            print("-" * 60)
            for i, link in enumerate(result['links'], 1):
                print(f"\n{i}. [{link['similarity']:.1%}] {link['title']}")
                print(f"   ID: {link['zettel_id']} (card_id: {link['card_id']})")
                if link['core_concept']:
                    print(f"   æ ¸å¿ƒæ¦‚å¿µ: {link['core_concept'][:80]}...")
        else:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {args.threshold} çš„å¡ç‰‡")

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {str(e)}")
        import traceback
        if args.verbose:
            traceback.print_exc()

    print("\n" + "=" * 60 + "\n")


def cmd_auto_link_all(args):
    """ç‚ºæ‰€æœ‰è«–æ–‡æ‰¹æ¬¡å»ºç«‹é€£çµ"""
    from src.knowledge_base.auto_link import auto_link_all_papers

    print("\n" + "=" * 60)
    print("ğŸ”— æ‰¹æ¬¡è‡ªå‹•é€£çµæ‰€æœ‰è«–æ–‡")
    print("=" * 60)
    print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.threshold}")
    print(f"æ¯ç¯‡æœ€å¤šé€£çµ: {args.max_links}")

    result = auto_link_all_papers(
        threshold=args.threshold,
        max_links=args.max_links,
        verbose=args.verbose
    )

    print("\n" + "=" * 60)
    print("âœ… æ‰¹æ¬¡è™•ç†å®Œæˆ")
    print("=" * 60)
    print(f"ç¸½è«–æ–‡æ•¸: {result['total_papers']}")
    print(f"æˆåŠŸè™•ç†: {result['processed']}")
    print(f"å¤±æ•—æ•¸é‡: {result['failed']}")
    print(f"ç¸½é€£çµæ•¸: {result['total_links_created']}")
    print(f"å¹³å‡æ¯ç¯‡: {result['average_links_per_paper']:.2f} å€‹é€£çµ")
    print("\n" + "=" * 60 + "\n")


def cmd_show_links(args):
    """æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ"""
    kb = KnowledgeBaseManager()

    paper = kb.get_paper_by_id(args.paper_id)
    if not paper:
        print(f"\nâŒ è«–æ–‡ ID {args.paper_id} ä¸å­˜åœ¨\n")
        return

    links = kb.get_paper_zettel_links(args.paper_id, min_similarity=args.min_similarity)

    print("\n" + "=" * 60)
    print(f"ğŸ”— è«–æ–‡çš„ Zettelkasten é€£çµ")
    print("=" * 60)
    print(f"è«–æ–‡: {paper['title']}")
    print(f"ID: {args.paper_id}")
    print(f"é€£çµæ•¸: {len(links)}")
    print("=" * 60)

    if links:
        for i, link in enumerate(links, 1):
            print(f"\n{i}. [{link['similarity']:.1%}] {link['title']}")
            print(f"   Zettel ID: {link['zettel_id']}")
            print(f"   Card ID: {link['card_id']}")
            print(f"   é¡å‹: {link['card_type']} | é ˜åŸŸ: {link['domain']}")
            if link['core_concept']:
                print(f"   æ ¸å¿ƒæ¦‚å¿µ: {link['core_concept'][:100]}...")
            print(f"   é€£çµæ–¹æ³•: {link['method']}")
            print(f"   å‰µå»ºæ™‚é–“: {link['created_at']}")
    else:
        threshold_msg = f" (ç›¸ä¼¼åº¦ >= {args.min_similarity})" if args.min_similarity > 0 else ""
        print(f"\nâš ï¸  æ­¤è«–æ–‡æ²’æœ‰é€£çµ{threshold_msg}")
        print("æç¤º: åŸ·è¡Œ 'python kb_manage.py auto-link <paper_id>' ä¾†å»ºç«‹é€£çµ")

    print("\n" + "=" * 60 + "\n")


def cmd_metadata_fix(args):
    """ä¿®å¾©ç¼ºå¤±çš„å…ƒæ•¸æ“š"""
    if MetadataFixer is None:
        print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° fix_metadata.py")
        print("è«‹ç¢ºä¿ fix_metadata.py åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„")
        sys.exit(1)

    fixer = MetadataFixer()

    field_name = {
        'year': 'å¹´ä»½',
        'keywords': 'é—œéµè©',
        'abstract': 'æ‘˜è¦',
        'all': 'æ‰€æœ‰ç¼ºå¤±å­—æ®µ'
    }.get(args.field, 'æœªçŸ¥')

    print(f"\n{'=' * 60}")
    print(f"ğŸ”§ ä¿®å¾©å…ƒæ•¸æ“š: {field_name}")
    print(f"{'=' * 60}\n")

    # ç²å–éœ€è¦ä¿®å¾©çš„è«–æ–‡
    papers = fixer.get_papers_needing_repair(field=args.field if args.field != 'all' else None)

    if not papers:
        print("âœ… æ²’æœ‰è«–æ–‡éœ€è¦ä¿®å¾©ï¼")
        print(f"{'=' * 60}\n")
        return

    print(f"æ‰¾åˆ° {len(papers)} ç¯‡éœ€è¦ä¿®å¾©çš„è«–æ–‡\n")

    if args.dry_run:
        print("âš ï¸ é è¦½æ¨¡å¼ï¼ˆä¸æœƒå¯¦éš›ä¿®å¾©ï¼‰\n")
        for paper in papers[:10]:  # åªé¡¯ç¤ºå‰10å€‹
            print(f"[ID {paper['id']}] {paper['title'][:60]}")
        if len(papers) > 10:
            print(f"... é‚„æœ‰ {len(papers) - 10} ç¯‡")
        print(f"\n{'=' * 60}\n")
        return

    if args.batch:
        # æ‰¹æ¬¡ä¿®å¾©
        success = 0
        failed = 0

        for i, paper in enumerate(papers, 1):
            print(f"[{i}/{len(papers)}] ID {paper['id']}: {paper['title'][:50]}")

            result = fixer.auto_fix_paper(
                paper['id'],
                fields=[args.field] if args.field != 'all' else ['year', 'keywords', 'abstract']
            )

            if result.get('updates'):
                fixer.update_paper_metadata(
                    paper['id'],
                    year=result['updates'].get('year'),
                    keywords=result['updates'].get('keywords'),
                    abstract=result['updates'].get('abstract')
                )
                print(f"  âœ… å·²ä¿®å¾©: {', '.join(result['updates'].keys())}")
                success += 1
            else:
                print(f"  âš ï¸ ç„¡æ³•ä¿®å¾©: {result.get('reason', 'æœªçŸ¥')}")
                failed += 1

        print(f"\n{'=' * 60}")
        print(f"ä¿®å¾©å®Œæˆ:")
        print(f"  æˆåŠŸ: {success}")
        print(f"  å¤±æ•—: {failed}")
        print(f"{'=' * 60}\n")
    else:
        # å–®ç¯‡ä¿®å¾©ï¼ˆäº¤äº’å¼ï¼‰
        print("ä½¿ç”¨ --batch é¸é …é€²è¡Œæ‰¹æ¬¡ä¿®å¾©")
        print(f"{'=' * 60}\n")


def cmd_metadata_sync_yaml(args):
    """åŒæ­¥è³‡æ–™åº«æ¨™é¡Œåˆ° YAML front matter"""
    kb = KnowledgeBaseManager()

    print(f"\n{'=' * 60}")
    print("ğŸ”„ åŒæ­¥æ¨™é¡Œåˆ° YAML")
    print(f"{'=' * 60}\n")

    # ç²å–æ‰€æœ‰è«–æ–‡
    conn = sqlite3.connect(kb.db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT id, title, file_path FROM papers ORDER BY id")
    papers = cursor.fetchall()
    conn.close()

    success = 0
    skipped = 0
    failed = 0

    for paper_id, db_title, file_path in papers:
        file_path_obj = Path(file_path)

        if not file_path_obj.exists():
            print(f"[{paper_id}] âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            skipped += 1
            continue

        # è®€å–ä¸¦æ›´æ–° YAML
        try:
            with open(file_path_obj, 'r', encoding='utf-8') as f:
                content = f.read()

            yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)$', content, re.DOTALL)
            if not yaml_match:
                print(f"[{paper_id}] âš ï¸ æ‰¾ä¸åˆ° YAML front matter")
                skipped += 1
                continue

            metadata = yaml.safe_load(yaml_match.group(1))
            yaml_title = metadata.get('title', '')

            if yaml_title == db_title:
                skipped += 1
                continue

            # æ›´æ–°æ¨™é¡Œ
            metadata['title'] = db_title
            new_yaml = yaml.dump(metadata, allow_unicode=True, default_flow_style=False, sort_keys=False)
            new_content = f"---\n{new_yaml}---\n{yaml_match.group(2)}"

            if not args.dry_run:
                with open(file_path_obj, 'w', encoding='utf-8') as f:
                    f.write(new_content)

            print(f"[{paper_id}] âœ… å·²æ›´æ–°: {db_title[:60]}")
            success += 1

        except Exception as e:
            print(f"[{paper_id}] âŒ éŒ¯èª¤: {e}")
            failed += 1

    print(f"\n{'=' * 60}")
    print(f"åŒæ­¥å®Œæˆ:")
    print(f"  æˆåŠŸ: {success}")
    print(f"  è·³é: {skipped}")
    print(f"  å¤±æ•—: {failed}")
    print(f"{'=' * 60}\n")


def cmd_cleanup(args):
    """æ¸…ç†å­¤ç«‹çš„è³‡æ–™åº«è¨˜éŒ„"""
    kb = KnowledgeBaseManager()

    print(f"\n{'=' * 60}")
    print("ğŸ—‘ï¸ æ¸…ç†å­¤ç«‹è¨˜éŒ„")
    print(f"{'=' * 60}\n")

    conn = sqlite3.connect(kb.db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT id, title, file_path FROM papers")
    papers = cursor.fetchall()

    orphans = []
    for pid, title, file_path in papers:
        if not Path(file_path).exists():
            orphans.append((pid, title, file_path))

    if not orphans:
        print("âœ… æ²’æœ‰ç™¼ç¾å­¤ç«‹è¨˜éŒ„ï¼")
        conn.close()
        print(f"{'=' * 60}\n")
        return

    print(f"æ‰¾åˆ° {len(orphans)} ç­†å­¤ç«‹è¨˜éŒ„:\n")

    for pid, title, file_path in orphans:
        print(f"ID {pid}: {title[:60]}")
        print(f"  æª”æ¡ˆ: {file_path}\n")

    if args.dry_run:
        print("âš ï¸ é è¦½æ¨¡å¼ï¼ˆä¸æœƒå¯¦éš›åˆªé™¤ï¼‰")
    else:
        # åˆªé™¤å­¤ç«‹è¨˜éŒ„
        for pid, _, _ in orphans:
            cursor.execute("DELETE FROM papers WHERE id = ?", (pid,))
            cursor.execute("DELETE FROM paper_topics WHERE paper_id = ?", (pid,))
        conn.commit()
        print(f"âœ… å·²åˆªé™¤ {len(orphans)} ç­†è¨˜éŒ„")

    conn.close()
    print(f"{'=' * 60}\n")


def cmd_import_papers(args):
    """å°å…¥æœªè¨˜éŒ„çš„ Markdown æª”æ¡ˆ"""
    kb = KnowledgeBaseManager()
    papers_dir = Path("knowledge_base/papers")

    print(f"\n{'=' * 60}")
    print("ğŸ“¥ å°å…¥æœªè¨˜éŒ„çš„ Markdown æª”æ¡ˆ")
    print(f"{'=' * 60}\n")

    # ç²å–æ‰€æœ‰ Markdown æª”æ¡ˆ
    actual_files = set(f.name for f in papers_dir.glob("*.md"))

    # ç²å–å·²è¨˜éŒ„çš„æª”æ¡ˆ
    conn = sqlite3.connect(kb.db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT file_path FROM papers')
    db_files = set(Path(row[0]).name for row in cursor.fetchall())
    conn.close()

    # æ‰¾å‡ºæœªè¨˜éŒ„çš„æª”æ¡ˆ
    unrecorded = actual_files - db_files

    if not unrecorded:
        print("âœ… æ²’æœ‰æœªè¨˜éŒ„çš„æª”æ¡ˆï¼")
        print(f"{'=' * 60}\n")
        return

    print(f"æ‰¾åˆ° {len(unrecorded)} å€‹æœªè¨˜éŒ„çš„æª”æ¡ˆ\n")

    success = 0
    failed = 0

    for filename in sorted(unrecorded):
        file_path = papers_dir / filename
        print(f"[{success + failed + 1}/{len(unrecorded)}] {filename}")

        try:
            # è®€å– YAML front matter
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            yaml_match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
            if not yaml_match:
                print(f"  âš ï¸ æ‰¾ä¸åˆ° YAML front matter\n")
                failed += 1
                continue

            metadata = yaml.safe_load(yaml_match.group(1))

            title = metadata.get('title', filename.replace('.md', ''))
            authors = metadata.get('authors', '')
            year = metadata.get('year')
            keywords = metadata.get('keywords', [])

            # è§£æ authors
            if isinstance(authors, str):
                authors_list = [a.strip() for a in authors.split(',') if a.strip()]
            elif isinstance(authors, list):
                authors_list = authors
            else:
                authors_list = []

            # è§£æ keywords
            if isinstance(keywords, str):
                keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
            elif isinstance(keywords, list):
                keywords_list = keywords
            else:
                keywords_list = []

            if not args.dry_run:
                # å°å…¥åˆ°è³‡æ–™åº«
                paper_id = kb.add_paper(
                    file_path=str(file_path),
                    title=title,
                    authors=authors_list,
                    year=year,
                    keywords=keywords_list,
                    source='imported'
                )
                print(f"  âœ… å·²å°å…¥ (ID: {paper_id})")
            else:
                print(f"  [DRY RUN] æ¨™é¡Œ: {title[:60]}")

            success += 1

        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            failed += 1

        print()

    print(f"{'=' * 60}")
    print(f"å°å…¥å®Œæˆ:")
    print(f"  æˆåŠŸ: {success}")
    print(f"  å¤±æ•—: {failed}")
    print(f"{'=' * 60}\n")


def cmd_find_relations(args):
    """ç™¼ç¾è«–æ–‡é—œä¿‚ (Phase 2.1)"""
    if RelationFinder is None:
        print("âŒ RelationFinder æœªå®‰è£ï¼Œè«‹ç¢ºèª src/analyzers/ å·²å»ºç«‹")
        return

    finder = RelationFinder()

    print(f"\n{'=' * 80}")
    print(f"ğŸ” è«–æ–‡é—œä¿‚åˆ†æ (ID: {args.paper_id})")
    print(f"{'=' * 80}\n")

    # ç²å–è«–æ–‡ä¿¡æ¯
    kb = KnowledgeBaseManager()
    try:
        paper = kb.get_paper(args.paper_id)
        print(f"ğŸ“„ è«–æ–‡: {paper['title'][:60]}")
        print(f"   ä½œè€…: {', '.join(paper.get('authors', [])[:3])}")
        print(f"   å¹´ä»½: {paper.get('year', 'æœªçŸ¥')}\n")
    except:
        print(f"âŒ è«–æ–‡ ID {args.paper_id} ä¸å­˜åœ¨\n")
        return

    # ç™¼ç¾æ‰€æœ‰é—œä¿‚
    all_relations = finder.find_all_relations(args.paper_id)

    total_relations = sum(len(rels) for rels in all_relations.values())

    if total_relations == 0:
        print("æœªç™¼ç¾ä»»ä½•é—œä¿‚")
        print(f"{'=' * 80}\n")
        return

    # é¡¯ç¤ºå„é¡é—œä¿‚
    for rel_type, relations in all_relations.items():
        if not relations:
            continue

        type_names = {
            'citation': 'å¼•ç”¨é—œä¿‚',
            'shared_topic': 'ä¸»é¡Œé—œè¯',
            'author_collaboration': 'ä½œè€…åˆä½œ',
            'similarity': 'ç›¸ä¼¼è«–æ–‡'
        }

        print(f"ğŸ”— {type_names.get(rel_type, rel_type)} ({len(relations)}å€‹)\n")

        for i, rel in enumerate(relations[:args.limit], 1):
            try:
                target = kb.get_paper(rel.target_id)
                print(f"   {i}. [{rel.target_id}] {target['title'][:60]}")
                print(f"      å¼·åº¦: {rel.strength:.2%}")

                if 'shared_keywords' in rel.metadata:
                    kw_display = ', '.join(rel.metadata['shared_keywords'][:5])
                    if len(rel.metadata['shared_keywords']) > 5:
                        kw_display += f" (+{len(rel.metadata['shared_keywords'])-5}å€‹)"
                    print(f"      å…±äº«é—œéµè©: {kw_display}")

                if 'shared_authors' in rel.metadata:
                    print(f"      å…±åŒä½œè€…: {', '.join(rel.metadata['shared_authors'])}")

                print()
            except:
                print(f"   {i}. [ERROR] Paper {rel.target_id} ç„¡æ³•è¼‰å…¥\n")

    print(f"{'=' * 80}")
    print(f"ç¸½è¨ˆ: {total_relations} å€‹é—œä¿‚")
    print(f"{'=' * 80}\n")


def cmd_build_network(args):
    """æ§‹å»ºå¼•ç”¨ç¶²çµ¡ (Phase 2.1)"""
    if RelationFinder is None:
        print("âŒ RelationFinder æœªå®‰è£ï¼Œè«‹ç¢ºèª src/analyzers/ å·²å»ºç«‹")
        return

    finder = RelationFinder()

    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š æ§‹å»ºå¼•ç”¨ç¶²çµ¡")
    print(f"{'=' * 80}\n")

    # è§£æè«–æ–‡IDåˆ—è¡¨
    if args.paper_ids:
        paper_ids = [int(pid.strip()) for pid in args.paper_ids.split(',')]
        print(f"ç›®æ¨™è«–æ–‡: {len(paper_ids)} ç¯‡ (ID: {', '.join(map(str, paper_ids))})\n")
    else:
        print(f"ç›®æ¨™è«–æ–‡: æ‰€æœ‰è«–æ–‡\n")
        paper_ids = None

    # æ§‹å»ºç¶²çµ¡
    print("æ­£åœ¨æ§‹å»ºç¶²çµ¡...")
    network = finder.build_citation_network(paper_ids)

    print(f"\nâœ… ç¶²çµ¡æ§‹å»ºå®Œæˆ:")
    print(f"   ç¯€é»æ•¸: {network['metadata']['total_nodes']}")
    print(f"   é‚Šæ•¸: {network['metadata']['total_edges']}")

    # å°å‡ºJSON
    if args.output:
        finder.export_to_json(network, args.output)
        print(f"\nğŸ’¾ å·²å°å‡ºJSON: {args.output}")

    # å°å‡ºGraphML (if networkx available)
    if args.graphml:
        try:
            G = finder.export_to_networkx(network)
            finder.export_to_graphml(G, args.graphml)
            print(f"ğŸ’¾ å·²å°å‡ºGraphML: {args.graphml}")
        except ImportError:
            print(f"\nâš ï¸  NetworkXæœªå®‰è£ï¼Œç„¡æ³•å°å‡ºGraphML")
            print(f"    å®‰è£: pip install networkx")

    print(f"\n{'=' * 80}\n")


def cmd_analyze_relations(args):
    """åˆ†æ Zettelkasten æ¦‚å¿µé—œä¿‚ (Phase 2.1)"""
    if RelationFinder is None:
        print("âŒ RelationFinder æœªå®‰è£ï¼Œè«‹ç¢ºèª src/analyzers/ å·²å»ºç«‹")
        return

    print("\n" + "=" * 70)
    print("ğŸ” Zettelkasten æ¦‚å¿µé—œä¿‚åˆ†æ (Phase 2.1)")
    print("=" * 70)

    finder = RelationFinder()

    # æ ¹æ“šåƒæ•¸é¸æ“‡æ“ä½œæ¨¡å¼
    if args.mode == 'find':
        # æ¨¡å¼ 1: åƒ…è­˜åˆ¥é—œä¿‚
        print(f"\næ¨¡å¼: è­˜åˆ¥æ¦‚å¿µé—œä¿‚")
        print(f"æœ€å°ç›¸ä¼¼åº¦: {args.min_similarity}")
        print(f"æœ€å°ä¿¡åº¦: {args.min_confidence}")
        if args.relation_types:
            print(f"é—œä¿‚é¡å‹: {args.relation_types}")

        relations = finder.find_concept_relations(
            min_similarity=args.min_similarity,
            relation_types=args.relation_types.split(',') if args.relation_types else None,
            limit=args.limit
        )

        # éæ¿¾ä¿¡åº¦
        relations = [r for r in relations if r.confidence_score >= args.min_confidence]

        if relations:
            print(f"\nğŸ“‹ é—œä¿‚åˆ—è¡¨ (top {min(len(relations), 20)}):")
            print("-" * 70)
            for i, rel in enumerate(relations[:20], 1):
                print(f"\n{i}. {rel.card_id_1} --{rel.relation_type}--> {rel.card_id_2}")
                print(f"   {rel.card_title_1[:35]} â†’ {rel.card_title_2[:35]}")
                print(f"   ä¿¡åº¦: {rel.confidence_score:.3f} | ç›¸ä¼¼åº¦: {rel.semantic_similarity:.3f}")
                if rel.link_explicit:
                    print(f"   âœ“ æ˜ç¢ºé€£çµå­˜åœ¨")
                if rel.shared_concepts:
                    print(f"   å…±åŒæ¦‚å¿µ: {', '.join(rel.shared_concepts[:5])}")
        else:
            print("\næœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„é—œä¿‚")

    elif args.mode == 'network':
        # æ¨¡å¼ 2: å»ºæ§‹å®Œæ•´ç¶²çµ¡
        print(f"\næ¨¡å¼: å»ºæ§‹æ¦‚å¿µç¶²çµ¡")
        print(f"æœ€å°ç›¸ä¼¼åº¦: {args.min_similarity}")
        print(f"æœ€å°ä¿¡åº¦: {args.min_confidence}")

        network = finder.build_concept_network(
            min_similarity=args.min_similarity,
            relation_types=args.relation_types.split(',') if args.relation_types else None,
            min_confidence=args.min_confidence
        )

        # é¡¯ç¤ºç¶²çµ¡æ‘˜è¦ï¼ˆå·²åœ¨ build_concept_network ä¸­é¡¯ç¤ºï¼‰

        # å°å‡ºç‚º JSON
        if args.output:
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(network, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ ç¶²çµ¡æ•¸æ“šå·²å°å‡º: {args.output}")

        # ç”Ÿæˆå ±å‘Š
        if args.report:
            report_path = Path(args.report)
            report_path.parent.mkdir(parents=True, exist_ok=True)

            report = _generate_relation_report(network, finder)
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ’¾ å ±å‘Šå·²ç”Ÿæˆ: {args.report}")

        # ç”Ÿæˆ Mermaid åœ–è¡¨
        if args.mermaid:
            mermaid_path = Path(args.mermaid)
            mermaid_path.parent.mkdir(parents=True, exist_ok=True)

            mermaid_diagram = _generate_mermaid_diagram(network, max_nodes=args.max_nodes)
            with open(mermaid_path, 'w', encoding='utf-8') as f:
                f.write(f"# Zettelkasten æ¦‚å¿µç¶²çµ¡\n\n{mermaid_diagram}")
            print(f"ğŸ’¾ Mermaid åœ–è¡¨å·²ç”Ÿæˆ: {args.mermaid}")

    print("\n" + "=" * 70 + "\n")


def _generate_relation_report(network: Dict, finder: 'RelationFinder') -> str:
    """ç”Ÿæˆ Zettelkasten æ¦‚å¿µé—œä¿‚åˆ†æå ±å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰"""
    stats = network['statistics']
    hub_nodes = network['hub_nodes']
    relations = network['relations']

    report_lines = [
        "# Zettelkasten æ¦‚å¿µé—œä¿‚åˆ†æå ±å‘Š",
        "",
        f"**ç”Ÿæˆæ™‚é–“**: {Path('.').absolute()}",
        "",
        "---",
        "",
        "## ğŸ“Š ç¶²çµ¡çµ±è¨ˆ",
        "",
        "| æŒ‡æ¨™ | æ•¸å€¼ |",
        "|------|------|",
        f"| **ç¯€é»æ•¸** | {stats['node_count']} |",
        f"| **é‚Šæ•¸** | {stats['edge_count']} |",
        f"| **å¹³å‡åº¦** | {stats['avg_degree']} |",
        f"| **æœ€å¤§åº¦** | {stats['max_degree']} |",
        f"| **æœ€å°åº¦** | {stats['min_degree']} |",
        f"| **ç¶²çµ¡å¯†åº¦** | {stats['density']} |",
        f"| **å¹³å‡ä¿¡åº¦** | {stats['avg_confidence']} |",
        f"| **å¹³å‡ç›¸ä¼¼åº¦** | {stats['avg_similarity']} |",
        "",
        "## ğŸ¯ é—œä¿‚é¡å‹åˆ†å¸ƒ",
        "",
        "| é—œä¿‚é¡å‹ | æ•¸é‡ | ä½”æ¯” |",
        "|---------|------|------|",
    ]

    total_relations = sum(stats['relation_type_counts'].values())
    for rel_type, count in sorted(stats['relation_type_counts'].items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_relations * 100) if total_relations > 0 else 0
        report_lines.append(f"| {rel_type} | {count} | {percentage:.1f}% |")

    report_lines.extend([
        "",
        "## ğŸŒŸ æ ¸å¿ƒç¯€é» (Hub Nodes)",
        "",
        "é«˜åº¦ä¸­å¿ƒæ€§ç¯€é»ï¼ˆé€£æ¥æ•¸æœ€å¤šçš„å¡ç‰‡ï¼‰ï¼š",
        "",
        "| æ’å | å¡ç‰‡ ID | æ¨™é¡Œ | åº¦ | å…¥åº¦ | å‡ºåº¦ |",
        "|------|---------|------|-----|------|------|",
    ])

    for i, node in enumerate(hub_nodes[:10], 1):
        title = node['title'][:40] + "..." if len(node['title']) > 40 else node['title']
        report_lines.append(
            f"| {i} | {node['card_id']} | {title} | {node['degree']} | "
            f"{node['in_degree']} | {node['out_degree']} |"
        )

    report_lines.extend([
        "",
        "## ğŸ”— é«˜ä¿¡åº¦é—œä¿‚ (Top 20)",
        "",
        "ä¿¡åº¦æœ€é«˜çš„æ¦‚å¿µé—œä¿‚ï¼š",
        "",
    ])

    sorted_relations = sorted(relations, key=lambda r: r['confidence_score'], reverse=True)
    for i, rel in enumerate(sorted_relations[:20], 1):
        report_lines.extend([
            f"### {i}. {rel['card_id_1']} â†’ {rel['card_id_2']}",
            f"- **é—œä¿‚é¡å‹**: {rel['relation_type']}",
            f"- **ä¿¡åº¦**: {rel['confidence_score']:.3f}",
            f"- **ç›¸ä¼¼åº¦**: {rel['semantic_similarity']:.3f}",
            f"- **æ˜ç¢ºé€£çµ**: {'æ˜¯' if rel['link_explicit'] else 'å¦'}",
        ])
        if rel['shared_concepts']:
            report_lines.append(f"- **å…±åŒæ¦‚å¿µ**: {', '.join(rel['shared_concepts'][:8])}")
        report_lines.append("")

    report_lines.extend([
        "---",
        "",
        "*å ±å‘Šç”± Knowledge Production System (Phase 2.1) è‡ªå‹•ç”Ÿæˆ*"
    ])

    return "\n".join(report_lines)


def _generate_mermaid_diagram(network: Dict, max_nodes: int = 50) -> str:
    """ç”Ÿæˆ Mermaid æ¦‚å¿µç¶²çµ¡åœ–è¡¨"""
    nodes = network['nodes']
    relations = network['relations']

    # é¸æ“‡æœ€é‡è¦çš„ç¯€é»ï¼ˆé«˜åº¦ç¯€é»ï¼‰
    sorted_nodes = sorted(nodes, key=lambda n: n['degree'], reverse=True)[:max_nodes]
    node_ids = {n['card_id'] for n in sorted_nodes}

    lines = [
        "```mermaid",
        "graph TD",
        ""
    ]

    # æ·»åŠ ç¯€é»
    for node in sorted_nodes:
        node_id = node['card_id'].replace('-', '_')  # Mermaid ID ä¸èƒ½æœ‰é€£å­—è™Ÿ
        title = node['title'][:25] + "..." if len(node['title']) > 25 else node['title']
        lines.append(f"    {node_id}[\"{title}\"]")

    lines.append("")

    # æ·»åŠ é‚Šï¼ˆåªé¡¯ç¤ºé«˜ä¿¡åº¦é—œä¿‚ï¼‰
    sorted_relations = sorted(relations, key=lambda r: r['confidence_score'], reverse=True)
    edge_count = 0
    for rel in sorted_relations:
        if rel['card_id_1'] not in node_ids or rel['card_id_2'] not in node_ids:
            continue

        node1_id = rel['card_id_1'].replace('-', '_')
        node2_id = rel['card_id_2'].replace('-', '_')

        # æ ¹æ“šé—œä¿‚é¡å‹é¸æ“‡ç®­é ­æ¨£å¼
        if rel['relation_type'] == 'leads_to':
            arrow = '-->'
        elif rel['relation_type'] == 'based_on':
            arrow = '<--'
        elif rel['relation_type'] == 'contrasts_with':
            arrow = '-..->'
        elif rel['confidence_score'] >= 0.7:
            arrow = '==>'  # é«˜ä¿¡åº¦
        else:
            arrow = '-->'

        lines.append(f"    {node1_id} {arrow} {node2_id}")
        edge_count += 1

        if edge_count >= 100:  # é™åˆ¶é‚Šæ•¸é¿å…åœ–è¡¨éæ–¼è¤‡é›œ
            break

    lines.append("```")
    return "\n".join(lines)


def cmd_check_cite_keys(args):
    """æª¢æŸ¥ç¼ºå°‘ cite_key çš„è«–æ–‡"""
    kb = KnowledgeBaseManager()
    papers = kb.list_papers_without_cite_key()

    if not papers:
        print("âœ… æ‰€æœ‰è«–æ–‡éƒ½æœ‰ cite_keyï¼")
        return

    print(f"âš ï¸  ç™¼ç¾ {len(papers)} ç¯‡è«–æ–‡ç¼ºå°‘ cite_keyï¼š\n")
    for p in papers:
        authors_str = ', '.join(p['authors'][:2]) if p['authors'] else 'æœªçŸ¥'
        if len(p['authors']) > 2:
            authors_str += f" ç­‰ {len(p['authors'])} ä½ä½œè€…"

        print(f"  ID {p['id']:2d}: {p['title'][:50]}")
        print(f"         ä½œè€…: {authors_str}")
        print(f"         å¹´ä»½: {p['year'] or 'æœªçŸ¥'}")
        print()

    print(f"\nğŸ’¡ è§£æ±ºæ–¹æ³•:")
    print(f"   1. å¾ Zotero å°å‡º 'My Library.bib' æ–‡ä»¶")
    print(f"      ï¼ˆZotero: File â†’ Export Library â†’ BibTeXï¼‰")
    print(f"   2. åŸ·è¡Œé è¦½ï¼špython kb_manage.py update-from-bib 'My Library.bib' --dry-run")
    print(f"   3. ç¢ºèªç„¡èª¤å¾ŒåŸ·è¡Œï¼špython kb_manage.py update-from-bib 'My Library.bib'")
    print()


def cmd_update_from_bib(args):
    """å¾ BibTeX æ–‡ä»¶æ›´æ–° cite_key"""
    from pathlib import Path

    kb = KnowledgeBaseManager()

    if not Path(args.bib_file).exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {args.bib_file}")
        return

    print(f"ğŸ“– æ­£åœ¨è§£æ {args.bib_file}...")
    try:
        result = kb.update_cite_keys_from_bib(args.bib_file, dry_run=args.dry_run)
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{str(e)}")
        return

    print(f"\n{'ğŸ” æ¨¡æ“¬çµæœ' if args.dry_run else 'âœ… æ›´æ–°çµæœ'}:")
    print(f"   ç¸½æ¢ç›®æ•¸: {result['total_entries']}")
    print(f"   æˆåŠŸæ›´æ–°: {result['success_count']}")
    print(f"   å·²æœ‰ cite_key: {result['already_has_key_count']}")
    print(f"   æœªæ‰¾åˆ°åŒ¹é…: {result['not_found_count']}")

    if result['updated']:
        print(f"\nâœ… å·²æ›´æ–°çš„è«–æ–‡:")
        for item in result['updated'][:10]:
            print(f"   ID {item['id']:2d}: {item['cite_key']:20s} - {item['title'][:40]}")
        if len(result['updated']) > 10:
            print(f"   ... ä»¥åŠå…¶ä»– {len(result['updated']) - 10} ç¯‡")

    if result['not_found']:
        print(f"\nâš ï¸  .bib ä¸­æœ‰ä½†çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°çš„è«–æ–‡:")
        for item in result['not_found'][:5]:
            reason = item.get('reason', '')
            print(f"   {item['cite_key']:20s} - {item['title'][:40]} {reason}")
        if len(result['not_found']) > 5:
            print(f"   ... ä»¥åŠå…¶ä»– {len(result['not_found']) - 5} ç¯‡")

    if args.dry_run:
        print(f"\nğŸ’¡ æç¤ºï¼šç§»é™¤ --dry-run åƒæ•¸ä»¥å¯¦éš›æ›´æ–°")


def cmd_set_cite_key(args):
    """æ‰‹å‹•è¨­ç½®è«–æ–‡çš„ cite_key"""
    kb = KnowledgeBaseManager()

    paper = kb.get_paper_by_id(args.paper_id)
    if not paper:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è«–æ–‡ ID {args.paper_id}")
        return

    print(f"è«–æ–‡ä¿¡æ¯:")
    print(f"  ID: {paper['id']}")
    print(f"  æ¨™é¡Œ: {paper['title']}")
    print(f"  ç•¶å‰ cite_key: {paper.get('cite_key', '(ç„¡)')}")
    print(f"  æ–° cite_key: {args.cite_key}")
    print()

    confirm = input("ç¢ºèªæ›´æ–°ï¼Ÿ(y/n): ")
    if confirm.lower() != 'y':
        print("å·²å–æ¶ˆ")
        return

    success = kb.update_cite_key(args.paper_id, args.cite_key)
    if success:
        print(f"âœ… æˆåŠŸæ›´æ–° cite_key ç‚º: {args.cite_key}")
    else:
        print(f"âŒ æ›´æ–°å¤±æ•—")


def cmd_check_llm(args):
    """æª¢æŸ¥ LLM æä¾›è€…è¨ªå•ç‹€æ…‹"""
    print("\n" + "=" * 60)
    print("LLM Access Status Check")
    print("=" * 60)
    print()

    # Check .env file
    env_file = Path(".env")
    if env_file.exists():
        print("[OK] .env file exists")
        try:
            from dotenv import load_dotenv
            load_dotenv()
        except ImportError:
            print("[WARN] python-dotenv not installed, using environment variables")
    else:
        print("[WARN] .env file not found, using environment variables")
    print()

    results = {}
    providers_tested = []

    # Test Ollama
    print("Testing Ollama (local)...", end=" ", flush=True)
    try:
        import requests
        url = "http://localhost:11434"
        response = requests.get(f"{url}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = [m['name'] for m in data.get('models', [])]
            results['Ollama'] = True
            providers_tested.append(f"Ollama: {len(models)} models available")
            print(f"[OK] {len(models)} models available")
        else:
            results['Ollama'] = False
            print(f"[FAIL] HTTP {response.status_code}")
    except Exception as e:
        results['Ollama'] = False
        print(f"[FAIL] {str(e)}")

    # Test Google Gemini
    print("Testing Google Gemini...", end=" ", flush=True)
    try:
        import os
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key or api_key.startswith("your-"):
            results['Gemini'] = False
            print("[FAIL] API key not configured")
        else:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            response = model.generate_content("Say 'Hi'", request_options={"timeout": 10})
            results['Gemini'] = True
            providers_tested.append("Gemini: API key valid")
            print(f"[OK] API key valid")
    except Exception as e:
        results['Gemini'] = False
        print(f"[FAIL] {str(e)}")

    # Test OpenAI
    print("Testing OpenAI...", end=" ", flush=True)
    try:
        import os
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or api_key.startswith("your-"):
            results['OpenAI'] = False
            print("[FAIL] API key not configured")
        else:
            from openai import OpenAI
            client = OpenAI(api_key=api_key, timeout=10)
            models = client.models.list()
            results['OpenAI'] = True
            providers_tested.append(f"OpenAI: {len(models.data)} models available")
            print(f"[OK] {len(models.data)} models available")
    except Exception as e:
        results['OpenAI'] = False
        error_msg = str(e)
        if "401" in error_msg:
            print("[FAIL] Invalid API key")
        else:
            print(f"[FAIL] {error_msg[:50]}")

    # Test Anthropic Claude
    print("Testing Anthropic Claude...", end=" ", flush=True)
    try:
        import os
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key or api_key.startswith("your-"):
            results['Claude'] = False
            print("[FAIL] API key not configured")
        else:
            import anthropic
            client = anthropic.Anthropic(api_key=api_key, timeout=10)
            message = client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=10,
                messages=[{"role": "user", "content": "Hi"}]
            )
            results['Claude'] = True
            providers_tested.append("Claude: API key valid")
            print("[OK] API key valid")
    except Exception as e:
        results['Claude'] = False
        error_msg = str(e)
        if "401" in error_msg:
            print("[FAIL] Invalid API key")
        else:
            print(f"[FAIL] {error_msg[:50]}")

    # Summary
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)

    available = sum(results.values())
    total = len(results)
    print(f"Available providers: {available}/{total}")

    if available == 0:
        print("\n[WARN] No LLM providers available!")
        print("Suggestions:")
        print("  1. Check API keys in .env file")
        print("  2. Ensure Ollama service is running (ollama serve)")
        print("  3. Run 'python test_llm_access.py' for detailed diagnostics")
    elif available < total:
        print("\n[WARN] Some providers unavailable")
        unavailable = [name for name, success in results.items() if not success]
        print(f"Unavailable: {', '.join(unavailable)}")
        print("\nTo configure missing providers:")
        print("  - Edit .env file with valid API keys")
        print("  - See LLM_ACCESS_REPORT.md for setup instructions")
    else:
        print("\n[OK] All providers available!")

    # Show recommendations
    if args.verbose and available > 0:
        print("\n" + "-" * 60)
        print("RECOMMENDATIONS")
        print("-" * 60)
        available_providers = [name for name, success in results.items() if success]

        if 'Gemini' in available_providers:
            print("Primary: Google Gemini (free quota, fast, high quality)")
        if 'Ollama' in available_providers:
            print("Backup: Ollama (free, local, offline)")
        if 'Claude' in available_providers:
            print("Batch: Claude Haiku (cheap ~$0.02/use, fast)")
        if 'OpenAI' in available_providers:
            print("Premium: OpenAI GPT-4 (highest quality)")

    print()


def main():
    parser = argparse.ArgumentParser(
        description="çŸ¥è­˜åº«ç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹çµ±è¨ˆ
  python kb_manage.py stats

  # åˆ—å‡ºè«–æ–‡
  python kb_manage.py list
  python kb_manage.py list --limit 5

  # æœç´¢è«–æ–‡
  python kb_manage.py search "deep learning"
  python kb_manage.py search "AI" --limit 10

  # æŸ¥çœ‹è«–æ–‡è©³æƒ…
  python kb_manage.py show 1

  # å‰µå»ºä¸»é¡Œ
  python kb_manage.py add-topic "AIèˆ‡èªçŸ¥ç§‘å­¸" -d "AIæŠ€è¡“åœ¨èªçŸ¥ç§‘å­¸ä¸­çš„æ‡‰ç”¨"

  # é€£çµè«–æ–‡åˆ°ä¸»é¡Œ
  python kb_manage.py link 1 1 --relevance 0.95

  # æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡
  python kb_manage.py topic-papers "AIèˆ‡èªçŸ¥ç§‘å­¸"

  # æ·»åŠ å¼•ç”¨é—œä¿‚
  python kb_manage.py cite 1 2

  # èªç¾©æœç´¢
  python kb_manage.py semantic-search "æ·±åº¦å­¸ç¿’" --type papers --limit 5
  python kb_manage.py semantic-search "èªçŸ¥ç§‘å­¸" --type all --verbose

  # å°‹æ‰¾ç›¸ä¼¼å…§å®¹
  python kb_manage.py similar 1 --limit 5
  python kb_manage.py similar zettel_CogSci-20251029-001 --limit 3

  # æ··åˆæœç´¢
  python kb_manage.py hybrid-search "machine learning" --limit 10

  # è‡ªå‹•é€£çµè«–æ–‡åˆ°Zettelkastenï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰
  python kb_manage.py auto-link 14 --threshold 0.6 --max-links 5

  # æ‰¹æ¬¡ç‚ºæ‰€æœ‰è«–æ–‡å»ºç«‹é€£çµ
  python kb_manage.py auto-link-all --threshold 0.6 --max-links 5

  # æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ
  python kb_manage.py show-links 14
  python kb_manage.py show-links 14 --min-similarity 0.7

  # å…ƒæ•¸æ“šç®¡ç†
  python kb_manage.py metadata-fix --field year --batch
  python kb_manage.py metadata-fix --field all --batch --dry-run
  python kb_manage.py metadata-sync-yaml
  python kb_manage.py metadata-sync-yaml --dry-run

  # è³‡æ–™åº«ç¶­è­·
  python kb_manage.py cleanup
  python kb_manage.py cleanup --dry-run
  python kb_manage.py import-papers
  python kb_manage.py import-papers --dry-run

  # LLM è¨ªå•æª¢æŸ¥
  python kb_manage.py check-llm
  python kb_manage.py check-llm --verbose
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # stats å‘½ä»¤
    parser_stats = subparsers.add_parser('stats', help='é¡¯ç¤ºçŸ¥è­˜åº«çµ±è¨ˆ')
    parser_stats.set_defaults(func=cmd_stats)

    # list å‘½ä»¤
    parser_list = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰è«–æ–‡')
    parser_list.add_argument('--limit', type=int, default=50, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡')
    parser_list.set_defaults(func=cmd_list)

    # search å‘½ä»¤
    parser_search = subparsers.add_parser('search', help='æœç´¢è«–æ–‡')
    parser_search.add_argument('query', help='æœç´¢é—œéµè©')
    parser_search.add_argument('--limit', type=int, default=10, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡')
    parser_search.set_defaults(func=cmd_search)

    # show å‘½ä»¤
    parser_show = subparsers.add_parser('show', help='é¡¯ç¤ºè«–æ–‡è©³æƒ…')
    parser_show.add_argument('id', type=int, help='è«–æ–‡ID')
    parser_show.set_defaults(func=cmd_show)

    # add-topic å‘½ä»¤
    parser_topic = subparsers.add_parser('add-topic', help='å‰µå»ºä¸»é¡Œ')
    parser_topic.add_argument('name', help='ä¸»é¡Œåç¨±')
    parser_topic.add_argument('-d', '--description', default='', help='ä¸»é¡Œæè¿°')
    parser_topic.set_defaults(func=cmd_add_topic)

    # link å‘½ä»¤
    parser_link = subparsers.add_parser('link', help='é€£çµè«–æ–‡åˆ°ä¸»é¡Œ')
    parser_link.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_link.add_argument('topic_id', type=int, help='ä¸»é¡ŒID')
    parser_link.add_argument('--relevance', type=float, default=1.0, help='ç›¸é—œåº¦ (0-1)')
    parser_link.set_defaults(func=cmd_link)

    # topic-papers å‘½ä»¤
    parser_tp = subparsers.add_parser('topic-papers', help='æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡')
    parser_tp.add_argument('name', help='ä¸»é¡Œåç¨±')
    parser_tp.set_defaults(func=cmd_topic_papers)

    # cite å‘½ä»¤
    parser_cite = subparsers.add_parser('cite', help='æ·»åŠ å¼•ç”¨é—œä¿‚')
    parser_cite.add_argument('source', type=int, help='ä¾†æºè«–æ–‡ID')
    parser_cite.add_argument('target', type=int, help='ç›®æ¨™è«–æ–‡ID')
    parser_cite.add_argument('--type', default='cites', help='å¼•ç”¨é¡å‹')
    parser_cite.set_defaults(func=cmd_cite)

    # semantic-search å‘½ä»¤
    parser_semantic = subparsers.add_parser('semantic-search', help='èªç¾©æœç´¢è«–æ–‡æˆ–Zettelkasten')
    parser_semantic.add_argument('query', help='æœç´¢æŸ¥è©¢')
    parser_semantic.add_argument('--type', choices=['papers', 'zettel', 'all'],
                                default='all', help='æœç´¢é¡å‹ (é»˜èª: all)')
    parser_semantic.add_argument('--limit', type=int, default=5, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 5)')
    parser_semantic.add_argument('--provider', choices=['gemini', 'ollama'],
                                default='gemini', help='åµŒå…¥æä¾›è€… (é»˜èª: gemini)')
    parser_semantic.add_argument('--verbose', '-v', action='store_true',
                                help='é¡¯ç¤ºè©³ç´°ä¿¡æ¯ï¼ˆæ‘˜è¦/å…§å®¹é è¦½ï¼‰')
    parser_semantic.set_defaults(func=cmd_semantic_search)

    # similar å‘½ä»¤
    parser_similar = subparsers.add_parser('similar', help='å°‹æ‰¾ç›¸ä¼¼çš„è«–æ–‡æˆ–Zettelkastenå¡ç‰‡')
    parser_similar.add_argument('id', help='è«–æ–‡ID (æ•¸å­—) æˆ– Zettelkasten ID (å¦‚: zettel_xxx)')
    parser_similar.add_argument('--limit', type=int, default=5, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 5)')
    parser_similar.set_defaults(func=cmd_similar)

    # hybrid-search å‘½ä»¤
    parser_hybrid = subparsers.add_parser('hybrid-search', help='æ··åˆæœç´¢ï¼ˆå…¨æ–‡+èªç¾©ï¼‰')
    parser_hybrid.add_argument('query', help='æœç´¢æŸ¥è©¢')
    parser_hybrid.add_argument('--limit', type=int, default=10, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 10)')
    parser_hybrid.add_argument('--provider', choices=['gemini', 'ollama'],
                              default='gemini', help='åµŒå…¥æä¾›è€… (é»˜èª: gemini)')
    parser_hybrid.set_defaults(func=cmd_hybrid_search)

    # auto-link å‘½ä»¤
    parser_auto_link = subparsers.add_parser('auto-link', help='è‡ªå‹•å»ºç«‹è«–æ–‡èˆ‡Zettelkastençš„é€£çµï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰')
    parser_auto_link.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_auto_link.add_argument('--threshold', type=float, default=0.6,
                                 help='ç›¸ä¼¼åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.6)')
    parser_auto_link.add_argument('--max-links', type=int, default=5,
                                 help='æœ€å¤šå»ºç«‹å¹¾å€‹é€£çµ (é»˜èª: 5)')
    parser_auto_link.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°éŒ¯èª¤ä¿¡æ¯')
    parser_auto_link.set_defaults(func=cmd_auto_link)

    # auto-link-all å‘½ä»¤
    parser_auto_link_all = subparsers.add_parser('auto-link-all', help='ç‚ºæ‰€æœ‰è«–æ–‡æ‰¹æ¬¡å»ºç«‹é€£çµ')
    parser_auto_link_all.add_argument('--threshold', type=float, default=0.6,
                                     help='ç›¸ä¼¼åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.6)')
    parser_auto_link_all.add_argument('--max-links', type=int, default=5,
                                     help='æ¯ç¯‡è«–æ–‡æœ€å¤šå»ºç«‹å¹¾å€‹é€£çµ (é»˜èª: 5)')
    parser_auto_link_all.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°é€²åº¦')
    parser_auto_link_all.set_defaults(func=cmd_auto_link_all)

    # show-links å‘½ä»¤
    parser_show_links = subparsers.add_parser('show-links', help='æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ')
    parser_show_links.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_show_links.add_argument('--min-similarity', type=float, default=0.0,
                                  help='æœ€å°ç›¸ä¼¼åº¦éæ¿¾ (0-1ï¼Œé»˜èª: 0.0)')
    parser_show_links.set_defaults(func=cmd_show_links)

    # metadata fix å‘½ä»¤
    parser_metadata_fix = subparsers.add_parser('metadata-fix', help='ä¿®å¾©ç¼ºå¤±çš„å…ƒæ•¸æ“š')
    parser_metadata_fix.add_argument('--field', choices=['year', 'keywords', 'abstract', 'all'],
                                    default='all', help='è¦ä¿®å¾©çš„å­—æ®µ (é»˜èª: all)')
    parser_metadata_fix.add_argument('--batch', action='store_true', help='æ‰¹æ¬¡ä¿®å¾©')
    parser_metadata_fix.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼ˆä¸å¯¦éš›ä¿®å¾©ï¼‰')
    parser_metadata_fix.set_defaults(func=cmd_metadata_fix)

    # metadata sync-yaml å‘½ä»¤
    parser_metadata_sync = subparsers.add_parser('metadata-sync-yaml',
                                                help='åŒæ­¥è³‡æ–™åº«æ¨™é¡Œåˆ° YAML front matter')
    parser_metadata_sync.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼ˆä¸å¯¦éš›ä¿®æ”¹ï¼‰')
    parser_metadata_sync.set_defaults(func=cmd_metadata_sync_yaml)

    # cleanup å‘½ä»¤
    parser_cleanup = subparsers.add_parser('cleanup', help='æ¸…ç†å­¤ç«‹çš„è³‡æ–™åº«è¨˜éŒ„')
    parser_cleanup.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼ˆä¸å¯¦éš›åˆªé™¤ï¼‰')
    parser_cleanup.set_defaults(func=cmd_cleanup)

    # import-papers å‘½ä»¤
    parser_import = subparsers.add_parser('import-papers', help='å°å…¥æœªè¨˜éŒ„çš„ Markdown æª”æ¡ˆ')
    parser_import.add_argument('--dry-run', action='store_true', help='é è¦½æ¨¡å¼ï¼ˆä¸å¯¦éš›å°å…¥ï¼‰')
    parser_import.set_defaults(func=cmd_import_papers)

    # find-relations å‘½ä»¤ (Phase 2.1)
    parser_find_relations = subparsers.add_parser('find-relations',
                                                  help='ç™¼ç¾è«–æ–‡é—œä¿‚ï¼ˆå¼•ç”¨ã€ä¸»é¡Œã€ä½œè€…åˆä½œï¼‰')
    parser_find_relations.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_find_relations.add_argument('--limit', type=int, default=10,
                                      help='æ¯ç¨®é—œä¿‚æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 10)')
    parser_find_relations.set_defaults(func=cmd_find_relations)

    # build-network å‘½ä»¤ (Phase 2.1)
    parser_build_network = subparsers.add_parser('build-network',
                                                 help='æ§‹å»ºè«–æ–‡å¼•ç”¨ç¶²çµ¡')
    parser_build_network.add_argument('--paper-ids', type=str,
                                     help='è«–æ–‡IDåˆ—è¡¨ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œå¦‚ "1,2,5,6"ï¼Œä¸æŒ‡å®šå‰‡ç‚ºæ‰€æœ‰è«–æ–‡ï¼‰')
    parser_build_network.add_argument('--output', type=str,
                                     help='JSONè¼¸å‡ºè·¯å¾‘ (ä¾‹å¦‚: network.json)')
    parser_build_network.add_argument('--graphml', type=str,
                                     help='GraphMLè¼¸å‡ºè·¯å¾‘ (ä¾‹å¦‚: network.graphmlï¼Œéœ€å®‰è£networkx)')
    parser_build_network.set_defaults(func=cmd_build_network)

    # analyze-relations å‘½ä»¤ (Phase 2.1 - Zettelkasten)
    parser_analyze_relations = subparsers.add_parser('analyze-relations',
                                                     help='åˆ†æ Zettelkasten æ¦‚å¿µé—œä¿‚ (Phase 2.1)')
    parser_analyze_relations.add_argument('--mode', choices=['find', 'network'],
                                         default='network',
                                         help='æ“ä½œæ¨¡å¼: find (åƒ…è­˜åˆ¥é—œä¿‚) æˆ– network (å»ºæ§‹ç¶²çµ¡)')
    parser_analyze_relations.add_argument('--min-similarity', type=float, default=0.4,
                                         help='æœ€å°èªç¾©ç›¸ä¼¼åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.4)')
    parser_analyze_relations.add_argument('--min-confidence', type=float, default=0.3,
                                         help='æœ€å°ä¿¡åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.3)')
    parser_analyze_relations.add_argument('--relation-types', type=str,
                                         help='é—œä¿‚é¡å‹éæ¿¾ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œå¦‚ "leads_to,based_on"ï¼‰')
    parser_analyze_relations.add_argument('--limit', type=int, default=100,
                                         help='æ¯å¼µå¡ç‰‡æª¢æŸ¥çš„æœ€å¤§ç›¸ä¼¼å¡ç‰‡æ•¸ (é»˜èª: 100)')
    parser_analyze_relations.add_argument('--output', type=str,
                                         help='ç¶²çµ¡æ•¸æ“š JSON è¼¸å‡ºè·¯å¾‘')
    parser_analyze_relations.add_argument('--report', type=str,
                                         help='Markdown å ±å‘Šè¼¸å‡ºè·¯å¾‘')
    parser_analyze_relations.add_argument('--mermaid', type=str,
                                         help='Mermaid åœ–è¡¨è¼¸å‡ºè·¯å¾‘')
    parser_analyze_relations.add_argument('--max-nodes', type=int, default=50,
                                         help='Mermaid åœ–è¡¨æœ€å¤§ç¯€é»æ•¸ (é»˜èª: 50)')
    parser_analyze_relations.set_defaults(func=cmd_analyze_relations)

    # check-cite-keys å‘½ä»¤ (Phase 2)
    parser_check_keys = subparsers.add_parser('check-cite-keys',
                                             help='æª¢æŸ¥ç¼ºå°‘ cite_key çš„è«–æ–‡')
    parser_check_keys.set_defaults(func=cmd_check_cite_keys)

    # update-from-bib å‘½ä»¤ (Phase 2)
    parser_update_bib = subparsers.add_parser('update-from-bib',
                                             help='å¾ BibTeX æ–‡ä»¶æ›´æ–° cite_key')
    parser_update_bib.add_argument('bib_file', help='.bib æ–‡ä»¶è·¯å¾‘')
    parser_update_bib.add_argument('--dry-run', action='store_true',
                                  help='åªæ¨¡æ“¬ï¼Œä¸å¯¦éš›æ›´æ–°')
    parser_update_bib.set_defaults(func=cmd_update_from_bib)

    # set-cite-key å‘½ä»¤ (Phase 2)
    parser_set_key = subparsers.add_parser('set-cite-key',
                                          help='æ‰‹å‹•è¨­ç½®è«–æ–‡çš„ cite_key')
    parser_set_key.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_set_key.add_argument('cite_key', help='BibTeX cite_key')
    parser_set_key.set_defaults(func=cmd_set_cite_key)

    # check-llm å‘½ä»¤
    parser_check_llm = subparsers.add_parser('check-llm',
                                            help='æª¢æŸ¥ LLM æä¾›è€…è¨ªå•ç‹€æ…‹')
    parser_check_llm.add_argument('--verbose', '-v', action='store_true',
                                 help='é¡¯ç¤ºè©³ç´°å»ºè­°')
    parser_check_llm.set_defaults(func=cmd_check_llm)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    args.func(args)


if __name__ == "__main__":
    main()
