#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è­˜åº«ç®¡ç†å‘½ä»¤è¡Œå·¥å…·
ä½¿ç”¨æ–¹å¼: python kb_manage.py <command> [options]
"""

import sys
import argparse
from pathlib import Path

# è¨­ç½®UTF-8ç·¨ç¢¼ï¼ˆWindowsç›¸å®¹æ€§ï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

sys.path.insert(0, str(Path(__file__).parent))

from src.knowledge_base import KnowledgeBaseManager
from src.embeddings.providers import GeminiEmbedder, OllamaEmbedder
from src.embeddings.vector_db import VectorDatabase


def cmd_stats(args):
    """é¡¯ç¤ºçŸ¥è­˜åº«çµ±è¨ˆä¿¡æ¯"""
    kb = KnowledgeBaseManager()
    stats = kb.get_stats()

    print("\n" + "=" * 60)
    print("ğŸ“Š çŸ¥è­˜åº«çµ±è¨ˆ")
    print("=" * 60)
    print(f"è«–æ–‡ç¸½æ•¸: {stats['total_papers']}")
    print(f"ä¸»é¡Œç¸½æ•¸: {stats['total_topics']}")
    print(f"å¼•ç”¨ç¸½æ•¸: {stats['total_citations']}")
    print("=" * 60 + "\n")


def cmd_list(args):
    """åˆ—å‡ºæ‰€æœ‰è«–æ–‡"""
    kb = KnowledgeBaseManager()
    papers = kb.list_papers(limit=args.limit)

    print("\n" + "=" * 60)
    print(f"ğŸ“„ è«–æ–‡åˆ—è¡¨ (æœ€å¤š {args.limit} ç¯‡)")
    print("=" * 60)

    if papers:
        for paper in papers:
            print(f"\n[ID: {paper['id']}] {paper['title']}")
            print(f"  ä½œè€…: {', '.join(paper['authors'][:3])}")
            if len(paper['authors']) > 3:
                print(f"        (+{len(paper['authors'])-3} ä½)")
            print(f"  å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
            if paper['keywords']:
                print(f"  é—œéµè©: {', '.join(paper['keywords'][:5])}")
            print(f"  æ™‚é–“: {paper['created_at']}")
    else:
        print("\nâš ï¸  çŸ¥è­˜åº«ä¸­é‚„æ²’æœ‰è«–æ–‡")

    print("\n" + "=" * 60 + "\n")


def cmd_search(args):
    """æœç´¢è«–æ–‡"""
    kb = KnowledgeBaseManager()
    results = kb.search_papers(args.query, limit=args.limit)

    print("\n" + "=" * 60)
    print(f"ğŸ” æœç´¢: '{args.query}'")
    print("=" * 60)

    if results:
        print(f"\næ‰¾åˆ° {len(results)} å€‹çµæœ:\n")
        for i, paper in enumerate(results, 1):
            print(f"{i}. [ID: {paper['id']}] {paper['title']}")
            print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
            print(f"   å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
            if paper['abstract']:
                preview = paper['abstract'][:100].replace('\n', ' ')
                print(f"   æ‘˜è¦: {preview}...")
            print()
    else:
        print(f"\nâŒ æœªæ‰¾åˆ°åŒ…å« '{args.query}' çš„è«–æ–‡\n")

    print("=" * 60 + "\n")


def cmd_show(args):
    """é¡¯ç¤ºè«–æ–‡è©³æƒ…"""
    kb = KnowledgeBaseManager()
    paper = kb.get_paper_by_id(args.id)

    print("\n" + "=" * 60)
    print(f"ğŸ“„ è«–æ–‡è©³æƒ… (ID: {args.id})")
    print("=" * 60)

    if paper:
        print(f"\næ¨™é¡Œ: {paper['title']}")
        print(f"ä½œè€…: {', '.join(paper['authors'])}")
        print(f"å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")
        print(f"æª”æ¡ˆ: {paper['file_path']}")
        print(f"å‰µå»º: {paper['created_at']}")
        print(f"æ›´æ–°: {paper['updated_at']}")

        if paper['keywords']:
            print(f"é—œéµè©: {', '.join(paper['keywords'])}")

        if paper['abstract']:
            print(f"\næ‘˜è¦:")
            print(f"{paper['abstract'][:500]}")
            if len(paper['abstract']) > 500:
                print("...")
    else:
        print(f"\nâŒ æ‰¾ä¸åˆ°IDç‚º {args.id} çš„è«–æ–‡")

    print("\n" + "=" * 60 + "\n")


def cmd_add_topic(args):
    """å‰µå»ºä¸»é¡Œ"""
    kb = KnowledgeBaseManager()
    topic_id = kb.add_topic(args.name, args.description)

    print("\n" + "=" * 60)
    print("ğŸ·ï¸  å‰µå»ºä¸»é¡Œ")
    print("=" * 60)
    print(f"åç¨±: {args.name}")
    print(f"æè¿°: {args.description}")
    print(f"ID: {topic_id}")
    print("=" * 60 + "\n")


def cmd_link(args):
    """é€£çµè«–æ–‡åˆ°ä¸»é¡Œ"""
    kb = KnowledgeBaseManager()
    kb.link_paper_to_topic(args.paper_id, args.topic_id, args.relevance)

    print("\n" + "=" * 60)
    print("ğŸ”— é€£çµè«–æ–‡èˆ‡ä¸»é¡Œ")
    print("=" * 60)
    print(f"è«–æ–‡ID: {args.paper_id}")
    print(f"ä¸»é¡ŒID: {args.topic_id}")
    print(f"ç›¸é—œåº¦: {args.relevance}")
    print("âœ… é€£çµæˆåŠŸ")
    print("=" * 60 + "\n")


def cmd_topic_papers(args):
    """æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡"""
    kb = KnowledgeBaseManager()
    papers = kb.get_papers_by_topic(args.name)

    print("\n" + "=" * 60)
    print(f"ğŸ·ï¸  ä¸»é¡Œ: {args.name}")
    print("=" * 60)

    if papers:
        print(f"\næ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡:\n")
        for paper in papers:
            print(f"[ID: {paper['id']}] {paper['title']}")
            print(f"  ä½œè€…: {', '.join(paper['authors'][:3])}")
            print(f"  ç›¸é—œåº¦: {paper['relevance']:.2f}")
            print()
    else:
        print(f"\nâš ï¸  ä¸»é¡Œ '{args.name}' ä¸‹æ²’æœ‰è«–æ–‡\n")

    print("=" * 60 + "\n")


def cmd_cite(args):
    """æ·»åŠ å¼•ç”¨é—œä¿‚"""
    kb = KnowledgeBaseManager()
    kb.add_citation(args.source, args.target, args.type)

    print("\n" + "=" * 60)
    print("ğŸ“š æ·»åŠ å¼•ç”¨é—œä¿‚")
    print("=" * 60)
    print(f"ä¾†æºè«–æ–‡ID: {args.source}")
    print(f"ç›®æ¨™è«–æ–‡ID: {args.target}")
    print(f"å¼•ç”¨é¡å‹: {args.type}")
    print("âœ… å¼•ç”¨é—œä¿‚å·²æ·»åŠ ")
    print("=" * 60 + "\n")


def cmd_semantic_search(args):
    """èªç¾©æœç´¢è«–æ–‡æˆ–Zettelkastenå¡ç‰‡"""
    # åˆå§‹åŒ–
    if args.provider == "gemini":
        embedder = GeminiEmbedder()
    else:
        embedder = OllamaEmbedder()

    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)
    print(f"ğŸ” èªç¾©æœç´¢: '{args.query}'")
    print(f"æä¾›è€…: {args.provider.upper()}")
    print("=" * 60)

    # ç”ŸæˆæŸ¥è©¢å‘é‡
    print("\nç”ŸæˆæŸ¥è©¢å‘é‡...")
    query_embedding = embedder.embed(args.query, task_type="retrieval_query")

    # æœç´¢è«–æ–‡
    if args.type in ['papers', 'all']:
        print(f"\nğŸ“„ æœç´¢è«–æ–‡ (top {args.limit}):")
        print("-" * 60)

        results = vector_db.semantic_search_papers(
            query_embedding=query_embedding,
            n_results=args.limit
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            for i, (paper_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                metadata = results['metadatas'][0][i]
                similarity = (1 - distance) * 100  # è½‰æ›ç‚ºç™¾åˆ†æ¯”

                # å¾çŸ¥è­˜åº«ç²å–å®Œæ•´ä¿¡æ¯
                pid = int(paper_id.replace('paper_', ''))
                paper = kb.get_paper_by_id(pid)

                if paper:
                    print(f"\n{i+1}. [{similarity:.1f}%] {paper['title']}")
                    print(f"   ID: {pid}")
                    print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
                    if len(paper['authors']) > 3:
                        print(f"         (+{len(paper['authors'])-3} ä½)")
                    print(f"   å¹´ä»½: {paper['year'] or 'æœªçŸ¥'}")

                    if args.verbose and paper['abstract']:
                        preview = paper['abstract'][:150].replace('\n', ' ')
                        print(f"   æ‘˜è¦: {preview}...")
        else:
            print("\næœªæ‰¾åˆ°ç›¸é—œè«–æ–‡")

    # æœç´¢Zettelkasten
    if args.type in ['zettel', 'all']:
        print(f"\nğŸ—‚ï¸  æœç´¢ Zettelkasten å¡ç‰‡ (top {args.limit}):")
        print("-" * 60)

        results = vector_db.semantic_search_zettel(
            query_embedding=query_embedding,
            n_results=args.limit
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            for i, (zettel_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                metadata = results['metadatas'][0][i]
                similarity = (1 - distance) * 100
                title = metadata.get('title', 'Unknown')

                print(f"\n{i+1}. [{similarity:.1f}%] {title}")
                print(f"   ID: {zettel_id}")

                if args.verbose:
                    doc = results['documents'][0][i]
                    preview = doc[:150].replace('\n', ' ')
                    print(f"   å…§å®¹: {preview}...")
        else:
            print("\næœªæ‰¾åˆ°ç›¸é—œå¡ç‰‡")

    print("\n" + "=" * 60 + "\n")


def cmd_similar(args):
    """å°‹æ‰¾ç›¸ä¼¼çš„è«–æ–‡æˆ–Zettelkastenå¡ç‰‡"""
    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)

    # åˆ¤æ–·æ˜¯è«–æ–‡é‚„æ˜¯Zettelkasten
    if args.id.startswith('paper_') or args.id.isdigit():
        # è«–æ–‡
        paper_id = f"paper_{args.id}" if args.id.isdigit() else args.id
        pid = int(paper_id.replace('paper_', ''))

        paper = kb.get_paper_by_id(pid)
        if not paper:
            print(f"âŒ æ‰¾ä¸åˆ°è«–æ–‡ ID: {args.id}")
            print("=" * 60 + "\n")
            return

        print(f"ğŸ” å°‹æ‰¾èˆ‡è«–æ–‡ç›¸ä¼¼çš„å…§å®¹")
        print(f"è«–æ–‡: {paper['title']}")
        print("=" * 60)

        # å°‹æ‰¾ç›¸ä¼¼è«–æ–‡
        results = vector_db.find_similar_papers(
            paper_id=paper_id,
            n_results=args.limit,
            exclude_self=True
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            print(f"\nğŸ“„ ç›¸ä¼¼è«–æ–‡ (top {args.limit}):")
            print("-" * 60)

            for i, (sim_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                similarity = (1 - distance) * 100
                sim_pid = int(sim_id.replace('paper_', ''))
                sim_paper = kb.get_paper_by_id(sim_pid)

                if sim_paper:
                    print(f"\n{i+1}. [{similarity:.1f}%] {sim_paper['title']}")
                    print(f"   ID: {sim_pid}")
                    print(f"   ä½œè€…: {', '.join(sim_paper['authors'][:3])}")
        else:
            print("\næœªæ‰¾åˆ°ç›¸ä¼¼è«–æ–‡")

    else:
        # Zettelkasten
        zettel_id = args.id

        print(f"ğŸ” å°‹æ‰¾èˆ‡å¡ç‰‡ç›¸ä¼¼çš„å…§å®¹")
        print(f"å¡ç‰‡ ID: {zettel_id}")
        print("=" * 60)

        # å°‹æ‰¾ç›¸ä¼¼å¡ç‰‡
        results = vector_db.find_similar_zettel(
            zettel_id=zettel_id,
            n_results=args.limit,
            exclude_self=True
        )

        if results['ids'] and len(results['ids'][0]) > 0:
            print(f"\nğŸ—‚ï¸  ç›¸ä¼¼å¡ç‰‡ (top {args.limit}):")
            print("-" * 60)

            for i, (sim_id, distance) in enumerate(zip(results['ids'][0], results['distances'][0])):
                similarity = (1 - distance) * 100
                metadata = results['metadatas'][0][i]
                title = metadata.get('title', 'Unknown')

                print(f"\n{i+1}. [{similarity:.1f}%] {title}")
                print(f"   ID: {sim_id}")
        else:
            print("\næœªæ‰¾åˆ°ç›¸ä¼¼å¡ç‰‡")

    print("\n" + "=" * 60 + "\n")


def cmd_hybrid_search(args):
    """æ··åˆæœç´¢ï¼šçµåˆå…¨æ–‡æœç´¢å’Œèªç¾©æœç´¢"""
    # åˆå§‹åŒ–
    if args.provider == "gemini":
        embedder = GeminiEmbedder()
    else:
        embedder = OllamaEmbedder()

    vector_db = VectorDatabase()
    kb = KnowledgeBaseManager()

    print("\n" + "=" * 60)
    print(f"ğŸ” æ··åˆæœç´¢: '{args.query}'")
    print(f"æä¾›è€…: {args.provider.upper()}")
    print("=" * 60)

    # 1. å…¨æ–‡æœç´¢
    print("\nğŸ“ å…¨æ–‡æœç´¢çµæœ:")
    print("-" * 60)
    fts_results = kb.search_papers(args.query, limit=args.limit)
    fts_ids = set()

    if fts_results:
        for i, paper in enumerate(fts_results, 1):
            fts_ids.add(paper['id'])
            print(f"{i}. [FTS] {paper['title']}")
            print(f"   ID: {paper['id']}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    # 2. èªç¾©æœç´¢
    print(f"\nğŸ” èªç¾©æœç´¢çµæœ:")
    print("-" * 60)
    print("ç”ŸæˆæŸ¥è©¢å‘é‡...")
    query_embedding = embedder.embed(args.query, task_type="retrieval_query")

    sem_results = vector_db.semantic_search_papers(
        query_embedding=query_embedding,
        n_results=args.limit
    )

    sem_ids = set()
    sem_scores = {}

    if sem_results['ids'] and len(sem_results['ids'][0]) > 0:
        for i, (paper_id, distance) in enumerate(zip(sem_results['ids'][0], sem_results['distances'][0])):
            pid = int(paper_id.replace('paper_', ''))
            similarity = (1 - distance) * 100
            sem_ids.add(pid)
            sem_scores[pid] = similarity

            paper = kb.get_paper_by_id(pid)
            if paper:
                print(f"{i+1}. [{similarity:.1f}%] {paper['title']}")
                print(f"   ID: {pid}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    # 3. æ··åˆçµæœ
    print(f"\nâœ¨ æ··åˆçµæœ (å…©ç¨®æ–¹æ³•çš„è¯é›†):")
    print("-" * 60)

    all_ids = fts_ids | sem_ids
    both_ids = fts_ids & sem_ids

    if all_ids:
        # æŒ‰èªç¾©ç›¸ä¼¼åº¦æ’åº
        sorted_ids = sorted(all_ids,
                          key=lambda x: sem_scores.get(x, 0),
                          reverse=True)[:args.limit]

        for i, pid in enumerate(sorted_ids, 1):
            paper = kb.get_paper_by_id(pid)
            if paper:
                tags = []
                if pid in fts_ids:
                    tags.append("FTS")
                if pid in sem_ids:
                    tags.append(f"SEM {sem_scores[pid]:.1f}%")

                tag_str = " + ".join(tags)

                print(f"\n{i}. [{tag_str}] {paper['title']}")
                print(f"   ID: {pid}")
                print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}")
    else:
        print("æœªæ‰¾åˆ°çµæœ")

    print(f"\nçµ±è¨ˆ:")
    print(f"  å…¨æ–‡æœç´¢: {len(fts_ids)} ç¯‡")
    print(f"  èªç¾©æœç´¢: {len(sem_ids)} ç¯‡")
    print(f"  å…±åŒçµæœ: {len(both_ids)} ç¯‡")
    print(f"  ç¸½è¨ˆ: {len(all_ids)} ç¯‡")

    print("\n" + "=" * 60 + "\n")


def cmd_auto_link(args):
    """ç‚ºè«–æ–‡è‡ªå‹•å»ºç«‹èˆ‡Zettelkastençš„é€£çµï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
    from src.knowledge_base.auto_link import auto_link_v2

    print("\n" + "=" * 60)
    print(f"ğŸ”— è‡ªå‹•é€£çµè«–æ–‡ {args.paper_id}")
    print("=" * 60)
    print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.threshold}")
    print(f"æœ€å¤šé€£çµ: {args.max_links}")

    try:
        result = auto_link_v2(
            paper_id=args.paper_id,
            threshold=args.threshold,
            max_links=args.max_links
        )

        print(f"\nâœ… å®Œæˆï¼")
        print(f"è«–æ–‡: {result['paper_title']}")
        print(f"å»ºç«‹é€£çµ: {result['links_created']} å€‹")
        print(f"å€™é¸ç¸½æ•¸: {result['candidates_found']} å€‹ (>= {args.threshold} ç›¸ä¼¼åº¦)")

        if result['links']:
            print("\né€£çµè©³æƒ…:")
            print("-" * 60)
            for i, link in enumerate(result['links'], 1):
                print(f"\n{i}. [{link['similarity']:.1%}] {link['title']}")
                print(f"   ID: {link['zettel_id']} (card_id: {link['card_id']})")
                if link['core_concept']:
                    print(f"   æ ¸å¿ƒæ¦‚å¿µ: {link['core_concept'][:80]}...")
        else:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {args.threshold} çš„å¡ç‰‡")

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {str(e)}")
        import traceback
        if args.verbose:
            traceback.print_exc()

    print("\n" + "=" * 60 + "\n")


def cmd_auto_link_all(args):
    """ç‚ºæ‰€æœ‰è«–æ–‡æ‰¹æ¬¡å»ºç«‹é€£çµ"""
    from src.knowledge_base.auto_link import auto_link_all_papers

    print("\n" + "=" * 60)
    print("ğŸ”— æ‰¹æ¬¡è‡ªå‹•é€£çµæ‰€æœ‰è«–æ–‡")
    print("=" * 60)
    print(f"ç›¸ä¼¼åº¦é–¾å€¼: {args.threshold}")
    print(f"æ¯ç¯‡æœ€å¤šé€£çµ: {args.max_links}")

    result = auto_link_all_papers(
        threshold=args.threshold,
        max_links=args.max_links,
        verbose=args.verbose
    )

    print("\n" + "=" * 60)
    print("âœ… æ‰¹æ¬¡è™•ç†å®Œæˆ")
    print("=" * 60)
    print(f"ç¸½è«–æ–‡æ•¸: {result['total_papers']}")
    print(f"æˆåŠŸè™•ç†: {result['processed']}")
    print(f"å¤±æ•—æ•¸é‡: {result['failed']}")
    print(f"ç¸½é€£çµæ•¸: {result['total_links_created']}")
    print(f"å¹³å‡æ¯ç¯‡: {result['average_links_per_paper']:.2f} å€‹é€£çµ")
    print("\n" + "=" * 60 + "\n")


def cmd_show_links(args):
    """æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ"""
    kb = KnowledgeBaseManager()

    paper = kb.get_paper_by_id(args.paper_id)
    if not paper:
        print(f"\nâŒ è«–æ–‡ ID {args.paper_id} ä¸å­˜åœ¨\n")
        return

    links = kb.get_paper_zettel_links(args.paper_id, min_similarity=args.min_similarity)

    print("\n" + "=" * 60)
    print(f"ğŸ”— è«–æ–‡çš„ Zettelkasten é€£çµ")
    print("=" * 60)
    print(f"è«–æ–‡: {paper['title']}")
    print(f"ID: {args.paper_id}")
    print(f"é€£çµæ•¸: {len(links)}")
    print("=" * 60)

    if links:
        for i, link in enumerate(links, 1):
            print(f"\n{i}. [{link['similarity']:.1%}] {link['title']}")
            print(f"   Zettel ID: {link['zettel_id']}")
            print(f"   Card ID: {link['card_id']}")
            print(f"   é¡å‹: {link['card_type']} | é ˜åŸŸ: {link['domain']}")
            if link['core_concept']:
                print(f"   æ ¸å¿ƒæ¦‚å¿µ: {link['core_concept'][:100]}...")
            print(f"   é€£çµæ–¹æ³•: {link['method']}")
            print(f"   å‰µå»ºæ™‚é–“: {link['created_at']}")
    else:
        threshold_msg = f" (ç›¸ä¼¼åº¦ >= {args.min_similarity})" if args.min_similarity > 0 else ""
        print(f"\nâš ï¸  æ­¤è«–æ–‡æ²’æœ‰é€£çµ{threshold_msg}")
        print("æç¤º: åŸ·è¡Œ 'python kb_manage.py auto-link <paper_id>' ä¾†å»ºç«‹é€£çµ")

    print("\n" + "=" * 60 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="çŸ¥è­˜åº«ç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹çµ±è¨ˆ
  python kb_manage.py stats

  # åˆ—å‡ºè«–æ–‡
  python kb_manage.py list
  python kb_manage.py list --limit 5

  # æœç´¢è«–æ–‡
  python kb_manage.py search "deep learning"
  python kb_manage.py search "AI" --limit 10

  # æŸ¥çœ‹è«–æ–‡è©³æƒ…
  python kb_manage.py show 1

  # å‰µå»ºä¸»é¡Œ
  python kb_manage.py add-topic "AIèˆ‡èªçŸ¥ç§‘å­¸" -d "AIæŠ€è¡“åœ¨èªçŸ¥ç§‘å­¸ä¸­çš„æ‡‰ç”¨"

  # é€£çµè«–æ–‡åˆ°ä¸»é¡Œ
  python kb_manage.py link 1 1 --relevance 0.95

  # æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡
  python kb_manage.py topic-papers "AIèˆ‡èªçŸ¥ç§‘å­¸"

  # æ·»åŠ å¼•ç”¨é—œä¿‚
  python kb_manage.py cite 1 2

  # èªç¾©æœç´¢
  python kb_manage.py semantic-search "æ·±åº¦å­¸ç¿’" --type papers --limit 5
  python kb_manage.py semantic-search "èªçŸ¥ç§‘å­¸" --type all --verbose

  # å°‹æ‰¾ç›¸ä¼¼å…§å®¹
  python kb_manage.py similar 1 --limit 5
  python kb_manage.py similar zettel_CogSci-20251029-001 --limit 3

  # æ··åˆæœç´¢
  python kb_manage.py hybrid-search "machine learning" --limit 10

  # è‡ªå‹•é€£çµè«–æ–‡åˆ°Zettelkastenï¼ˆåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ï¼‰
  python kb_manage.py auto-link 14 --threshold 0.6 --max-links 5

  # æ‰¹æ¬¡ç‚ºæ‰€æœ‰è«–æ–‡å»ºç«‹é€£çµ
  python kb_manage.py auto-link-all --threshold 0.6 --max-links 5

  # æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ
  python kb_manage.py show-links 14
  python kb_manage.py show-links 14 --min-similarity 0.7
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # stats å‘½ä»¤
    parser_stats = subparsers.add_parser('stats', help='é¡¯ç¤ºçŸ¥è­˜åº«çµ±è¨ˆ')
    parser_stats.set_defaults(func=cmd_stats)

    # list å‘½ä»¤
    parser_list = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰è«–æ–‡')
    parser_list.add_argument('--limit', type=int, default=50, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡')
    parser_list.set_defaults(func=cmd_list)

    # search å‘½ä»¤
    parser_search = subparsers.add_parser('search', help='æœç´¢è«–æ–‡')
    parser_search.add_argument('query', help='æœç´¢é—œéµè©')
    parser_search.add_argument('--limit', type=int, default=10, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡')
    parser_search.set_defaults(func=cmd_search)

    # show å‘½ä»¤
    parser_show = subparsers.add_parser('show', help='é¡¯ç¤ºè«–æ–‡è©³æƒ…')
    parser_show.add_argument('id', type=int, help='è«–æ–‡ID')
    parser_show.set_defaults(func=cmd_show)

    # add-topic å‘½ä»¤
    parser_topic = subparsers.add_parser('add-topic', help='å‰µå»ºä¸»é¡Œ')
    parser_topic.add_argument('name', help='ä¸»é¡Œåç¨±')
    parser_topic.add_argument('-d', '--description', default='', help='ä¸»é¡Œæè¿°')
    parser_topic.set_defaults(func=cmd_add_topic)

    # link å‘½ä»¤
    parser_link = subparsers.add_parser('link', help='é€£çµè«–æ–‡åˆ°ä¸»é¡Œ')
    parser_link.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_link.add_argument('topic_id', type=int, help='ä¸»é¡ŒID')
    parser_link.add_argument('--relevance', type=float, default=1.0, help='ç›¸é—œåº¦ (0-1)')
    parser_link.set_defaults(func=cmd_link)

    # topic-papers å‘½ä»¤
    parser_tp = subparsers.add_parser('topic-papers', help='æŒ‰ä¸»é¡ŒæŸ¥çœ‹è«–æ–‡')
    parser_tp.add_argument('name', help='ä¸»é¡Œåç¨±')
    parser_tp.set_defaults(func=cmd_topic_papers)

    # cite å‘½ä»¤
    parser_cite = subparsers.add_parser('cite', help='æ·»åŠ å¼•ç”¨é—œä¿‚')
    parser_cite.add_argument('source', type=int, help='ä¾†æºè«–æ–‡ID')
    parser_cite.add_argument('target', type=int, help='ç›®æ¨™è«–æ–‡ID')
    parser_cite.add_argument('--type', default='cites', help='å¼•ç”¨é¡å‹')
    parser_cite.set_defaults(func=cmd_cite)

    # semantic-search å‘½ä»¤
    parser_semantic = subparsers.add_parser('semantic-search', help='èªç¾©æœç´¢è«–æ–‡æˆ–Zettelkasten')
    parser_semantic.add_argument('query', help='æœç´¢æŸ¥è©¢')
    parser_semantic.add_argument('--type', choices=['papers', 'zettel', 'all'],
                                default='all', help='æœç´¢é¡å‹ (é»˜èª: all)')
    parser_semantic.add_argument('--limit', type=int, default=5, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 5)')
    parser_semantic.add_argument('--provider', choices=['gemini', 'ollama'],
                                default='gemini', help='åµŒå…¥æä¾›è€… (é»˜èª: gemini)')
    parser_semantic.add_argument('--verbose', '-v', action='store_true',
                                help='é¡¯ç¤ºè©³ç´°ä¿¡æ¯ï¼ˆæ‘˜è¦/å…§å®¹é è¦½ï¼‰')
    parser_semantic.set_defaults(func=cmd_semantic_search)

    # similar å‘½ä»¤
    parser_similar = subparsers.add_parser('similar', help='å°‹æ‰¾ç›¸ä¼¼çš„è«–æ–‡æˆ–Zettelkastenå¡ç‰‡')
    parser_similar.add_argument('id', help='è«–æ–‡ID (æ•¸å­—) æˆ– Zettelkasten ID (å¦‚: zettel_xxx)')
    parser_similar.add_argument('--limit', type=int, default=5, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 5)')
    parser_similar.set_defaults(func=cmd_similar)

    # hybrid-search å‘½ä»¤
    parser_hybrid = subparsers.add_parser('hybrid-search', help='æ··åˆæœç´¢ï¼ˆå…¨æ–‡+èªç¾©ï¼‰')
    parser_hybrid.add_argument('query', help='æœç´¢æŸ¥è©¢')
    parser_hybrid.add_argument('--limit', type=int, default=10, help='æœ€å¤šé¡¯ç¤ºæ•¸é‡ (é»˜èª: 10)')
    parser_hybrid.add_argument('--provider', choices=['gemini', 'ollama'],
                              default='gemini', help='åµŒå…¥æä¾›è€… (é»˜èª: gemini)')
    parser_hybrid.set_defaults(func=cmd_hybrid_search)

    # auto-link å‘½ä»¤
    parser_auto_link = subparsers.add_parser('auto-link', help='è‡ªå‹•å»ºç«‹è«–æ–‡èˆ‡Zettelkastençš„é€£çµï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰')
    parser_auto_link.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_auto_link.add_argument('--threshold', type=float, default=0.6,
                                 help='ç›¸ä¼¼åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.6)')
    parser_auto_link.add_argument('--max-links', type=int, default=5,
                                 help='æœ€å¤šå»ºç«‹å¹¾å€‹é€£çµ (é»˜èª: 5)')
    parser_auto_link.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°éŒ¯èª¤ä¿¡æ¯')
    parser_auto_link.set_defaults(func=cmd_auto_link)

    # auto-link-all å‘½ä»¤
    parser_auto_link_all = subparsers.add_parser('auto-link-all', help='ç‚ºæ‰€æœ‰è«–æ–‡æ‰¹æ¬¡å»ºç«‹é€£çµ')
    parser_auto_link_all.add_argument('--threshold', type=float, default=0.6,
                                     help='ç›¸ä¼¼åº¦é–¾å€¼ (0-1ï¼Œé»˜èª: 0.6)')
    parser_auto_link_all.add_argument('--max-links', type=int, default=5,
                                     help='æ¯ç¯‡è«–æ–‡æœ€å¤šå»ºç«‹å¹¾å€‹é€£çµ (é»˜èª: 5)')
    parser_auto_link_all.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°é€²åº¦')
    parser_auto_link_all.set_defaults(func=cmd_auto_link_all)

    # show-links å‘½ä»¤
    parser_show_links = subparsers.add_parser('show-links', help='æŸ¥çœ‹è«–æ–‡çš„Zettelkastené€£çµ')
    parser_show_links.add_argument('paper_id', type=int, help='è«–æ–‡ID')
    parser_show_links.add_argument('--min-similarity', type=float, default=0.0,
                                  help='æœ€å°ç›¸ä¼¼åº¦éæ¿¾ (0-1ï¼Œé»˜èª: 0.0)')
    parser_show_links.set_defaults(func=cmd_show_links)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    args.func(args)


if __name__ == "__main__":
    main()
