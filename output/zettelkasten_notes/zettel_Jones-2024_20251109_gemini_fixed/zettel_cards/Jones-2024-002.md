---
title: "Multimodal Language Models (MLLMs)"
summary: |-
  "Multimodal Language Models are deep learning models that process and integrate information from multiple modalities, such as text, images, and audio, to perform various natural language and vision tasks."
---

## èªªæ˜
MLLMs go beyond traditional language models by incorporating visual and auditory information alongside textual data. This allows them to develop a more comprehensive understanding of the world and perform tasks that require cross-modal reasoning.

## é€£çµç¶²çµ¡


**åŸºæ–¼** â†’ [[Jones-2024-001]]


**å°å‘** â†’ [[Jones-2024-004]], [[Jones-2024-006]]


**ç›¸é—œ** â†” [[Jones-2024-010]]



## ä¾†æºè„ˆçµ¡
- ğŸ“„ **æ–‡ç»**: 



## å€‹äººç­†è¨˜


ğŸ¤– **AI**: If MLLMs truly embody simulation, how does the architecture and training data contribute to or hinder their ability to "feel" or "understand" sensory experiences, particularly in comparison with embodiment in biological systems as introduced in [[Jones-2024-001]]?

âœï¸ **Human**:



## å¾…è§£å•é¡Œ
What are the most effective architectures for MLLMs to perform embodied simulation?
