---
title: "Bias in Training Data"
summary: |-
  "Bias in the training data can lead to MLLMs developing skewed or inaccurate representations of embodied experiences."
---

## èªªæ˜
This concept acknowledges the potential for bias in the data used to train MLLMs, which can lead to skewed or unfair outcomes. It emphasizes the importance of carefully curating and evaluating training data to mitigate bias.

## é€£çµç¶²çµ¡




**ç›¸é—œ** â†” [[Jones-2024-013]]



## ä¾†æºè„ˆçµ¡
- ğŸ“„ **æ–‡ç»**: 



## å€‹äººç­†è¨˜


ğŸ¤– **AI**: How can we ensure that training data for MLLMs is representative of the diversity of human experiences and perspectives? What methods can we use to detect and mitigate bias in these datasets, a crucial component given the questions raised in [[Jones-2024-013]]?

âœï¸ **Human**:



## å¾…è§£å•é¡Œ
How can we develop more robust and fair MLLMs that are less susceptible to bias in the training data?
