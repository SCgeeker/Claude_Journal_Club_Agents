---
title: "Cross-Modal Reasoning"
summary: |-
  "Cross-modal reasoning involves the ability to integrate information from different modalities to draw inferences and solve problems that require understanding relationships between these modalities."
---

## èªªæ˜
Cross-modal reasoning is a key capability for MLLMs. It allows them to understand the connections between text, images, audio, and other types of data, leading to more sophisticated and nuanced understanding of the world.

## é€£çµç¶²çµ¡


**åŸºæ–¼** â†’ [[Jones-2024-002]]


**å°å‘** â†’ [[Jones-2024-009]]


**ç›¸é—œ** â†” [[Jones-2024-010]]



## ä¾†æºè„ˆçµ¡
- ğŸ“„ **æ–‡ç»**: 



## å€‹äººç­†è¨˜


ğŸ¤– **AI**: Is cross-modal reasoning sufficient to demonstrate embodied simulation? Or are other factors, such as the ability to generate novel behaviors and predictions, also necessary as suggested in [[Jones-2024-002]]?

âœï¸ **Human**:



## å¾…è§£å•é¡Œ
What are the limitations of current cross-modal reasoning abilities in MLLMs?
