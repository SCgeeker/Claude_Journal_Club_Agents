---
id: CogSci-20251031-019
title: "Training Data Magnitude Advantage"
tags: [[TrainingData], [Scale], [Advantage]]
source: "Günther-2025a" (2025)
paper_id: 
created: 2025-10-31
type: concept
---

# Training Data Magnitude Advantage

> **核心**: "First, they are trained using very large amounts of text, which surpass human language experience by multiple orders of magnitude."

## 說明
LLM 在極大量語料上進行預訓練，使其獲得比人類經驗更為廣泛的語言樣本。這種數量級的差異推動了其在多任務學習與通用語言理解中的強大性能。

## 連結網絡


**基於** → [[CogSci-20251031-001]]





## 來源脈絡
- 📄 **文獻**: Günther-2025a



## 個人筆記


**[AI Agent]**: [AI Agent]：雖然訓練大量文本提升了模型的普遍性，但也可能在某些語料豐富的領域產生過擬合。探索「數據規模與泛化」之間的平衡是重要課題。


**[Human]**: (TODO) <!-- 請在此處添加您的個人思考、批判性評論或延伸想法 -->


## 待解問題
大量訓練是否掩蓋了小語言或多樣性語料的獨特性？如何調整訓練策略以保留語言多樣性而不降低性能？


---
*卡片類型*: concept
