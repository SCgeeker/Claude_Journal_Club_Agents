{
  "file_path": "D:\\core\\Research\\Program_verse\\+\\pdf\\Zeelenberg-2024.pdf",
  "file_name": "Zeelenberg-2024.pdf",
  "full_text": "Journal Pre-proof\nNon-native Language Comprehenders Encode Implied Shapes of Objects in Memory\nRené Zeelenberg, Diane Pecher, Mirthe E.M. van der Meijden, Sean Trott, Benjamin\nBergen\nPII: S0010-9452(24)00255-7\nDOI: https://doi.org/10.1016/j.cortex.2024.09.008\nReference: CORTEX 4001\nTo appear in: Cortex\nReceived Date: 15 March 2024\nRevised Date: 22 August 2024\nAccepted Date: 20 September 2024\nPlease cite this article as: René Z, Pecher D, van der Meijden MEM, Trott S, Bergen B, Non-\nnative Language Comprehenders Encode Implied Shapes of Objects in Memory, CORTEX, https://\ndoi.org/10.1016/j.cortex.2024.09.008.\nThis is a PDF file of an article that has undergone enhancements after acceptance, such as the addition\nof a cover page and metadata, and formatting for readability, but it is not yet the definitive version of\nrecord. This version will undergo additional copyediting, typesetting and review before it is published\nin its final form, but we are providing this version to give early visibility of the article. Please note that,\nduring the production process, errors may be discovered which could affect the content, and all legal\ndisclaimers that apply to the journal pertain.\n© 2024 The Author(s). Published by Elsevier Ltd.\n\n1\nNon-native Language Comprehenders Encode Implied Shapes of Objects in Memory\nRené Zeelenberg, Diane Pecher, and Mirthe E.M. van der Meijden\nf\nErasmus University Rotterdam o\no\nr\np\nSean Trott and Benjamin Bergen\n-\ne\nUniversity of California, San Diego\nr\nP\nl\na\nn\nr\nu\no\nJ\nCorrespondence concerning this article should be addressed to Diane Pecher or René Zeelenberg,\nErasmus University Rotterdam, Woudestein T13-48, PO Box 1738, 3000 DR Rotterdam, The\nNetherlands.\nEmail: pecher@essb.eur.nl or zeelenberg@essb.eur.nl\nDeclarations of interest: none\n\n1\nNon-native Language Comprehenders Encode Implied Shapes of Objects in Memory\nRené Zeelenberg, Diane Pecher, and Mirthe E.M. van der Meijden\nf\nErasmus University Rotterdam o\no\nr\np\nSean Trott and Benjamin Bergen\n-\ne\nUniversity of California, San Diego\nr\nP\nl\na\nn\nr\nu\no\nJ\nCorrespondence concerning this article should be addressed to Diane Pecher or René Zeelenberg,\nErasmus University Rotterdam, Woudestein T13-48, PO Box 1738, 3000 DR Rotterdam, The\nNetherlands.\nEmail: pecher@essb.eur.nl or zeelenberg@essb.eur.nl\nDeclarations of interest: none\n\n2\nAbstract\nBarsalou (1999) proposes that conceptual knowledge is represented by mental simulations\ncontaining perceptual information derived from actual experiences. Although a substantial number\nof studies have provided evidence consistent with this view in native language comprehension, it\nremains unclear whether the non-native language comprehension processes also include mental\nsimulations. The current study successfully replicates the shape match effect in sentence-picture\nverification (Zwaan et al., 2002) for non-native English language comprehenders, indicating native-\nf\nlike visual simulations. In addition, participants displayed better delayoed recognition memory when\no\nthe shape of the depicted objects matched the shape that was implied by the sentence than when it\nr\np\ndid not, suggesting that visual simulations were generated spontaneously in naturalistic non-native\n-\ne\nlanguage comprehension. Additional correlational analyses revealed no relationship between English\nr\nproficiency and the size of the match effect. P\nl\na\nKeywords: mental simulantion, recognition memory, sentence processing, non-native\nr\nlanguage comprehension\nu\no\nJ\n\n3\n1. Non-Native Language Comprehenders Mentally Simulate the Implied Shape of Objects\nMore than half of the people in the world use more than one language (Grosjean, 1982), yet\ncompared with native language1 processing, little is known about the relation between conceptual\nmemory and language comprehension in a non-native language. The use of native and non-native\nlanguages differs in several ways that relate to the form of language including pronunciation,\ngrammatical competence, speech rate, vocabulary size, and others. These differences raise the\nquestion of whether the nature of meaning processing might differ as well. In particular, we ask\nf\nwhether non-native language comprehension activates conceptual moemory in the same way as\no\nnative language comprehension and whether non-native language comprehension leads to similar\nr\np\nmemory traces of the content of language as native language comprehension.\n-\ne\n1.1 Mental Simulation and Language Comprehension\nr\nAccording to a strong version of the Pgrounded cognition view, conceptual memory relies on\nsensorimotor processes to ground concelptual representations (Barsalou, 1999; Glenberg, 1997;\na\nPulvermüller, 1999). This embodnied approach to concept representation proposes that mental\nr\nrepresentations reflect the details and dynamics of actual experiences, which has been corroborated\nu\no\nby both behavioural (e.g., Borghi et al., 2004; Mannaert et al, 2017; Pecher et al., 2003, 2004, 2009;\nJ\nSolomon & Barsalou, 2004; Stanfield & Zwaan, 2001; Zwaan & Pecher, 2012; Zwaan et al., 2002), and\nneural findings (e.g., Pulvermüller, 1999). A widely adopted explanation of how cognition might be\ngrounded in sensorimotor processes is the perceptual symbols theory, in which Barsalou (1999)\nsuggests that, instead of amodal propositional processing, conceptual knowledge is represented\nthrough mental simulations of sensorimotor experiences. The perceptual symbols that make up\nthese sensorimotor simulations are believed to be modal and analogical, as they are assumed to be\nrepresented in the same neural systems that process perceptual and motor information of physical\nexemplars. Barsalou describes these perceptual symbols as records of neural activation patterns that\n1 We use native language to refer to a person’s first language and non-native language for any other language\nthat a person speaks. We prefer to use non-native over L2 because the language that is tested in a study is not\nalways the second (rather than third, fourth, etc.) language that a person learned or speaks.\n\n4\nemerge during perception, rather than physical pictures or mental images. He assumes them to be\ndynamic, meaning that different contexts may activate varying neural activation patterns. For\nexample, the simulation of a watermelon will be different when reading about someone buying one\n(e.g., a green sphere) as opposed to reading about someone eating one (e.g., a red slice) (Wu &\nBarsalou, 2009).\nSeveral paradigms have been developed to further explore the mechanisms of grounded\ncognition. Perhaps one of the most productive and influential ones relating language comprehension\nf\nto mental simulation of concepts was developed by Zwaan et al. (200o2). The sentence-picture\no\nverification task provides evidence that readers construct visual simulations of objects that vary as a\nr\np\nfunction of linguistic context. Participants read sentences that imply a particular shape for an object.\n-\ne\nFor example, the sentence There was an owl in the sky implies that the owl is flying, whereas the\nr\nsentence There was an owl in the barn impliePs that the owl is perched. After reading the sentence,\nparticipants are shown a picture and havle to decide whether the object in the picture was\na\nmentioned in the sentence. On tnrials of interest, the picture shows the object mentioned in the\nr\nsentence (e.g., an owl) that either matches (e.g., outstretched wings for the first sentence) or\nu\no\nmismatches (e.g., folded wings for the first sentence) the implied shape. In the original work and in\nJ\nseveral replications (De Koning et al., 2017; Pecher et al., 2009; Sato et al., 2017; Zwaan & Pecher,\n2012; Zwaan et al., 2018), participants respond faster and more accurately when the shape of the\nobject in the picture matches the shape that was implied by the sentence than when they mismatch.\nThese findings are obtained even though the shape of the object is not explicitly described in the\nsentence; moreover, the task does not require participants to pay attention to the specific shape of\nthe object depicted in the picture. While the necessity of simulations for language comprehension\nhas been debated (Mahon & Caramazza, 2008), the observed match effect suggests that sentence\ncomprehension does trigger the generation of visual simulations corresponding to the sentence\ncontent.. In other words, language and memory interact to form a mental simulation; the content of\n\n5\nthe mental simulation is determined both by the sentence content and by the information retrieved\nfrom conceptual memory.\nStudies using the sentence-picture verification task, and variations thereof, have reported\nsimilar findings for other perceptual properties such as orientation (Pecher et al., 2009; Stanfield &\nZwaan, 2001; Zwaan & Pecher, 2012), color (Mannaert et al., 2017; Zwaan & Pecher, 2012, but see\nConnell, 2007), size (de Koning et al., 2017) and motor actions (Bergen et al., 2003). However, this\nparadigm suffers from the potential criticism that even though successfully responding to stimuli\nf\ndoes not require construction of shape-specific visual simulations, theo task might promote this\no\nprocess simply because sentences are immediately followed by images. The delayed picture\nr\np\nrecognition task is a related paradigm that addresses this concern (Pecher et al., 2009). During the\n-\ne\ninitial study phase of this paradigm, participants read sentences and are asked to indicate whether\nr\nthey make sense or not (i.e., filler sentences Pare nonsensical). In the subsequent surprise test phase,\nparticipants see pictures of objects and inldicate whether the object in the picture was mentioned in\na\none of the sentences presented dnuring the earlier phase of the study. Because the surprise memory\nr\ntest is given only after all sentences have been read, participants do not know while reading\nu\no\nsentences that images will be presented later on, hence any visual details that they generate during\nJ\ncomprehension cannot be the product of this aspect of the design. Studies using this paradigm\nreveal better recognition memory when the perceptual characteristics of the object in the picture\nmatch those implied by the sentence than when they mismatch, providing evidence for spontaneous\nmental simulation during language comprehension (Pecher et al., 2007, 2009).\n1.2 The Role of Mental Simulation in Non-Native Language Comprehension\nIn the studies described above, language comprehenders read sentences in their native (i.e.,\nfirst) language. In the work described here, we will address whether non-native language users\ndisplay the same behavioral indices of sensorimotor processes in conceptual memory during\nlanguage comprehension as native speakers do and, if so, whether this occurs spontaneously during\nnaturalistic language processing or only in tasks that bring attention to these features. Researchers\n\n6\nhave argued that the processes involved in native and non-native language comprehension may be\ndifferent because the languages are learned in different ways (Chen et al., 2020; Günther et al.,\n2018; Norman & Peleg, 2022). The reasoning is that the native language is learned in direct\ninteraction with other people and the physical world. This is assumed to promote the formation of\nlinks between sensorimotor representations and the native language. Non-native languages learned\nlater in life are often acquired in a formal setting such as school rather than in direct interaction with\nthe world. Hence, people may have weaker links between sensorimotor representations and the\nf\nnon-native language (Chen et al., 2020; Foroni, 2015; Kogan et al., 20o20; Norman & Peleg, 2022). As\no\na result, mental simulations may play no role in non-native language comprehension. Thus far, the\nr\np\nhandful of studies addressing this issue have reported mixed results.\n-\ne\nParticipants in a study by Ahn and Jiang (2018) performed a sentence-picture verification\nr\ntask in their non-native, second language (L2P), in this case, Korean. The results showed a shape\nmatch effect for non-native speakers, whlich indicates that non-native comprehenders simulate\na\nsentence meaning. However, then stimuli in this study were not fully counterbalanced, as only one\nr\npicture was paired with each object. Consequently, sentences were confounded with condition,\nu\no\ncomplicating the interpretation of results. Furthermore, using a fully counterbalanced design,\nJ\nNorman and Peleg (2022) found a shape match effect in the immediate sentence-picture verification\ntask when participants read sentences in their native language (Hebrew) but not in when they read\nsentences in their non-native language (English). Chen et al. (2020) tested the delayed shape match\neffect in L1 (Cantonese), L2 (Mandarin), and L3 (English), but found no overall effect of picture match\nin the hit rates nor an interaction with language. When they analyzed the reaction times, however,\nthey observed a match effect in L1 but not in L2 and L3. This finding, together with that of Norman\nand Peleg, suggests that perceptual simulation is restricted to one’s native language. Nonetheless,\nthe lack of an effect in the hit rates even for native speakers is puzzling, since Pecher et al. (2009) did\nfind better recognition memory for matching than for nonmatching pictures. Additionally, the\nanalysis of reaction times poses a particular challenge because they can only be analyzed for hits,\n\n7\ncausing the exclusion rate to be quite high (over 25% of the trials in their study). Thus, these results\nsuggest that non-native speakers do not activate visual simulations during sentence comprehension,\nalbeit with some caveats, and stand in contrast to results from studies using different paradigms that\nhave found evidence for sensory-motor simulations in non-native speakers (Ahlberg et al., 2018;\nBergen et al., 2010; Dudschig et al., 2014; Wheeler & Stojanovic, 2006; see Kogan et al., 2020;\nMonaco et al., 2019 for reviews).\n1.3 Overview of the Present Study\nf\nThe present study examined the match effect using two paraodigms, one of them being the\no\nstandard sentence-picture verification task (Zwaan et al., 2002). Because this match effect has\nr\np\nalready been well established in native speakers we presented the task to non-native speakers only.\n-\ne\nIf non-native language comprehension processing routinely involves mental simulation, participants\nr\nshould show faster and more accurate respoPnses when the shape of the depicted object matches\nthe shape that was implied in the precedling sentence than when it mismatches the shape that was\na\nimplied in the sentence—just as nit does for native comprehension. We tested the match effect in a\nr\nsample of university students who are exposed to English (their non-native language) almost every\nu\no\nday. They follow lectures in English, most of their study materials are in English, and students in the\nJ\ninternational track have discussion meetings in English. Outside their educational activities they\nspeak English informally with fellow students. Given that the speed of semantic activation in non-\nnative language processing may depend on language use (De Bruin et al., 2016) we thus optimized\nthe chances of obtaining match effects in the sentence-picture verification task.\nSecond, we used the delayed picture recognition task (Pecher et al., 2009) to rule out the\nalternative explanation that a possible match effect in the standard sentence-picture verification\ntask depends on strategically employed mental imagery. For the standard sentence-picture\nverification task it is possible that participants use mental imagery as a memory aid, given that the\npictures are presented directly after each sentence. In this case, those results would not be\nrepresentative of naturalistic non-native language comprehension and thus, in the delayed\n\n8\nrecognition task, performance in the match and mismatch conditions would not differ. If, however,\nvisual simulations are generated spontaneously during language comprehension and encoded in\nmemory, picture recognition memory performance should be better for the match than for the\nmismatch condition.\nFinally, we ask whether the degree of competence in a non-native language predicts the\ndegree of native-like sensorimotor activation of conceptual representations during comprehension.\nWe conduct correlational analyses to investigate whether the degree of mental simulation was\nf\nrelated to the level of English language proficiency, as measured by thoe LexTALE task, a validated and\no\nperformance-based test of proficiency (Lemhöfer & Broersma, 2011). De Grauwe (2014) and Vukovic\nr\np\n(2013) have previously suggested an association between second-language proficiency and motor\n-\ne\nsimulation, an idea that has received some empirical support (Ahlberg et al., 2021; Foroni, 2015;\nr\nWheeler & Stojanovic, 2006). The relationshiPp between second-language proficiency and visual\nsimulation, on the other hand, has not relceived empirical support. Norman and Peleg (2022)\na\nmeasured language proficiency inn the non-native language using the LexTALE task, as well as other\nr\nlanguage proficiency measures, and did not find that language proficiency predicted performance in\nu\no\nthe sentence-picture verification task. However, because they did not find a match effect for the\nJ\nnon-native language it is difficult to draw strong conclusions regarding the role of language\nproficiency in visual simulation in non-native language comprehension. Given that multiple studies\nhave reported evidence for a relationship between second-language proficiency and motor\nsimulation, we tested this relationship for visual simulation as well. Specifically, we asked whether\nparticipants who are less proficient in the English language generate visual simulations to a lesser\nextent than highly proficient participants.\nMaterials and Data Availability\nAll materials used in the experiments and all data used in the analyses of Experiment 1 and 2 are\navailable at https://osf.io/dsgc9/.\n2. Experiment 1\n\n9\n2.1 Method\nWe report how we determined our sample size, all data exclusions (if any), all\ninclusion/exclusion criteria, whether inclusion/exclusion criteria were established prior to\ndata analysis, all manipulations, and all measures in the study. No part of the study\nprocedures or analysis plans was preregistered prior to the research being conducted.\n2.1.1 Participants\nA total of 109 non-native speakers of English were recruited for the study. All participants\nwere students at the Erasmus University Rotterdam who participated in exchange for course credits.\nf\no\nThe data of 88 participants were included in the final statistical analyses (information about\no\nexclusion criteria is provided further down). The participants rermaining in the analyses consisted of\np\n73 females (83%), 14 males (16%), and 1 participant who did not identify with either gender (1%).\n-\ne\nTheir ages ranged from 18 to 31 years old (M = 20.4, SD = 2.1).\nr\nP\nStudents in the Psychology program are proficient in English language comprehension.\nl\nErasmus University offers two tracks ain the 3-year bachelor Psychology program: a Dutch language\nn\ntrack and an international (English language) track. In both language tracks, lectures are provided in\nr\nu\nEnglish and students read English textbooks and journal articles. The major difference is that the\no\nsmall group meetings students attend two or three times a week contain literature discussions and\nJ\nexercises in either Dutch or English, depending on the track the student has chosen. Most Dutch\nstudents have 6 years of secondary education throughout which they have taken English classes and,\nin more informal settings, have been exposed to English via movies and television programs on\nregular broadcast (with Dutch subtitles) since childhood. Additionally, English is frequently spoken\nwhen travelling abroad. The educational and cultural background of the international students is\nmore diverse, however they have all self-selected to take part in the English language program at\nErasmus University and needed to show proof of English proficiency prior to admission. This could\nconstitute either a secondary education diploma of which English was part of the final exam, proof\nof having attended secondary school in an English-speaking country (e.g., UK or New Zealand), or\nhaving obtained a TOEFL score ≥ 6.5 or IELTS score ≥ 90.\n\n10\nThe sample included native speakers of 18 different languages: Dutch (n = 47), German (n =\n9), Romanian (n = 4), Turkish (n = 4), Spanish (n = 3), Vietnamese (n = 3), Russian (n = 2), Papiamento\n(n = 2), Bulgarian (n = 2), Italian (n = 2), Arabic (n = 1), Croatian (n = 1), Estonian (n = 1), Finnish (n =\n1), Greek (n = 1), Nepali (n = 1), Norwegian (n = 1), Portuguese (n = 1), and multilingual (n = 2; 1\nGerman/Romanian and 1 Russian/Latvian).\n2.1.2 Stimulus Materials\nFor the sentence-picture verification task, a set of 60 sentence-picture quadruplets was\nf\ngenerated comparable to the materials used by Zwaan et al. (2018) coonsisting of 31 artefacts, 9\no\nanimals, and 20 other natural objects. One quadruplet contained two sentences and two grayscale\nr\np\nphotographs showing the object on a white background (see Table 1 for an example). Each object\n-\ne\npicture matched the shape of the object that was implied by one of the sentences, enabling the\nr\ncreation of two matching and two mismatchPing sentence-picture combinations. Each participant was\npresented only one sentence and one piclture from each quadruplet. Four counterbalanced versions\na\nof the experiment were generatend to enable the use of all sentence-picture combinations. All\nr\ncounterbalanced versions contained 30 matching and 30 mismatching sentence-picture pairs. Across\nu\no\nthe four versions, all items were used equally often in the match and mismatch condition.\nJ\nTable 1\nExample of a Quadruplet\nSentence Picture\nThe owl was in the barn\nThe owl was in the sky\n\n11\nNote. The sentence and picture presented on the same line of the table form matching\ncombinations. Mismatching combinations are formed by switching the sentences.\nA set of 60 additional sentence-picture pairs were used as fillers. The filler sentences were\nsimilar to the experimental sentences in length and position of object nouns, but were followed by\nan unrelated picture (i.e., one not mentioned in the preceding sentence), thus requiring a “no”\nresponse. For half of the filler sentences there was a comprehension question, 15 requiring a “yes”\nresponse and 15 a “no” response (see Table 2 for an example). These comprehension questions\nf\nwere included to ensure that participants processed the meaning of tohe sentences. An additional set\no\nof 10 sentence-picture pairs, 6 including a comprehension question, were used for practice. Filler\nr\np\nand practice stimuli were identical for all participants.\n-\ne\nStimuli used for the LexTALE task were obtained from Lemhöfer and Broersma (2011) and\nr\nconsisted of 40 words (e.g., scornful), 20 nonPwords (e.g., mensible), and 3 practice items (2 words\nand 1 nonword). All words and sentencesl were spelled in British English. The tasks were\na\nadministered online using Inquisnit Web (Millisecond).\nr\nu\no\nTable 2\nJ\nExample of a Filler Trial With Comprehension Question\nSentence Picture Comprehension Question\nThere was a kayak in the river. Was the kayak in the water?\nNote. Comprehension questions were presented on half of the filler trials.\n2.1.3 Procedure\n\n12\nParticipants first completed the LexTALE task (Lemhöfer & Broersma, 2011) to assess their\nlanguage proficiency. The instructions and procedure for the LexTALE task were similar to the online\nversion on www.lextale.com. On each trial an item was presented in the center of the computer\nscreen with two response buttons below the item. The “no” button was red and presented on the\nleft, the “yes” button was green and presented on the right. Participants used the mouse to click on\nthe “yes” button for words and on the “no” button for nonwords. After their response the screen\nwas blank for 500 ms, followed by the next item. The order of items was fixed in the order used by\nf\nLemhöfer and Boersma (2011). o\no\nr\np\nFigure 1\n-\ne\nProcedure of the Sentence-Picture Verification Task\nr\nP\nl\na\nn\nr\nu\no\nJ\nNote. Panel A: Example of an experimental trial sequence. Panel B: Example of a filler trial sequence.\nAfter completing the LexTALE task, participants received instructions stating they would be\npresented sentences and pictures, after which they had to decide whether the depicted object had\nbeen mentioned by the sentence. Participants were informed that some trials included a\ncomprehension question following their response to the picture. Each trial started with a fixation (+),\nvertically centered and left-justified, for 500 ms, immediately followed by the sentence. The\n\n13\nsentence was also left-justified so that the first letter appeared at the same location as the fixation.\nThe sentence remained on the screen until participants pressed the space bar to indicate they had\nread and understood the sentence. Then a fixation (+) was presented in the center of the screen for\n500 ms, immediately followed by the picture. The picture remained on the screen until the\nparticipant responded by pressing the K-key if the depicted object was mentioned in the sentence,\nor the S-key if the depicted object was not mentioned in the sentence. An incorrect response was\nfollowed by feedback (“Incorrect”) for 500 ms. Half of the filler trials were followed by a yes/no\nf\ncomprehension question. Participants also used the K and S keys to aonswer the comprehension\no\nquestion. Each trial was followed by an interval of 1000 ms before the next trial started. Participants\nr\np\nwere instructed to respond as quickly and accurately as possible. The experiment started with 10\n-\ne\npractice trials, followed by 120 experimental trials. The trial procedure was identical for practice and\nr\nexperimental items. Practice items and expePrimental items were presented in a new random order\nfor each participant. l\na\nAfter completing the senntence-picture verification task, participants were asked to provide\nr\ntheir gender, age, and native language. The tasks were administered online using Inquisit Web\nu\no\n(Millisecond).\nJ\n2.1.4 Data Analysis\nThe dependent measures for the sentence-picture verification task were the mean response\ntimes (RT) and proportion of errors. In total, data from 21 participants were removed from the\nanalysis, following exclusion criteria used in prior studies with the sentence-picture verification task\n(Mannaert et al., 2017; Zwaan & Pecher, 2012), because a) the participant responded incorrectly on\nmore than 20% of the pictures (n = 4), b) their mean reaction time was more than 3 SD above the\ngroup mean (n = 1), c) they had already participated earlier (n = 2), or d) they reported to be a native\nspeaker of English (n = 8). To ensure equal numbers of participants in each of the four\ncounterbalanced versions, we excluded the last tested participants (n = 6) in versions that had more\nparticipants than the version with the fewest so that 22 participants remained in each version. For\n\n14\nall participants remaining in the analyses, reaction times that deviated more than 3 SD from the\nparticipant’s mean were treated as outliers. All data from both experiments that were used for the\nanalyses are available from https://osf.io/dsgc9/\nLexTALE scores were obtained by computing d’ values, where hit rates are the proportions of\ncorrect (‘yes’) responses to words and false alarm rates are the proportions of incorrect (‘yes’)\nresponses to nonwords. Based on these hit and false alarm rates we calculated d’ using the\nSnodgrass-Corwin correction (Snodgrass & Corwin, 1988).\nf\n2.2 Results and Discussion o\no\nMean RT and accuracy for the sentence-picture verification task were calculated for each\nr\np\nparticipant and condition. Reaction times that deviated more than 3 SD from the participant’s mean\n-\ne\nwere removed as outliers (2.08 % of the correct reaction times). The mean error rates and trimmed\nr\nreaction times are presented in Table 3. PartPicipants responded faster to match than to mismatch\npictures, t(87) = 2.08, p = .040, Cohen’s dlz = 0.22, and made fewer errors to match than to mismatch\na\npictures, t(87) = 2.12, p = .037, Cnohen’s d = 0.23. Thus, we replicated the match effect.\nz\nr\nBecause studies using the sentence-picture verification task often base statistical analyses\nu\no\non the median RTs for each participant and condition (e.g., Mannaert et al., 2017; Stanfield & Zwaan,\nJ\n2001; Zwaan et al. 2002; Zwaan & Pecher, 2012), we also performed a paired sample t-test on the\nmedian RTs. This analysis also indicated that participants responded faster in the match condition\n(mean median RT = 665 ms) than in the mismatch condition (mean median RT = 689 ms), t(87) =\n2.87, p = .005, Cohen’s d = 0.31.\nz\nTable 3\nMean Error Rates and Reaction Times and SE in the Sentence-Picture Verification Task in\nExperiment 1\nReaction Times Error Rates\nM SE M SE\nMatch 725 17 .05 .01\n\n15\nMismatch 743 18 .07 .01\nTo examine whether there was an association between English language proficiency and\nRTs, correlational analyses were conducted. The average LexTALE d’ score was 1.64 (range 0.37-3.35,\nmean hit rate = 0.78, mean false alarm rate = 0.24). The results did not show a relationship between\nLexTALE scores and the size of the match effect (i.e., the difference in mean RT between match and\nmismatch conditions), r(86) = .11, p = .315. Given the difficulty of interprefting null effects, we also\no\ncalculated Bayes Factors for correlations (JASP Team, 2023). This analysis indicated moderate\no\nr\nevidence for the absence of a correlation between LexTALE and the match effect, BF = 4.59 (Figure\np 0/1\n2A). However, the analysis did reveal a significant negati-ve association between LexTALE scores and\ne\noverall RTs on critical trials, r(86) = -.24, p = .024, rBF = 1.62 (Figure 2B), indicating that participants\n1/0\nP\nwith higher English proficiency levels responded faster than those with lower English proficiency\nl\na\nlevels.2\nn\nr\nu\nFigure 2\no\nScatterplots with RJegression Lines for the Relation between English Language Proficiency (LexTALE\nScore) and the Size of the Match Effect (Panel A) and Overall Reaction Time (Panel B).\n2\nLemhöfer and Broersma (2011) used another index (I ) to compute scores for English language proficiency. We\nSDT\ntherefore also examined the association between LexTALE scores and RTs using I as a measure of language proficiency\nSDT\n(M = .58, range .15-.93). The results also did not show a relationship between LexTALE scores and the size of the match\neffect, r(86) = .14, p = .197, BF = 3.32, and a significant negative association between LexTALE scores and overall RTs on\n0/1\ncritical trials, r(86) = -.27, p = .010, BF = 3.48.\n1/0\n\n16\nNote. The match effect (Panel A) was calculated by taking the difference in mean RT between the\nf\nmatch and mismatch conditions for each participant. The overall reacotion time (Panel B) was\ncalculated by taking the average of the mean RT across all critical trials for each participant.\no\nr\np\nThe finding of a match effect in Experiment 1 extends previously reported findings in the\n-\nsentence-picture verification task with native languaege comprehenders (e.g., Zwaan et al., 2002) to\nr\nnon-native speakers, providing support for thPe view that non-native language comprehenders\ngenerate mental simulations in a native-like manner. Thus, in contrast to Norman and Peleg (2022)\nl\na\nwe did find evidence for mental simulations in non-native language comprehension. Experiment 2\nn\nr\nwas conducted to see if converging evidence for this claim would be found with a different\nu\nparadigm. As mentionoed earlier, in the sentence-picture verification task, sentences and pictures are\nJ\npresented in an alternating fashion, which could evoke strategic use of imagery to facilitate\nresponding to the presented pictures. In this case, the match effect would not reflect naturalistic\nlanguage processing. To rule out this alternative explanation of the match effect, a delayed picture\nrecognition task (Pecher et al., 2009) was used in Experiment 2. As argued by Pecher et al. (2009),\nthis task provides the opportunity to examine whether mental simulations are generated\nspontaneously during language comprehension. In the delayed picture recognition task, participants\nreceive a surprise memory task after having read all sentences. Because participants are unaware\nthat their memory will be tested and do not see any of the object pictures before having read all the\nsentences, there is no reason to engage in strategic imagery during sentence reading. Crucially, we\nassumed that activation of the meaning of a sentence would result in the formation of a memory\n\n17\ntrace of the activated concepts, and that this trace would reflect the visual simulation of the object’s\nshape as implied by the sentence context. Earlier findings of context sensitivity (e.g., Anderson et al.,\n1976; Barclay et al., 1974; Barsalou, 1993; Cabeza, 1994; Mulligan et al., 1999; Pecher et al., 2004,\n2007; Van Dantzig et al., 2011; Vriezen et al., 1995; Yee & Thompson-Schill, 2016; Zeelenberg, 2005;\nZeelenberg et al., 2003) lead to the hypothesis that a match effect might occur in such a task (i.e.,\nbetter recognition memory performance for matching pictures than for mismatching pictures). If so,\nthis would provide further evidence for the claim that non-native speakers form mental simulations\nf\nduring language comprehension, and that they do so even under moroe naturalistic comprehension\no\ncircumstances.\nr\np\n-\ne\n3. Experiment 2\nr\n3.1 Method P\n3.1.1 Participants l\na\nA total of 109 non-nativen speakers of English were recruited for the study. None of the\nr\nparticipants in the reported analyses had participated in Experiment 1. The data from 92 participants\nu\no\nwere included in the final statistical analyses (information about exclusion criteria is provided\nJ\nbelow). All participants were students at the Erasmus University Rotterdam who participated in\nexchange for course credit. Participants remaining in the analyses consisted of 83 females (90%), 8\nmales (9%), and 1 participant who identified with neither gender (1%). Ages ranged from 18 to 44\nyears old (M = 20.9, SD = 3.2).\nThe sample Included speakers of 22 different native languages: Dutch (n = 48), German (n =\n10), Turkish (n = 4), Spanish (n = 3), Danish (n = 2), Georgian (n = 2), Italian (n = 2), Albanian (n = 1),\nArabic (n = 1), Bosnian (n = 1), Bulgarian (n = 1), Finnish (n = 1), French (n = 1), Greek (n = 1), Hebrew\n(n = 1), Hindi (n = 1), Hungarian (n = 1), Romanian (n = 1), Russian (n = 1), Serbian (n = 1), Swedish (n\n= 1), Ukrainian (n = 1), and multilingual (n = 6; 2 Dutch/Turkish, 1 Bosnian/Croatian/Dutch, 1\nDutch/Finnish, 1 German/Swedish, and 1 Urdu/Punjabi).\n\n18\n3.1.2 Stimulus Materials\nThe same sentence quadruplets and filler pictures as in Experiment 1 were used for the\ndelayed picture recognition task. Sixty nonsense sentences were created to serve as distractors\nduring the study phase. They were similar to the experimental sentences in length and structure, but\nwere semantically nonsensical, thus requiring a “no” response (e.g., There was a hill in the glove).\nThese nonsense sentences mentioned only objects that were not included in the quadruplets nor\nthe filler pictures. A set of 60 filler pictures was added to serve as distractors for the recognition\nf\ntask. The stimuli for the LexTALE task were the same as those used ino Experiment 1.\no\n3.1.3 Procedure\nr\np\nThe procedure for the LexTALE task was identical to that of Experiment 1. After completion\n-\ne\nof the LexTALE task, participants completed the study and test phase of the delayed picture\nr\nrecognition task (Pecher et al., 2009). The stuPdy phase included 30 match sentences, 30 mismatch\nsentences, and 60 nonsense filler sentenlces for a total of 120 trials. On each trial, a fixation stimulus\na\n(+) was presented in the centre onf the screen for 500 ms, followed by the presentation of a sentence\nr\nat the centre of the screen until a response was given. Participants indicated whether the sentence\nu\no\nmade sense by clicking on a red no-button or a green yes-button. The next trial started 400 ms after\nJ\nthe response.\nAfter completion of the study phase, the delayed recognition phase was initiated. This phase\ncontained 30 match pictures and 30 mismatch pictures, corresponding to the match and mismatch\nsentences from the study phase, as well as 60 non-studied pictures for a total of 120 trials.\nParticipants were instructed that they would see pictures of objects and their task was to indicate\nfor each picture if the depicted object had been mentioned in one of the sentences in the sensibility\njudgment task. A fixation stimulus (+) was presented in the centre of the screen for 500 ms, followed\nby the presentation of a picture until a response was given. Participants indicated whether the\ndepicted object was mentioned in any of the sentences presented during the study phase by clicking\non a red no-button or a green yes-button. The next trial started 400 ms after the response.\n\n19\nDuring both the study and test phase, participants could take as long as they wanted to\nprovide a response. Stimuli were presented in a new random order for each participant during each\nphase. After completion of the recognition task, participants were asked to provide their gender,\nage, and native language. The tasks were administered online using Inquisit Web (Millisecond).\n3.1.4 Data Analysis\nIn total, data from 17 participants were removed from the analysis because of the following\nexclusion criteria, a) the participant’s overall d’ was below 0 (n = 4), b) they had already participated\nf\nin the experiment (n = 2), or c) they reported to be a native speaker oof English (n = 8). To ensure\no\nequal numbers of participants in each of the four counterbalanced versions, we excluded data from\nr\np\nthe last tested participants (n = 3) in versions that had more participants than the version with the\n-\ne\nfewest so that 23 participants remained in each version.\nr\n3.2 Results and Discussion P\nBased on the hit and false alarm lrates in the recognition task, we calculated d’ for match and\na\nmismatch pictures using the Snondgrass-Corwin correction (Snodgrass & Corwin, 1988). The mean d’,\nr\nhit rates, and false alarm rates are presented in Table 4. Participants’ d’s were higher for match than\nu\no\nmismatch pictures, t(91) = 5.47, p < .001, Cohen’s d = .57. Thus, we replicated the match effect.\nz\nJ\nThat is, participants had a better ability to discriminate between old (i.e., studied) and new (i.e., non-\nstudied) stimuli when the shape of the depicted object matched the shape that was implied by the\nsentence.\nTable 4\nMean d’, Hit Rates, and False Alarm Rates in the Picture Recognition Task in Experiment 2\nd’ Hit Rate False Alarm Rate\nM SE M SE M SE\nMatch 1.34 .05 .66 .02\n\n20\nMismatch 1.16 .06 .60 .02\nDistractors .21 .01\nNote. All hit rates, false alarm rates, and d’ values reported in the paper and used in the statistical\nanalyses were based on values obtained after applying the Snodgrass-Corwin correction.\nThe average LexTALE d’ score was 1.81 (range 0.42-4.23, mean hit rate = 0.77, mean false\nalarm rate = 0.18). Correlational analyses were conducted to examine whether there was an\nassociation between English language proficiency and memory performance. The analysis revealed\nf\nno significant correlation between LexTALE scores and match effects o(i.e., the difference in d’\no\nbetween match and mismatch conditions), r(90) = -.08, p = .478, BF = 5.97 (Figure 3A) nor between\n0/1\nr\np\nLexTALE scores and overall d’s, r(90) = .16, p = .128, BF = 2.48 (Figure 3B), indicating no relation\n0/1\n-\ne\nbetween English proficiency levels and memory performance.3\nr\nThese findings agree with the resultsP for native language comprehenders reported by Pecher\net al. (2009) and suggest that mental simlulations were generated spontaneously during sentence\na\ncomprehension. Together, the rensults of Experiments 1 and 2 can be interpreted as support for the\nr\nview that mental simulatiouns are formed in conceptual memory during normal non-native language\no\ncomprehension.\nJ\nFigure 3\nScatterplots with Regression Lines for the Relation between English Language Proficiency (LexTALE\nScore) and the Size of the Match Effect (Panel A) and Overall Recognition Performance (Panel B).\n3\nCorrelational analyses were also conducted using I to compute LexTALE scores (M = .60, range .18-1.00). This revealed\nSDT\nno significant correlation between LexTALE scores and match effects, r(90) = -.10, p = .360, BF = 5.08. There was,\n0/1\nhowever, a marginally significant positive correlation between LexTALE scores and overall d’s, r(90) = .20, p = .054, BF =\n1/0\n0.81, indicating better overall recognition memory for participants with higher English proficiency levels.\n\n21\nf\nNote. The match effect (Panel A) was calculated by taking the differenoce in d’ between the match\nand mismatch conditions for each participant. Overall d’ (Panel B) was calculated by taking the\no\naverage d’ across matching and mismatching conditions for each participant.\nr\np\n4. General Discu-ssion\ne\nThe current study provides evidence for trhe spontaneous activation of mental simulations in\nP\nnon-native language comprehension. In Experiment 1, using the sentence-picture verification task\nl\n(Zwaan et al., 2002), we found a matach effect. That is, participants were faster to verify that a\nn\npictured object was mentioned in the immediately preceding sentence when the shape of the object\nr\nu\nmatched the shape implied by the sentence than when the shape of the object mismatched the\no\nshape implied by tJhe sentence. In Experiment 2, similar findings were obtained in a delayed picture\nrecognition task (Pecher et al., 2009). In this task, participants first read all the sentences and were\nsubsequently tested in a surprise recognition task. Recognition memory was better if the shape of\nthe picture matched the shape implied by the sentence than if the shape of the object mismatched\nthe shape implied by the sentence. The findings extend those of Experiment 1 indicating again that\nparticipants generated visual simulations during language comprehension. Because in Experiment 2\nsentences and pictures were not presented in an alternating fashion and participants did not know\nthat their memory would be tested, these results provide additional evidence that the match effect\nrelies on mental simulations that are generated spontaneously during naturalistic language\ncomprehension rather than only as a product of strategically deployed imagery (Pecher et al., 2009).\nTogether, the results of Experiments 1 and 2 indicate that behavioral indices of mental simulations\n\n22\nfor non-native language comprehension are similar to those that have been previously observed for\nnative language comprehension.\nAdditionally, we examined whether the size of the match effects in Experiments 1 and 2\nwere related to language proficiency yet found no evidence for such an association. Although the\nresults of Experiment 1 indicated that overall RTs were related to English proficiency levels, the\nstrength of this relation was weak. Moreover, the association between the size of the match effect\nand English proficiency was absent in both experiments. That is, the differences between RTs\nf\n(Experiment 1) and d’ values (Experiment 2) for the match and mismaotch conditions were not\no\nrelated to English proficiency scores.\nr\np\nThe results from the sentence-picture verification task and delayed recognition task indicate\n-\ne\nthat non-native language comprehenders, just as native language comprehenders, generated visual\nr\nsimulations that include information about tPhe shape of objects mentioned in the sentences, even\nthough object shape was not explicitly delscribed. These findings align with those reported for native\na\nlanguage comprehenders by Zwanan et al. (2002; also see Mannaert et al, 2017; Stanfield & Zwaan,\nr\n2001; Zwaan & Pecher, 2012) and Pecher et al. (2009), and can be accounted for by the perceptual\nu\no\nsymbols theory of conceptual memory (Barsalou, 1999). According to this theory, language\nJ\ncomprehenders simulate an actual perceptual encounter with the object that is described, and\ndifferent sentence contexts activate different simulations of the same object. This visual simulation\nof the sentence content is then used for comparison with the presented object picture, hence when\nthe shape of the object in the sentence matches the shape of the depicted object, participants\nrespond faster than when they mismatch. In addition, the results for the delayed picture recognition\ntask revealed that the visual simulation is encoded in memory.\nIn both Experiment 1 and Experiment 2 we did not find a correlation between the size of the\nmatch effect and language proficiency (as measured by LexTALE). The lack of an association suggests\nthat amount of language exposure and degree of mastery of non-native language comprehenders\nmay not affect the extent of visual simulation during language comprehension. However, this\n\n23\nobservation must be interpreted with some caution. First, like any measure, LexTALE is unlikely to\nfully capture all aspects of language proficiency and may therefore be limited in its ability to predict\nthe size of match effects. We note, however, that LexTALE scores are reliable and do predict\nperformance in a variety of language tasks (Lemhöfer & Broersma, 2011). Nevertheless, it might be\nfruitful for future studies to use other language proficiency measures in addition to LexTALE. Second,\nHedge et al. (2017) argued that experimental tasks are often not well suited to detect individual\ndifferences. The fact that these tasks are developed to produce robust effects (i.e., have a relatively\nf\nlow between-participant variability), suggests that their reliability foro detecting individual differences\no\nis not likely to be very high. As the observable correlation is limited by the reliability of the included\nr\np\nmeasures (Spearman, 1904), it is also possible that the lack of correlations between the match\n-\ne\neffects and LexTALE scores underestimate the true correlations because of low reliability of the\nr\nexperimental task.4 It is, however, uncertain Pwhether this is the case, as the reliability of the tasks for\nthe detection of individual differences is lunknown. As can be seen in Figure 2B and Figure 3B, the\na\nmatch effect size did vary over pnarticipants, however it is unclear to what extent this variation is due\nr\nto actual individual differences in mental simulation and to what extent it is due to measurement\nu\no\nerror. A comparison of the effect size found here in Experiment 1 (d = 0.22 for mean RTs, d = 0.31\nz z\nJ\nfor median RTs) with those found in previously published studies using native language\ncomprehenders provides some additional insight in the potential role of language proficiency. Zwaan\nand Pecher (2019), using a different stimulus set, reported effect sizes of d = 0.15 (Experiment 2a)\nz\nand d = 0.18 (Experiment 2b) for shape match effects in the standard picture-sentence verification\nz\ntask. In another study, using the same stimulus set as the present study, Zwaan et al. (2018)\nreported effect sizes of d = 0.22 and d = 0.33 for shape match effects.5 These effect sizes are\nz z\n4 It might seem odd that a task that has a low reliability can produce robust effects but consider the following. Suppose\nthat every person in the population has a ‘true’ Stroop effect of 100 ms. Given that an experiment contains a large number\nof experimental trials the influence of random noise would be low and all participants would show a Stroop effect in the\norder of 100 ms. Obviously, this would produce a robust Stroop effect. In terms of individual differences, however, the\nreliability would be extremely low (i.e., 0).\n5 Zwaan et al. (2018) retested participants three to four days later in the same experiment. In this second wave they\nobserved effect sizes of d = 0.66 and d = 0.31.\nz z\n\n24\nroughly the same size as the one we obtained in Experiment 1. Overall, this lends some support to\nthe idea that language proficiency may have little effect on mental simulation (with the caveat, of\ncourse, that the language comprehender must know the meanings of the words presented).\nAlthough several researchers have obtained evidence for the use of sensory-motor\nsimulations during non-native language comprehension (Ahlberg et al., 2018; Bergen et al., 2010;\nDudschig et al., 2014; Wheeler & Stojanovic, 2006), the results in the sentence-picture verification\ntask have been mixed. Ahn and Jiang (2018) did find a match effect, but with stimulus materials that\nf\nwere not counterbalanced. Chen et al. (2020) and Normand and Peleog (2022), in contrast, did not\no\nfind a match effect. We note that Chen et al. used a delayed recognition task (as we did in\nr\np\nExperiment 2) and had a relatively small sample size of 36 participants. Even for native speakers they\n-\ne\nfailed to find an effect on hit rates, in contrast to Pecher et al. (2009). It is possible that the low\nr\npower to detect a statistically significant matPch effect in the Chen et al. study was responsible for\nthe null finding. Norman and Peleg’s studly had a larger sample size of 80 participants, but they\na\nnevertheless did not find a matchn effect in the standard version of the sentence-picture verification\nr\ntask. Although one cannot exclude the possibility that the lack of a significant match effect for non-\nu\no\nnative speakers in their study was due to a type-2 error, other explanations are also possible. One\nJ\nexplanation might lie in the fact that the participant samples in our study and that of Norman and\nPeleg (as well as some other studies) differed in how regularly the participants use English (De Bruin\n& Della Sala, 2016). Our participants use their ",
  "char_count": 50000,
  "truncated": true,
  "structure": {
    "title": "Journal Pre-proof",
    "authors": [
      "Journal Pre",
      "Sean Trott",
      "Encode Implied",
      "Language Comprehenders",
      "Diane Pecher",
      "Revised Date",
      "Accepted Date",
      "Received Date"
    ],
    "abstract": "Barsalou (1999) proposes that conceptual knowledge is represented by mental simulations\ncontaining perceptual information derived from actual experiences. Although a substantial number\nof studies have provided evidence consistent with this view in native language comprehension, it\nremains unclear whether the non-native language comprehension processes also include mental\nsimulations. The current study successfully replicates the shape match effect in sentence-picture\nverification (Zwaan et al., 2002) for non-native English language comprehenders, indicating native-\nf\nlike visual simulations. In addition, participants displayed better delayoed recognition memory when\no\nthe shape of the depicted objects matched the shape that was implied by the sentence than when it\nr\np\ndid not, suggesting that visual simulations were generated spontaneously in naturalistic non-native\n-\ne\nlanguage comprehension. Additional correlational analyses revealed no relationship between English\nr\nproficiency and the size of the match effect. P\nl\na\nKeywords: mental simulantion, recognition memory, sentence processing, non-native\nr\nlanguage comprehension\nu\no\nJ\n\n3",
    "sections": [
      {
        "title": "1\nNon-native Language Comprehenders Encode Implied Shapes of Objects in Memory",
        "position": 1240
      },
      {
        "title": "1\nNon-native Language Comprehenders Encode Implied Shapes of Objects in Memory",
        "position": 1787
      },
      {
        "title": "2\nAbstract",
        "position": 2334
      },
      {
        "title": "1. Non-Native Language Comprehenders Mentally Simulate the Implied Shape of Objects",
        "position": 3497
      },
      {
        "title": "4\nemerge during perception, rather than physical pictures or mental images. He assumes them to be",
        "position": 6050
      },
      {
        "title": "5\nthe mental simulation is determined both by the sentence content and by the information retrieved",
        "position": 8427
      },
      {
        "title": "6\nhave argued that the processes involved in native and non-native language comprehension may be",
        "position": 10868
      },
      {
        "title": "8\nrecognition task, performance in the match and mismatch conditions would not differ. If, however,",
        "position": 15821
      },
      {
        "title": "2. Experiment 1",
        "position": 17994
      },
      {
        "title": "3. Experiment 2",
        "position": 33649
      },
      {
        "title": "4. General Discu-ssion",
        "position": 41250
      }
    ],
    "keywords": [
      "mental simulantion",
      "recognition memory",
      "sentence processing",
      "non-native"
    ],
    "references_found": false
  },
  "extraction_method": "pdfplumber"
}