{
  "file_path": "D:\\core\\Research\\Program_verse\\+\\pdf\\Ostarek-2019a.pdf",
  "file_name": "Ostarek-2019a.pdf",
  "full_text": "Cognition 182 (2019) 84–94\nContentslistsavailableatScienceDirect\nCognition\njournal homepage: www.elsevier.com/locate/cognit\nOriginal Articles\n“ ” ff\nAre visual processes causally involved in perceptual simulation e ects in\nfi T\nthe sentence-picture veri cation task?\nMarkus Ostareka,b,⁎ , Dennis Joosena, Adil Ishagc, Monique de Nijsa, Falk Huettiga\naMaxPlanckInstituteforPsycholinguistics,Nijmegen,TheNetherlands\nbInternationalMaxPlanckResearchSchoolforLanguageSciences,TheNetherlands\ncInternationalUniversityofAfrica,Khartoum,Sudan\nARTICLE INFO ABSTRACT\nKeywords: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction\nLanguagecomprehension timeadvantageforshape-matchingpicturesinthesentence-pictureverificationtask.Typically,thisfindinghas\nConceptualprocessing beeninterpretedasevidenceforperceptualsimulation,i.e.,thataccesstoimplicitshapeinformationinvolves\nPerceptualsimulation theactivationofmodality-specificvisualprocesses.Itfollowsfromthisproposalthatdisruptingvisualproces-\nEmbodiedcognition singduringsentencecomprehensionshouldinterferewithperceptualsimulationandobliteratethematcheffect.\nSentence-pictureverification\nHerewedirectlytestthishypothesis.Participantslistenedtosentenceswhileseeingeithervisualnoisethatwas\npreviously shown to strongly interfere with basic visual processing or a blank screen. Experiments 1 and 2\nreplicatedthematcheffectbutcruciallyvisualnoisedidnotmodulateit.Whenaninterferencetechniquewas\nusedthattargetedhigh-levelsemanticprocessing(Experiment3)howeverthematcheffectvanished.Visual\nnoisespecificallytargetinghigh-levelvisualprocesses(Experiment4)onlyhadaminimaleffectonthematch\neffect.Weconcludethattheshapematcheffectinthesentence-pictureverificationparadigmisunlikelytorely\nonperceptualsimulation.\n1. Introduction Herrnberger,&Kiefer,2008;Ostarek&Huettig,2017a;vanDam,van\nDijk,Bekkering,&Rueschemeyer,2012;Yee&Thompson-Schill,2016)\nIn theoretical and empirical efforts to understand conceptual pro- by conceiving of conceptual processing as a form of ad hoc sampling\ncessing during language comprehension recent work has focused on fromafeaturespacethatisconstrainedbybothlong-termmemoryand\ntwomainproblems.Thefirstisconcernedwithanaccuratedescription immediatecontext.\noftheinformationalcontentthatisactivatedasweprocesslanguage, Recent behavioural and neuroimaging studies have begun to un-\nwhereas theseconddeals withthenatureoftheneuralandcognitive ravel the underlying mechanisms and started painting a multifaceted\nmechanisms that are used to provide this information. Even though picture of a widely distributed system that includes modality-specific\nboth are closely related, it is crucial to address both separately processes (Fernandinoet al.,2016; Hauk, Johnsrude, &Pulvermüller,\n(Barsalou, 1999, 2016; Binder, 2016; Borghesani & Piazza, 2017; 2004; Lewis & Poeppel, 2014; Ostarek & Huettig, 2017b, 2017a;\nMahon&Caramazza,2008;Mahon,2015). Vukovic, Feurra, Shpektor, Myachykov, & Shtyrov, 2017), different\nRegarding conceptual content, an overwhelming body of evidence stagesofconvergencepossiblyculminatinginamodality-independent\nsuggeststhatlanguageprocessinginvolvesthecontextualizedretrieval centralhub(Bruffaertsetal.,2013;Fernandinoetal.,2016;Patterson,\nof a multitude of conceptual features that, together, constitute their Nestor, & Rogers, 2007; Ralph, Jefferies, Patterson, & Rogers, 2017),\nmeanings (Andersonetal.,2016;Binder&Desai,2011;Binderetal., andflexibleretrievalmechanisms(Kan&Thompson-Schill,2004).\n2016;Collins&Loftus,1975;Cree&McRae,2003;Fernandinoetal., Thepresentstudyfocusesononeparticularsemanticfeature;object\n2016; Fernandino, Humphries, Conant, Seidenberg, & Binder, 2016; shape.Visualworldeye-trackingstudiesindicatethatprocessingnouns\nHuettig & McQueen, 2007; Vigliocco, Meteyard, Andrews, & Kousta, referring to concrete objects activates information about their typical\n2009;Vigliocco,Vinson,Lewis,&Garrett,2004).Thisviewistheore- shapes(Dahan&Tanenhaus,2005;Huettig&Altmann,2007).Asmany\ntically appealing because it nicely accounts for the high degree of objects can occur in multiple different shapes, listeners often need to\nconceptual flexibility (Barsalou, 1993; Hoenig, Sim, Bochev, incorporatecontextualinformationinordertoretrievetheappropriate\n⁎Correspondingauthorat:MPIPsycholinguistics,Wundtlaan1,Nijmegen,TheNetherlands.\nE-mailaddress:markus.ostarek@mpi.nl(M.Ostarek).\nhttps://doi.org/10.1016/j.cognition.2018.08.017\nReceived7September2017;Receivedinrevisedform27August2018;Accepted27August2018\n0010-0277/ © 2018 Elsevier B.V. All rights reserved.\n\nM.Ostareketal. Cognition 182 (2019) 84–94\nshape representations. Using the sentence-picture verification task, a were listening to sentences to directly probe the functional role of\nclassic experiment by Zwaan, Stanfield, and Yaxley (2002) provided perceptualsimulationinthesentence-pictureverificationtask.\nevidence that contextually appropriate shape information is readily\nactivated during sentence comprehension. In that paradigm, partici- 2. Experiment1\npantsreadorlistentosentencesaboutobjectsthatareimpliedtohavea\ncertain shape (e.g., The ranger saw the eagle in the sky; implying out- The basic rationale for this experiment was that interfering with\nstretchedwings).Shortlyaftersentenceoffset,inthecriticalconditions basic visual processing while participants were listening to sentences\napictureappearsofthementionedobjecteitherinmatching(e.g.,an shouldsignificantlyreducetheusuallyobservedshape-matcheffectifit\neagle with outstretched wings) or mismatching shape (an eagle with relies on perceptual simulation. Conversely, if the match effect is in-\nclosedwings).Participantsthenhavetoindicateasquicklyandaccu- dependentofvisualsimulation,visualinterferenceshouldnothavean\nratelyaspossiblewhethertheobjectwasmentionedinthesentenceor impact on the match advantage. Experiment 1 used the same kind of\nnot by pressing one of two buttons. The critical finding (Zwaan & visualinterferencethatwasrecentlyshowntoimpairaccesstovisual\nPecher, 2012; Zwaanet al.,2002) isshorter response latencies inthe information during semantic processing (Edmiston & Lupyan, 2017;\nmatchingcondition,suggestingthatthesentencesactivateinformation Ostarek & Huettig, 2017a), consisting of dynamically changing Mon-\naboutobjectshapethatisspecificenoughtoproduceaprimingeffect drian-typemasksthatareusuallyusedforcontinuousflashsuppression\non the verification judgement. Although there has been some debate and are designed to maximally interfere with basic visual processing\naboutthereplicabilityofcongruencyeffectsofthistype(Papesh,2015; (Tsuchiya&Koch,2005).Wepredictedthatvisualinterferencewould\nRommers, Meyer, & Huettig, 2013) and about reproducibility more decrease the match advantage based on four considerations: (1) the\ngenerally(Pashler&Wagenmakers,2012;Wagenmakersetal.,2016), match effect pertains to visual shape information, (2) processing of\ntheshapematchadvantage,atleastinthesentence-pictureverification shapeinformationinearlyvisualcortexhas beenshowntobemodu-\nparadigm, has proven to be very robust and reproducible (Engelen, latedinthesentence-pictureverificationtask(Hirschfeldetal.,2011),\nBouwmeester,deBruin,&Zwaan,2011;Rommersetal.,2013;Zwaan (3) previous studies reported interference effects of visual noise on\n&Pecher,2012). semanticprocessingofsinglewords(Edmiston&Lupyan,2017;Ostarek\nPreviousstudieshaveimplicitlyorexplicitlygonefurtherandsug- & Huettig, 2017a), and (4) the intuitive proposal that contextually\ngested that the reaction time advantage in the match condition in- embeddedlanguagetendstoengagemorespecificrepresentationsand\ndicatesthekindofprocessthatprovidesshapeinformation,namelythe mightthusbemorelikelytoactivatemodality-specificprocessesthan\nprocess of perceptual simulation (Engelen et al., 2011; Pecher, van singlewords(Kurby&Zacks,2013;Zwaan,2014).\nDantzig,Zwaan,&Zeelenberg,2009;Yaxley&Zwaan,2007;Zwaan&\nPecher,2012;Zwaanetal.,2002).Accordingtothataccount,accessing 2.1. Method\nconceptual shapeinformation (e.g., abouta flyingeagle) involvesthe\napproximatere-instatementofsensoryprocessesthatareactiveduring 2.1.1. Participants\nvisualperceptionofrelevantobjects(e.g.,ofaflyingeagle). Werecruited115healthyparticipantswithnormalorcorrected-to-\nHowever,onedoesnotneedtoinvokesimulationinordertoexplain normalvisionandnormalhearingfromthelocalMPIsubjectdatabase.\nthebehaviouralpattern,asstudiesusingthesentence-pictureverifica- Four had to be excluded due to technical failure, and one due to ex-\ntionparadigmcanonlytellussomethingaboutthekindofinformation cessive error rates (>20%), resulting in 110 participants that were\nthat is accessed, but not about the kinds of processes andrepresenta- used for analysis. We opted for a higher number of participants com-\ntions involved. One way to get at the latter question is to study the paredtopreviousstudiesusingthisparadigmbasedonthefactthatour\nneuralcorrelatesoftheshapematcheffect.Hirschfeld,Zwitserlood,and designincludedtheadditionalfactorofVisualCondition(visualnoise\nDobel (2011) conducted a magnetoencephalography study using the vs. blank screen) and the conviction that high-powered studies are\nsentence-picture verification paradigmto assesschanges inneuralac- needed in the field of experimental psychology (Pashler &\ntivity for shape matching vs. mismatching pictures. They observed a Wagenmakers,2012).Participantsreceivedapaymentof6euros.The\nstronger positivity to pictures following shape matching vs. mis- study was covered by ethics approval from Radboud University Nij-\nmatchingsentencesinoccipitalcortexatca.120msafterpictureonset megen.\n(M1),suggestingatop-downmodulationofearlyvisualprocessingasa\nfunctionofshapematchvs.mismatch.However,changesinthewaythe 2.1.2. Materials,set-up,anddesign\ntargetpicturewasvisuallyprocesseddonotnecessarilyimplythatvi- WeusedthematerialsfromtheoriginalZwaanetal.(2002)study\nsualprocesseswereactivatedduringcomprehension.Indeed,thatsce- that were provided by Rommers, Meyer, Praamstra, and Huettig\nnario would predict repetition suppression, not enhancement. There- (2013). They included 40 quadruplets of pairs of sentences implying\nfore,thedataareconsistentwithwithtop-downinputfromhigher-level shape A or shape B and corresponding pairs of pictures of the men-\ncortical areas. Thus, this approach still cannot answer whether visual tionedobjectsinshapeAorshapeB,andtherewere40fillersentences\nprocesseswereinvolvedinsentence comprehension,as,similartoRT pairedwithtargetpicturesthatarenotmentionedinthesentence.In\nparadigms,whatismeasuredistheeffectofthecomprehensionprocess theoriginaldesign,everyparticipantsawoneoffoursentence-picture\nonpictureverificationthathappensonlyaftersentencecomprehension combinations,resultinginfourlists.Inthepresentstudy,theadditional\nisaccomplished(Mahon&Caramazza,2008). factor of Visual Condition (visual noise vs. blank screen) was added\nOne direct way of testing the hypothesis that visual processes are suchthateverysentence-picturepairwasstillonlyshownoncetoeach\nfunctionallyinvolvedinvisualinformationretrievalistointerferewith participant,butacrossparticipantseverypairoccurredequallyoftenin\nvisual processing during language comprehension and assess whether thevisualnoiseandblankscreencondition,resultingineightlists.\nvisual information retrieval is impaired. Recent studies have demon- Participants were seated 60cm from the screen and placed their\nstratedthatdynamiclow-levelvisualnoisepatternscanselectivelyin- headonachinrest.Presentation(NeurobehavioralSystems)wasused\nterfere with the retrieval of visual information during auditory single tocontrolthedisplayoftargetpicturesandvisualnoiseaswellasthe\nword processing (Ostarek & Huettig, 2017a) and in a property ver- sentences that were played back on headphones. Auditory sentences\nification task (Edmiston & Lupyan, 2017), and they can strongly di- wereusedinsteadofwrittensentencestobeabletointerferewithvisual\nminish the effectiveness of a word cue on a subsequent picture dis- processingduringsentencecomprehension.Thetaskwastolistentothe\ncriminationtask(Edmiston&Lupyan,2017).Here,weusedthevisual sentences and to decide as quickly and accurately as possible by\nnoise technique to interfere with visual processing while participants pressing one of two buttons (left/right on a house-built button box,\n85\n\nM.Ostareketal. Cognition 182 (2019) 84–94\nFig.1.Illustrationofthedesignandtrialstructurewithoneofthesentencesinthevisualnoisecondition.\ncounterbalanced across participants) whether the subsequently dis- 2.2. Resultsanddiscussion\nplayed picture represented an object that was mentioned in the sen-\ntenceornot. The results showed a significant main effect of Visual Condition\nEverytrial(seeFig.1)startedwithafixationcrossatthecentreof (estimate=−0.034, SE=0.014, t=−2.48, p=0.016) with slower\nthe screen (500ms) followed by an auditory sentence (ca. 2s on responses in visual noise trials (M=758ms, SD=252ms) compared\naverage). Sentences were accompanied by visual noise in half of the toblankscreentrials(M=741ms,SD=251ms).Wealsoobtaineda\ntrials. It consisted of 80 masks that were all generated by randomly main effect of Match Condition (estimate=−0.071, SE=0.030,\nsuperimposing1000rectanglesofdifferentcoloursandsizes(similarto t=−2.35,p=0.021)withshorterRTsintheshape-matchingcondi-\nHesselmann,Hebart,&Malach,2011).Foreverytrialarandomorder tion(M=735ms,SD=241ms)comparedtothemismatchingcondi-\nwasgeneratedforthe80masksandtheyweredisplayedatarateofca. tion(M=764ms,SD=261ms),thusreplicatingthematcheffect.As\n10Hzuntil250msaftersentenceoffset,atwhichpointthetargetpic- canbeseeninFig.2,therewas,however,noevidenceforaninteraction\nturewaspresented.Onceabuttonwaspressedor3selapsed,thenext betweenthetwofactors(t < 1).Thematcheffectwaspresentbothin\ntrial started. After half of the filler trials, a comprehension question blank screen trials (estimate=−0.076, SE=0.036, t=−2.13,\nappearedonthescreentoencourageparticipantstolistencloselytothe p=0.037) and in visual noise trials (estimate=−0.067, SE=0.03,\nsentences. t=−2.26,p=0.027).\nThus,ourresults suggestthatshapeinformationwasactivated(as\nreflectedbythematcheffect),butlow-levelvisualprocesseswerenot\n2.1.3. Analysis necessaryforit(asreflectedbytheirrelevanceofvisualinterferencefor\nPriortoanalysis,fillersandtrialswithincorrectresponsesorwith the match effect). In the context of two recent studies that reported\nRTsfasterthan300msorslowerthan2500mswereexcluded.Wethen disrupted access to visual information due to the same type of visual\nremoved trials with RTs 2.5 SDs or higher from the grand condition noise in paradigms using single words (Edmiston & Lupyan, 2017;\nmeans.RTswerestandardizedbysubtractingthemeananddividingby Ostarek & Huettig, 2017a), it seems implausible that the visual noise\nthe SD for analysis. The resulting dataset was analysed using linear techniquedidnotsufficientlyinterferewithbasicvisualprocessing.By\nmixedeffectsmodellingasimplementedintheRpackagelme4(Bates, extension,higherlevelprocessescanbeassumedtohaveprovidedthe\nMächler, Bolker, & Walker, 2014). The full model included Match implicit shape information. Regarding the study by Hirschfeld et al.\nCondition(matchvs.mismatch)andVisualCondition(visualnoisevs. (2011),this result speaks against thepossibility that theoccipital M1\nblank screen) and their interaction as fixed effects and by-participant modulation they observed reflected low-level visual simulations acti-\nand by-sentence random intercepts and slopes for Match Condition, vatedduringcomprehension.Thisisconsistentwiththeincreasedpo-\nVisual Condition, as well as the interaction term. The fixed effects sitivityobservedbyHirschfeldetal.(2011),asopposedtoadecrease\npredictors were coded as (1, −1). To obtain p-values, we computed thatwouldbeexpectedinapriming-via-re-activationexplanation(due\ntype 3 conditional F-tests with Kenward-Roger approximation for de- torepetitionsuppression).\ngrees of freedom as implemented in the Anova function of the car\npackage(Fox&Weisberg,2011),whichcallsthefunctionKRmodcomp 3. Experiment2\nofthepbkrtestpackage(Halekoh&Højsgaard,2014).Ofmaininterest\nwaswhetherwewouldfindareductionofthematcheffectinthevisual Experiment1didnotprovideevidenceforthehypothesisthatthe\nnoise condition, as reflected in the interaction between Match Condi- shape match effect in the Sentence-Picture Verification task relies on\ntionandVisualCondition.Weadditionallyperformedplannedfollow- low-levelperceptualsimulation.Itisimportanttonote,however,that\nupanalyseslookingattheeffectofMatchConditioninthevisualnoise weusedatypeofvisualnoisethatselectivelyinterfereswiththemost\ncondition and the blank screen condition separately. Specifically, we basic computations related to local colour, edge, and orientation de-\nuseddummycodingoftheVisualConditionfactortoobtainthesimple tection involving only horizontal and vertical components. Thus, it\neffectsofMatchConditionforblankscreenandvisualnoisetrials,re- remains possible that visual processes higher up in the hierarchy are\nspectively. The analysis scripts and raw data can be found at the causally involved in providing conceptual shape information. To test\nwebsite of the Open Science Framework (https://doi.org/10.17605/ this possibility, we replaced the low-level visual noise with what we\nOSF.IO/HNDG2). might call mid-level visual noise. 80 mid-level noise masks were\n86\n\n800\n750\n700\n650\n600\nno yes\nVisual Noise\ngeneratedbysuperimposing30–40imagesofrandomobjects(fromDe unlikely to evoke consistent semantic associations. Our rationale for\nGroot, Koelewijn, Huettig, & Olivers, 2016) and distorting them with using these masks as visual noise was that they should interfere with\ntheAdobePhotoshopfunctions“shear”,“ripple”,and“crystalize”such neuronpopulationswithlargerreceptivefieldsthataretunedtocom-\nthat they could no longer be recognised (see Fig. 3). The resulting plex conjunctions of multiple visual features (Peirce, 2015; Vernon,\nimages were at least as visually complex as real objects but were Gouws, Lawrence, Wade, & Morland, 2016). As such, the mid-level\n)sm(\nTR\nM.Ostareketal. Cognition 182 (2019) 84–94\nmatch\nmismatch\nFig.2.Experiment1.MeanRTstoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.\nErrorbarsindicate95%confidenceintervals.\nFig.3.Examplesofthemid-levelvisualnoisemasksusedinExperiment2.\n87\n\nvisual noiseallowed us to test whether mid-level visual processes are\ninvolvedintheshapematcheffect.\n3.1. Method\n3.1.1. Participants\nWerecruited114participantsfromtheMPIsubjectdatabase,oneof\nwhichwasexcludedduetoanerrorratehigherthan20%.\n3.1.2. Materials,set-up,design,andanalysis\nEverythingwasidenticaltoExperiment1exceptfortheuseofmid-\nlevelvisualnoisethatwaspresentedatca.10Hzinvisualnoisetrials.\n3.2. Resultsanddiscussion\nThedataweretrimmedforincorrectedresponsesandoutliersinthe\nsamewayasinExperiment1.Therewasagainasignificantmaineffect\nof Visual Condition (estimate=−0.065, SE=0.014, t=−4.70,\np < 0.001)withslowerRTsinthevisualnoisecondition(M=777ms, prior interpretation (Zwaan et al., 2002; Zwaan, 2003). Given the re-\nSD=239ms) compared to the blank screen condition (M=744ms, centdemonstrationsoftheinvolvementoflow-levelvisualprocessesin\nSD=231ms), and a main effect of Match Condition (esti- semanticprocessingusingthesameinterferencetechnique(Edmiston&\nmate=−0.069,SE=0.033,t=−2.10,p=0.039)withshorterRTs Lupyan, 2017; Ostarek & Huettig, 2017a), it is implausible that the\nin the match (M=746ms, SD=229ms) compared to the mismatch visual noise we used was not capable of impeding simulation. Never-\ncondition(M=774ms,SD=241ms).Again,asFig.4indicates,there theless, Experiment 3 was designed to ascertain that dynamic visual\nwasnoevidenceforaninteraction(t < 1).Thesizeofthematcheffect noise can inprinciple reducethe match effect inthe sentence-picture\nwas similar in the blank screen condition (estimate=−0.079, verificationparadigm.Tothatend,themeaninglessvisualnoisemasks\nSE=0.035, t=−2.23, p=0.029) and in the visual noise condition werereplacedwithpicturesofintactobjectsthatwereagaindisplayed\n(estimate=−0.06,SE=0.035,t=−1.70,p=0.094). atca.10Hz(henceforthsemanticnoise;seeFig.5).Thecriticaldiffer-\nThus, we again replicated the match effect but found no evidence ence to the previous two experiments was that pictures activate se-\nthatmid-levelvisualprocesseswerefunctionallyinvolved. mantic representations, even when they are presented for very short\ndurations and in rapid succession (Potter, 1976; Potter, Wyble,\nHagmann, & McCourt, 2014; Thorpe, Fize, & Marlot, 1996). Recent\n4. Experiment3\nstudieshavealsoshownthatvisualobjectrecognitioneveninvolvesthe\nThe results of the first two experiments point to a striking in- rapidactivationofobjectnamesinadults(McQueen&Huettig,2014),\ndependence oftheshapematcheffectfromvisualprocesses,givenits andevenintoddlers(Mani&Plunkett,2010).Assuch,semanticnoise\n800\n750\n700\n650\n600\nno yes\nVisual Noise\n)sm(\nTR\nM.Ostareketal. Cognition 182 (2019) 84–94\nFig.5.Examplesoftheobjectsusedassemanticnoise.\nmatch\nmismatch\nFig.4.Experiment2.MeanRTstoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.\nErrorbarsindicate95%confidenceintervals.\n88\n\n800\n750\n700\n650\n600\nno yes\nVisual Noise\ncan be expected to interfere with the access to conceptual shape in- thepreviousexperiments)aresummarisedinFig.6.Asintheprevious\nformationimplicitinsentences. experiments, there was a main effect of Visual Condition (esti-\nmate=−0.081,SE=0.018, t=−4.50,p < 0.001) withshorterre-\nsponses in the blank screen condition (M=720ms, SD=209ms)\n4.1. Method\ncompared to the visual noise condition (M=752ms, SD=206ms).\nThere was no significant main effect of Match Condition (matching:\n4.1.1. Participants\nM=727ms, SD=206ms; mismatching: M=745ms, SD=210ms;\nWe recruited 112 participants with normal or corrected-to-normal\nestimate=−0.047,SE=0.034,t=−1.38,p=0.172),butcrucially,\nvision and normal hearing from the MPI subject database. One parti-\nthe interaction was significant (estimate=−0.035, SE=0.017,\ncipantwasexcludedduetoanerrorrate>20%.\nt=−2.11, p=0.042), reflecting the match effect (of 31ms) in the\nblank screen condition (estimate=−0.081, SE=0.039, t=−2.11,\n4.1.2. Materials,set-up,design,andanalysis p=0.038) compared to the absent match effect (5ms) in the visual\nEverythingwasidenticaltoExperiment1and2exceptfortheuseof noise condition (estimate=−0.012, SE=0.036, t=−0.34,\nobjectpicturesasdynamicvisualnoise(againca.10Hz).Tothatend,\np > 0.7).\n80 pictures were randomly selected from the de Groot et al. (2016)\nThus, semantic noise strongly interfered with the access to shape\ndatabase with the constraint that they did not represent items men-\ninformationimplicitinsentences,demonstratingthatdynamicvisually\ntionedinanyofthesentences. presented stimuli can be effective at interfering with the retrieval of\nconceptualshapeinformationiftherelevantsystemistargetedbythe\n4.1.3. Resultsanddiscussion noise.Thisconfirmsthattheabsentimpactofvisualnoiseonthematch\nDuetoalargenumberofparticipantswithveryhighmeanRTsand effect in Experiments 1 and 2 was unlikely due to an inability of the\nlarge SDs, the outlier removal procedure used in the previous experi- presentinterferencetechniquetodiminishit,butratherduetoitsin-\nmentsresultedinveryfewobservationsperconditioninsomepartici- dependenceofmodality-specificvisualprocesses.\npantsandthemixedeffectsmodeldidnotconverge.Wethereforeex-\ncluded participants who had mean RTs larger than 1000ms and SDs 5. Experiment4\nhigherthan400msinatleastoneconditiontoreducenoise.1Thedata\nfromtheremaining73participants(thatweretrimmedforoutliersasin The experiments presented sofarindicate thatlow-level and mid-\nlevelprocessesdonotcontributefunctionallytotheshapematcheffect\n(at least not to a theoretically interesting extent), whereas semantic\nnoisereducedittonear-zero.Oneinterpretationoftheseresultsisthat\nthe match effect does not rely on modality-specific visual processes.\nAlternatively, it is conceivable that shape information is provided by\nmodality-specificvisualprocesses,butatahigherlevelthantestedin\nExperiments1and2.Specifically,adifferencebetweenthevisualand\n)sm(\nTR\nM.Ostareketal. Cognition 182 (2019) 84–94\nmatch\nmismatch\nFig.6.Experiment3.MeanRTsoftoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.\nErrorbarsindicate95%confidenceintervals.\n1Wealsoconductedageneralizedlinearmixedeffectsanalysiswithagamma\ndistributionusingallofthedata(nooutlierremovalexceptforRTs<300ms).\nThisanalysisiswellsuitedforpositivelyskeweddata,askindlypointedoutby\noneofthereviewers.Inshort,itsimilarlyshowedadecreaseofthematcheffect\nin the semantic noise condition (estimate=-7.814, SE=3.842, t=-2.03,\np=0.042), thus confirming the results above without outlier removal. See\nsupplementarymaterialsformoreinformationandequivalentanalysesforthe\notherexperiments.\n89\n\nM.Ostareketal. Cognition 182 (2019) 84–94\nFig.7.Examplesofthehigh-levelvisualnoisestimuli.\nsemantic noise manipulations was that the pictures used as noise in participants with RTs slower than 1s and SD higher than 400ms (on\nExperiment3areprocessedholisticallyandrequirefigure-grounddis- average)inatleastonecondition2.Theremaining93participantswere\ncrimination. Thus, it is possible that semantic noise interfered with usedfortheanalysesreportedbelow.TherewasamaineffectofVisual\naccesstoshapeinformationbecauseofinterferenceonahighvisual- Condition (estimate=−0.047, SE=0.015, t=−3.14, p=0.003),\nratherthansemantic-level.Toteasethesetwopossibilitiesapart,we withslowerresponsesinthevisualnoisecomparedtotheblankscreen\nran a final experiment with visual noise consisting of abstract shapes condition. There was also a main effect of Match Condition (esti-\nthatareperceivedasobjectsbutdonotevokeanyparticularsemantic mate=−0.076, SE=0.036, t=−2.09, p=0.04), with faster re-\nassociations(seeFig.7).Therationalewasthatthistypeofvisualnoise sponsesinthematchvs.mismatchcondition.However,therewaslittle\ntaps high-level visual processing related to holistic object perception evidence for an interaction between the two (estimate=−0.016,\nand figure-ground separation, but has no or very limited effects on SE=0.014, t=−1.17, p=0.25), suggesting that high-level visual\n(non-visual)semanticprocessing. noisedidnothavearobustimpactonthesizeofthematcheffect(see\nFig.8).Therewasadescriptivetrendtowardsareductionofthematch\n5.1. Method effectreflectedinthepatternthatthematcheffectwassignificantinthe\nblank screen trials (estimate=−0.092, SE=0.038, t=−2.42,\n5.1.1. Participants p=0.018), but not in the visual noise trials (estimate=−0.06,\n115 participants with normal hearing and normal or corrected-to- SE=0.040,t=−1.51,p=0.136).\nnormalvisionwererecruitedfromtheMPIdatabaseandwerepaidsix\neurosfortheirtime.Twohadtobeexcludedduetoerrorrateshigher\nthan20%. 6. Exploratoryfollow-upanalyses\n5.1.2. Materials,set-up,design,andanalysis InExperiments1,2,and4,basedonthehighstatisticalpowerofthe\nTheexperimentwasidenticaltothepreviousonesexceptforthe80 experiments and the small t-values we observed, we interpreted the\nvisual noise objects which were nonsense-objects designed in Adobe null-results for the interaction between Visual Condition and Match\nPhotoshop.Theyvariedinsize,colour,andshape,andweredesignedto Conditionasspeakingagainstafunctionalroleofvisualsimulationsfor\nlook like possible objects without resembling any particular existing the shape match effect. However, instead of a categorical yes/no\nobject. characterization of this result, it would be preferable to quantify the\namountofevidenceinfavourofaneffectofvisualnoiseonthematch\n5.1.3. Resultsanddiscussion\nThedataweretrimmedforoutliersasinthepreviousexperiments.\n2We again conducted a generalized linear mixed effects analysis with a\nAs in Experiment 3, there was an unexpected number of participants\ngamma distribution using all of the data (no outlier removal except for\nwith very longRTs that resulted innon-convergence ofthe mixed ef- RTs<300ms).TherewasstrongevidenceforamaineffectofVisualCondition\nfects model. As before, to reduce noise we therefore excluded (t=-3.35, p<.001) and a main effect of Match Condition (t=-5.15,\np<0.001),butnoevidenceforaninteraction(t<1,p>.7).Seethesupple-\nmentarymaterialsforadditionalinformation.\n90\n\n800\n750\n700\n650\n600\nno yes\nVisual Noise\neffect.Toachievethis,weconductedBayesianfollow-upanalysesusing directional BF favoured the null hypothesis, the directional BF sug-\nthebrmspackageinR(Bürkner,2016).Thestrengthofthispackageis gested that thereis some evidence foran interaction in the predicted\nthatitallowedustousethesamefixedandrandomeffectsstructureas direction (estimate=−0.017, SE=0.014, 95% CI=−0.044, 0.012,\ninthelinearmixedeffectsmodelsreportedaboveandcalculateBaye- BF01=2.46,BF =7.62).ThepicturewasdifferentinExperiment3\ndir\nsian mixed effects estimates and 95% credible intervals (CrIs). The (semantic noise) where zero was not within the 95% CrI, the non-di-\nhypothesisfunctionallowedustocomputeBayesFactorsbasedonthe rectionalBFprovidedevidenceagainstthenull,andthedirectionalBF\nSavage-Dickey method (Wagenmakers, Lodewyckx, Kuriyal, & provided very strong evidence for an interaction in the predicted di-\nGrasman,2010)whichcomputesanevidenceratiobetweenthefitofa rection (estimate=−0.032, SE=0.016, 95% CI: −0.063, −0.002,\nmodelthatassumestheregressionweightoftheinteractiontobezero BF01=0.34,BF =63.52).\ndir\n(nullhypothesis)andamodelthatassumesittobenon-zerotakinginto TheBayesian follow-up analyses thus confirmedthatthereisvery\naccount the priors and the new data (BF01; numbers larger than one little evidence that visual noise robustly diminished the match effect.\nindicate evidence infavor ofthe null). We also calculated directional This was further supported by a linear mixed-effects analysis that\nBayesFactorsasamoresensitivemeasureofareductionoftheshape pooledthedatafromallthreevisualnoiseexperiments(Experiments1,\nmatcheffect(BF ;numberslargerthanoneindicateevidenceinfavor 2,and4)toobtainmaximalpowertodetectasmallinterferenceeffect:\ndir\noftheinteractioneffectinthepredicteddirection).Theyarebasedon The results indicate that Visual Condition (t=−6.85) and Match\nanevidenceratiobetweenthefitofamodelthatassumestheregression Condition (t=−2.25) have robust effects, whereas the interaction\nweight of the interaction to be negative (i.e. that assumes that noise doesnot(t=−1.22).\nreducestheshapematcheffect)toamodelthatassumestheregression Notethatourstudieswerenotdesignedtoinvestigatewhetherse-\nweighttobepositive(i.e.thatassumesthatnoiseincreasestheshape manticnoiseproducesmoreinterferencethanvisualnoise(asthiswas\nmatcheffect).PriorsforExperiment1wereanestimatebasedonpre- not our research question). Experiment 3 (semantic noise) was con-\nvious experiments using the sentence-picture verification paradigm ductedtoprovideanotherdemonstration(inadditiontotheonesinthe\n(Rommersetal.,2013;Zwaan&Pecher,2012;Zwaanetal.,2002).For literature) that the noise manipulation can in principle interfere with\nthe remaining three experiments, the estimates and 95% credible in- semantic processing. Nevertheless, on reviewer request, we assessed\ntervals ofthe previous experiment (including all fixed effects andthe statistically whether semantic noise reduced the match effect more\nintercept) were used as priors for the following experiment, respec- stronglythanvisualnoise.Ideally,onewouldperformananalysisthat\ntively.100000iterationswererunpermodelandthethinningratewas adds Experiment (i.e. type of noise) as additional factor. This would\nsetto100. however yield a design with many more factors/levels for which our\nIn Experiment 1 (low-level visual noise) and Experiment 2 (mid- samplewouldbeunlikelytohaveadequatepower.Tomaintaina2x2\nlevelvisualnoise),the95%CrIsincludedzero,thenon-directionalBFs design, for which our sample was intended, we performed a subset\nsupportedthenullhypothesis,andthedirectionalBFprovidedminimal analysis without the blank screen trials and coded all types of visual\nevidence for an interaction in the predicted direction (Experiment 1: noise(low,mid,andhigh-level)as“visual”.Thus,weranalinearmixed\nestimate=−0.005, SE=0.013, 95% CI: −0.03, 0.02, BF01=7.82, effects model with Type of Noise (visual vs. semantic) and Match\nBF =1.84; Experiment 2: estimate=−0.008, SE=0.012, 95% CI: Condition(matchingvs.mismatching)asfixedeffects,per-participant\ndir\n−0.031, 0.016, BF01=3.35, BF =2.76). In Experiment 4 (high- randominterceptsandslopesfortheeffectofMatchCondition,andper-\ndir\nlevelvisualnoise),althoughthe95%CrIsincludedzeroandthenon- sentence random intercepts and slopes for both factors and their\n)sm(\nTR\nM.Ostareketal. Cognition 182 (2019) 84–94\nmatch\nmismatch\nFig.8.Experiment4.MeanRTsoftoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.\nErrorbarsindicate95%confidenceintervals.\n91\n\nM.Ostareketal. Cognition 182 (2019) 84–94\ninteraction (estimate=−0.025, SE=0.012, t=−1.99, p=0.05). This raises interesting questions for future research: One possible\nThisresultisconsistentwiththeviewthatsemanticnoisereducedthe accountisthatinterferenceincreasesastheactivationofsemanticin-\nmatch effect more strongly than visual noise. Future studies could formation increases. The more high-level a mask, the more likely it\ncomparetherelativeeffectofdifferenttypesofnoisebymanipulating presumably is to activate semantic information which then leads to\nthisfactorwithin-subjects. interference with the retrieval of semantic shape information during\nsentenceprocessing.Relatedtothat,itwouldbeinterestingtofindout\n7. Generaldiscussion what exactlyaccounts fortheinterference effect inducedbysemantic\nnoise.Itisplausiblethatthesemanticsystemissimplyoverloadeddue\nThesentence-pictureverificationtaskhasbeenofgreatvalueforthe tothehighrateofobjectsthatarebeingprocessed.Itisalsopossible\nfieldoflanguageprocessing,asitisaversatiletooltorevealthecon- thattheirrelevantobjectsareimplicitlylabelled.Thiscouldbetested\ntents of conceptual representations that are activated as listeners/ byusingobjectsthatareeasy(e.g.,apple)vs.difficult(e.g.,dragonfruit)\nreaders comprehend sentences. The key insight was that a match vs. tolabel.Alternatively,thepresenceofcertainvisualcharacteristicsin\nmismatch ina feature ofinterest between asentence and afollowing themasksmightdeterminetheamountofinterference.Onelimitation\npicturemodulatesresponselatenciesintheverificationtasktotheex- ofourstudyisthatvisualandsemanticnoisemasksdifferedinvisual\ntentthatthefeaturewasactivatedduringsentenceprocessing.Previous features. Thus, it is possible that the types of visual noise we used\nstudies have shown that listeners activate object shape information lackedvisualcharacteristicsthatarepresentinsimulations,suchas3D\nimplicit in sentences by demonstrating a reaction time advantage in structure, or the presence of different parts or textures. Whereas it\nsubsequent sentence-picture verification for target pictures that mat- seemsunlikelythataccesstothesekindsofdetailsplaysabiggerrole\nchedtheimpliedobjectshape(Hirschfeldetal.,2011;Rommersetal., fortheshapematcheffectthanaccesstoapproximateshapeinforma-\n2013;Zwaan&Pecher,2012;Zwaanetal.,2002). tion(seeHirschfeldetal.,2011),moreworkisneededtofullymapout\nHere, we asked which processes enable the retrieval of shape in- thefactorsthatdeterminetheamountofinterferencewiththeretrieval\nformation during online sentence comprehension. The match effect is ofsemanticshapeinformation.\ntypically interpreted as indexing perceptual simulation in sentence Overall,ourfindingsaretheoreticallyimportantastheyconstitute\ncomprehension (Engelen et al., 2011; Zwaan & Pecher, 2012; Zwaan evidence against a strong perceptual simulation-based explanation of\net al., 2002; Zwaan, 2003). In particular, the idea is that modality- the shape match effect. Our findings are compatible with accounts of\nspecific visual processes relevant for shape perception are recruited amodalrepresentationthatassumeconceptualprimingeffectstoarise\nduringlanguagecomprehensiontoprovideconceptualshapeinforma- in high-level systems with a non-modality-specific representational\ntion.Consequently,whenasubsequentshapematchingpictureappears system(e.g.,Fodor,1975).Theyarealsoconsistentwiththegrounding-\nit is processed more efficiently due to the pre-activation of relevant by-interaction model (Mahon & Caramazza, 2008; Mahon, 2015),\nvisual processes. However, while the sentence-picture verification which hypothesises amodal representations that are connected to the\nparadigm in its basic form is well-suited to uncover the contents of sensory systems, but only to the extent that sensory states do not\nrepresentations activated during sentence comprehension, it does not measurably affect processing in the amodal system. This model can\nallow inferences about how this content is represented. As Hirschfeld account for our behavioural results and the MEG data reported by\netal.(2011)pointedout;besidesperceptualsimulationthematcheffect Hirschfeldetal.(2011)bypostulatingacongruencyeffectinanamodal\nis consistent with a top-down effect based on amodal semantic re- conceptualsystemthatoptionallyinteractswithvisualprocessing.One\npresentations (Mahon, 2015) and with task-based expectations/pre- prediction following from this account that could be tested in future\ndictionsaboutthetarget(Rabagliati,Doumas,&Bemis,2017;Rommers studies is that visual interference diminishes the modulation of early\netal.,2013),eventhough shapecongruency wasalsofoundtobere- visualprocessing(asobservedbyHirschfeldetal.(2011))withoutaf-\nflected in the N400 (Coppens, Gootjes, & Zwaan, 2012) and in re- fectingthesizeofthematcheffect.\ncognition memory performance (Pecher et al., 2009) when sentences It is important to stress that we are not denying that perceptual\nandpicturesweretemporallydecoupled. simulation contributes to language comprehension, given the large\nTodirectlyprobethefunctionalroleofperceptualsimulationforthe body of evidence for this view (e.g., Barsalou, 2008; Correia et al.,\nshape match effect, in two high-powered experiments we employed 2014; Fernandino et al., 2016; Hauk et al., 2004; Lewis & Poeppel,\nvisual noise to interfere with basic visual processing during sentence 2014; Meteyard, Cuadrado, Bahrami, & Vigliocco, 2012; Ostarek &\ncomprehensionandobservedverylittleevidenceforadecreaseinthe Huettig, 2017b, 2017a; Pulvermüller, 2005; Vukovic et al., 2017).\nmatcheffect.Thiswasdespitetheuseofavisualnoisetechniquethat Otherparadigmshaverecentlyprovidedcompellingevidencethatlow-\nwasdevelopedtomaximallyinterferewithvisualprocessing(Tsuchiya level visual processes (likely related to shape) are engaged in the\n&Koch,2005)andthathasrecentlybeenshowntoselectivelyhinder comprehension of concrete object words (Edmiston & Lupyan, 2017;\naccess to visual information in single word processing (Edmiston & Lewis & Poeppel, 2014; Ostarek & Huettig, 2017b, 2017a). However,\nLupyan, 2017; Ostarek & Huettig, 2017a). A third experiment using thematcheffectinthesentence-picture verificationtaskseemstode-\npicturesofirrelevant objectsasnoise(semanticnoise) obliteratedthe pendonhigher-levelprocesses.\nmatch effect, suggesting that relatively high-level cognitive processes Thisisastrikingresultnotonlybecausetheshapematcheffectis\ndrivethematcheffect.Toprobewhetherthese processesarebestde- consideredahallmarkfindingfortheoriesofembodiedcognition,but\nscribed as high-level visual or modality-independent semantic pro- alsobecausetheparadigmseemssuchagoodcandidateforperceptual\ncesses, we conducted a final experiment in which we used nonsense simulation(whichmaypartlyexplainwhythematcheffecthasusually\nobjects to interfere with high-level visual processes related to figure- beeninterpretedthewayithas).EdmistonandLupyan(2017)observed\nground separation and holistic object perception whilst severely lim- acleareffectofvisualnoiseinsingleword-picture-verification where\niting the likelihood of(non-visual) semantic processing. Linear mixed wordcuesarefollowedbyamatchingormismatchingpictureincorrect\neffects analyses did not provide evidence for the view that high-level and inverted orientation and participants have to indicate which the\nvisualnoiserobustlyreducedthematcheffect,whichwasconfirmedby correctly oriented picture is. These results in combination with ours\nexploratory Bayesian analyses that provided only weak evidence for suggest that single words activate low-level visual processes (likely\nthathypothesis.Ourresultsthusbestfitwiththeviewthattheshape reflectingtypical objectfeatures), whereas implicitshapeinformation\nmatcheffectreliesmostlyonnon-visualsemanticprocesseswhichonly derived from event-level representations involves abstraction away\nminimally interface with low-level visual processes. The exploratory fromthesensorysystems.Thisisconsistentwithneuroimagingstudies\nBayesian analyses suggest that the likelihood of interference of high- that implicated anterior temporal regions with high-level semantic\nlevelvisualnoisewassmallbutnon-zero. processing and conceptual combination, whereas visual regions are\n92\n\nM.Ostareketal. Cognition 182 (2019) 84–94\nlinked to individual object features, such as size, colour, and shape othersuchconcretenouns).JournalofExperimentalPsychology:General,132(2),163.\n(Borghesanietal.,2016;Coutanche&Thompson-Schill,2014).Itwill Dahan,D.,&Tanenhaus,M.K.(2005).Lookingattheropewhenlookingforthesnake:\nConceptuallymediatedeyemovementsduringspoken-wordrecognition.Psychonomic\nbecrucialforfuturestudiestopreciselydelineatewhatdeterminesthe Bulletin&Review,12(3),453–459.\ninvolvementandroleofsensoryprocessesinconceptualprocessing. DeGroot,F.,Koelewijn,T.,Huettig,F.,&Olivers,C.N.(2016).Astimulussetofwords\nTheargumentsandmethodpresentedinthispapercanreadilybe andpicturesmatchedforvisualandsemanticsimilarity.JournalofCognitive\nPsychology,28(1),1–15.\napplied to other paradigms relying on congruency between sensory-\nEdmiston,P.,&Lupyan,G.(2017).Visualinterferencedisruptsvisualknowledge.Journal\nmotorcontentevokedbylinguisticinputandataskinvolvingsensory- ofMemoryandLanguage,92,281–292.\nmotor processing: Congruency effects do not provide evidence that Engelen,J.A.,Bouwmeester,S.,deBruin,A.B.,&Zwaan,R.A.(2011).Perceptualsi-\nsensory-motorsystemsproducethemunlessthetaskwhichlanguageis mulationindevelopinglanguagecomprehension.JournalofExperimentalChild\nPsychology,110(4),659–675.\nfound to have an effect on only involves sensory-motor processes Fernandino,Leonardo,Binder,J.R.,Desai,R.H.,Pendl,S.L.,Humphries,C.J.,Gross,W.\n(Lupyan & Ward, 2013; Ostarek & Huettig, 2017b). Therefore, match L.,...Seidenberg,M.S.(2016).ConceptRepresentationreflectsmultimodalab-\neffectsareausefulfirststepafterwhichfurtherinvestigationsarere- straction:Aframeworkforembodiedsemantics.CerebralCortex,26(5),2018–2034.\nhttps://doi.org/10.1093/cercor/bhv020.\nquired to reveal the underlying mechanisms. Interference techniques\nFernandino,L.,Humphries,C.J.,Conant,L.L.,Seidenberg,M.S.,&Binder,J.R.(2016).\nareappealingbecausetheygobeyondcorrelationalapproachesbydi- Heteromodalcorticalareasencodesensory-motorfeaturesofwordmeaning.Journal\nrectlytestingcausality.\nofNeuroscience,36(38),9763–9769.https://doi.org/10.1523/JNEUROSCI.4095-15.\n2016.\nFodor,J.A.(1975).Thelanguageofthought(Vol.5).HarvardUniversityPress.\nAcknowledgements Fox,J.,&Weisberg,S.(2011).An{R}companiontoappliedregression(2nded.).Thousand\nOaks,CA:SageURL:http://socserv.socsci.mcmaster.ca/jfox/Books/Companion.\nHalekoh,U.,&Højsgaard,S.(2014).AKenward-Rogerapproximationandparametric\nWe would like to thank Christoph Scheepers, Diane Pecher, and bootstrapmethodsfortestsinlinearmixedmodels–TheRpackagepbkrtest.Journal\nTomHeymanfortheirhelpfulsuggestionsonpreviousversionsofthe ofStatisticalSoftware,59(9),1–30.\nmanuscript,andPhillipAldayforhisadviceonBayesianstatistics. Hauk,O.,Johnsrude,I.,&Pulvermüller,F.(2004).Somatotopicrepresentationofaction\nwordsinhumanmotorandpremotorcortex.Neuron,41(2),301–307.\nHesselmann,G.,Hebart,M.,&Malach,R.(2011).DifferentialBOLDactivityassociated\nAppendixA. Supplementarymaterial withsubjectiveandobjectivereportsduring“blindsight”innormalobservers.Journal\nofNeuroscience,31(36),12936–12944.\nSupplementarydataassociatedwiththisarticlecanbefound,inthe\nHirschfeld,G.,Zwitserlood,P.,&Dobel,C.(2011).Effectsoflanguagecomprehensionon\nvisualprocessing–MEGdissociatesearlyperceptualandlateN400effects.Brainand\nonlineversion,athttps://doi.org/10.1016/j.cognition.2018.08.017. Language,116(2),91–96.\nHoenig,K.,Sim,E.-J.,Bochev,V.,Herrnberger,B.,&Kiefer,M.(2008).Conceptual\nReferences\nflexibilityinthehumanbrain:Dynamicrecruitmentofsemanticmapsfromvisual,\nmotor,andmotion-relatedareas.JournalofCognitiveNeuroscience,20(10),\n1799–1814.\nAnderson,A.J.,Binder,J.R.,Fernandino,L.,Humphries,C.J.,Conant,L.L.,Aguilar,M., Huettig,F.,&Altmann,G.T.(2007).Visual-shapecompetitionduringlanguage-mediated\n...Raizada,R.D.(2016).Predictingneuralactivitypatternsassociatedwithsentences attentionisbasedonlexicalinputandnotmodulatedbycontextualappropriateness.\nusinganeurobiologicallymotivatedmodelofsemanticrepresentation.Cerebral VisualCognition,15(8),985–1018.\nCortex.http://cercor.oxfordjournals.org/content/early/2016/08/12/cercor.bhw240. Huettig,F.,&McQueen,J.M.(2007).Thetugofwarbetweenphonological,semanticand\nabstract. shapeinformationinlanguage-mediatedvisualsearch.JournalofMemoryand\nBarsalou,L.W.(1993).Flexibility,structure,andlinguisticvagaryinconcepts: Language,57(4),460–482.\nManifestationsofacompositionalsystemofperceptualsymbols.TheoriesofMemory, Kan,I.P.,&Thompson-Schill,S.L.(2004).Selectionfromperceptualandconceptual\n1,29–31. representations.Cognitive,Affective,&BehavioralNeuroscience,4(4),466–482.\nBarsalou,L.W.(1999).Perceptionsofperceptualsymbols.BehavioralandBrainSciences, Kurby,C.A.,&Zacks,J.M.(2013).Theactivationofmodality-specificrepresentations\n22(04),637–660. duringdiscourseprocessing.BrainandLanguage,126(3),338–349.\nBarsalou,L.W.(2008).Groundedcognition.AnnualReviewofPsychology,59(1),617–645. Lewis,G.,&Poeppel,D.(2014).Theroleofvisualrepresentationsduringthelexical\nhttps://doi.org/10.1146/annurev.psych.59.103006.093639. accessofspokenwords.BrainandLanguage,134,1–10.\nBarsalou,L.W.(2016).Onstayinggroundedandavoidingquixoticdeadends. Lupyan,G.,&Ward,E.J.(2013).Languagecanboostotherwiseunseenobjectsintovisual\nPsychonomicBulletin&Review,23(4),1122–1142. awareness.ProceedingsoftheNationalAcademyofSciences,110(35),14196–14201.\nBates,D.,Mächler,M.,Bolker,B.,&Walker,S.(2014).Fittinglinearmixed-effectsmodels Mahon,B.Z.(2015).Whatisembodiedaboutcognition?Language,Cognitionand\nusinglme4.ArXivPreprintArXiv:1406.5823.Retrievedfromhttps://arxiv.org/abs/ Neuroscience,30(4),420–429.\n1406.5823. Mahon,B.Z.,&Caramazza,A.(2008).Acriticallookattheembodiedcognitionhy-\nBinder,J.R.(2016).Indefenseofabstractconceptualrepresentations.Psychonomic pothesisandanewproposalforgroundingconceptualcontent.JournalofPhysiology-\nBulletin&Review,23(4),1096–1108. Paris,102(1),59–70.\nBinder,J.R.,Conant,L.L.,Humphries,C.J.,Fernandino,L.,Simons,S.B.,Aguilar,M.,& Mani,N.,&Plunkett,K.(2010).Intheinfant’smind’sear:Evidenceforimplicitnamingin\nDesai,R.H.(2016).Towardabrain-basedcomponentialsemanticrepresentation. 18-month-olds.PsychologicalScience,21(7),908–913.\nCognitiveNeuropsychology,33(3–4),130–174.https://doi.org/10.1080/02643294. McQueen,J.M.,&Huettig,F.(2014).Interferenceofspokenwordrecognitionthrough\n2016.1147426. phonologicalprimingfromvisualobjectsandprintedwords.Attention,Perception,&\nBinder,J.R.,&Desai,R.H.(2011).Theneurobiologyofsemanticmemory.Trendsin Psychophysics,76(1),190–200.\nCognitiveSciences,15(11),527–536. Meteyard,L.,Cuadrado,S.R.,Bahrami,B.,&Vigliocco,G.(2012).Comingofage:A\nBorghesani,V.,&Piazza,M.(2017).Theneuro-cognitiverepresentationsofsymbols:The reviewofembodimentandtheneuroscienceofsemantics.Cortex,48(7),788–804.\ncaseofconcretewords.Neuropsychologia.Retrievedfromhttp://www.sciencedirect. Ostarek,M.,&Huettig,F.(2017b).Spokenwordscanmaketheinvisiblevisible—Testing\ncom/science/article/pii/S0028393217302397. theinvolvementoflow-levelvisualrepresentationsinspokenwordprocessing.\nBorghesani,V.,Pedregosa,F.,Buiatti,M.,Amadon,A.,Eger,E.,&Piazza,M.(2016). JournalofExperimentalPsychology:HumanPerceptionandPerformance,43(3),499.\nWordmeaningintheventralvisualpath:Aperceptualtoconceptualgradientof Ostarek,M.,&Huettig,F.(2017a).Atask-dependentcausalroleforlow-levelvisual\nsemanticcoding.NeuroImage,143,128–140. processesinspokenwordcomprehension.JournalofExperimentalPsychology:\nBruffaerts,R.,Dupont,P.,Peeters,R.,DeDeyne,S.,Storms,G.,&Vandenberghe,R. Learning,Memory,andCognition,43(8),1215.https://doi.org/10.1037/xlm0000375.\n(2013).SimilarityoffMRIactivitypatternsinleftperirhinalcortexreflectssemantic Papesh,M.H.(2015).Justoutofreach:Onthereliabilityoftheaction-sentencecom-\nsimilaritybetweenwords.JournalofNeuroscience,33(47),18597–18607. patibilityeffect.JournalofExperimentalPsychology:General,144(6),e116.\nBürkner,P.-C.(2016).brms:AnRpackageforBayesianmultilevelmodelsusingStan. Pashler,H.,&Wagenmakers,E.-J.(2012).Editors’introductiontothespecialsectionon\nJournalofStatisticalSoftware,80(1),1–28. replicabilityinpsychologicalscience:Acrisisofconfidence?Perspectiveson\nCollins,A.M.,&Loftus,E.F.(1975).Aspreading-activationtheoryofsemanticproces- PsychologicalScience,7(6),528–530.\nsing.PsychologicalReview,82(6),407. Patterson,K.,Nestor,P.J.,&Rogers,T.T.(2007).Wheredoyouknowwhatyouknow?\nCoppens,L.C.,Gootjes,L.,&Zwaan,R.A.(2012).Incidentalpictureexposureaffects Therepresentationofsemanticknowledgeinthehumanbrain.NatureReviews\nlaterreading:EvidencefromtheN400.BrainandLanguage,122(1),64–69. Neuroscience,8(12),976–987.https://doi.org/10.1038/nrn2277.\nCorreia,J.,Formisano,E.,Valente,G.,Hausfeld,L.,Jansma,B.,&Bonte,M.(2014). Pecher,D.,vanDantzig,S.,Zwaan,R.A.,&Zeelenberg,R.(2009).Languagecompre-\nBrain-basedtranslation:fMRIdecodingofspokenwordsinbilingualsrevealslan- ",
  "char_count": 50000,
  "truncated": true,
  "structure": {
    "title": "Cognition 182 (2019) 84–94",
    "authors": [
      "Dennis Joosena",
      "Falk Huettiga",
      "Original Articles",
      "Adil Ishagc",
      "Markus Ostareka"
    ],
    "abstract": "Keywords: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction\nLanguagecomprehension timeadvantageforshape-matchingpicturesinthesentence-pictureverificationtask.Typically,thisfindinghas\nConceptualprocessing beeninterpretedasevidenceforperceptualsimulation,i.e.,thataccesstoimplicitshapeinformationinvolves\nPerceptualsimulation theactivationofmodality-specificvisualprocesses.Itfollowsfromthisproposalthatdisruptingvisualproces-\nEmbodiedcognition singduringsentencecomprehensionshouldinterferewithperceptualsimulationandobliteratethematcheffect.\nSentence-pictureverification\nHerewedirectlytestthishypothesis.Participantslistenedtosentenceswhileseeingeithervisualnoisethatwas\npreviously shown to strongly interfere with basic visual processing or a blank screen. Experiments 1 and 2\nreplicatedthematcheffectbutcruciallyvisualnoisedidnotmodulateit.Whenaninterferencetechniquewas\nusedthattargetedhigh-levelsemanticprocessing(Experiment3)howeverthematcheffectvanished.Visual\nnoisespecificallytargetinghigh-levelvisualprocesses(Experiment4)onlyhadaminimaleffectonthematch\neffect.Weconcludethattheshapematcheffectinthesentence-pictureverificationparadigmisunlikelytorely\nonperceptualsimulation.",
    "sections": [
      {
        "title": "1. Introduction Herrnberger,&Kiefer,2008;Ostarek&Huettig,2017a;vanDam,van",
        "position": 1781
      },
      {
        "title": "4. Experiment3",
        "position": 20597
      },
      {
        "title": "7. Generaldiscussion what exactlyaccounts fortheinterference effect inducedbysemantic",
        "position": 33672
      },
      {
        "title": "References",
        "position": 44658
      }
    ],
    "keywords": [
      "Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction"
    ],
    "references_found": true
  },
  "extraction_method": "pdfplumber"
}