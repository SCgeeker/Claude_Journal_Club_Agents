{
  "file_path": "D:\\core\\Research\\Program_verse\\+\\pdf\\Jones-2024a.pdf",
  "file_name": "Jones-2024a.pdf",
  "full_text": "Multimodal Language Models Show Evidence of Embodied\nSimulation\nCameron R. Jones, Sean Trott\nDepartmentofCognitiveScience,UCSanDiego\n9500GilmanDrive,LaJolla,CA\ncameron@ucsd.edu,sttrott@ucsd.edu\nAbstract\nMultimodallargelanguagemodels(MLLMs)aregainingpopularityaspartialsolutionstothe“symbolgrounding\nproblem”facedbylanguagemodelstrainedontextalone. However,littleisknownaboutwhetherandhowthese\nmultiplemodalitiesareintegrated. Wedrawinspirationfromanalogousworkinhumanpsycholinguisticsonembodied\nsimulation,i.e.,thehypothesisthatlanguagecomprehensionisgrounded insensorimotorrepresentations. Weshow\nthatMLLMsaresensitivetoimplicit visualfeatureslikeobjectshape(e.g.,“Theeggwasintheskillet”impliesa\nfryingeggratherthanoneinashell). ThissuggeststhatMLLMsactivateimplicitinformationaboutobjectshape\nwhenitisimpliedbyaverbaldescriptionofanevent. Wefindmixedresultsforcolorandorientation,andruleout\nthepossibilitythatthisisduetomodels’insensitivitytothosefeaturesinourdatasetoverall. Wesuggestthatboth\nhumanpsycholinguisticsandcomputationalmodelsoflanguagecouldbenefitfromcross-pollination,e.g.,withthe\npotentialtoestablishwhethergroundedrepresentationsplayafunctional roleinlanguageprocessing.\nKeywords:grounding,multimodallanguagemodels,embodiment\n1. Introduction WinterandBergen,2012). Whilethereisongoing\ndebateaboutthefunctionalrelevanceofembodied\nRecent advances in Large Language Models simulation(Glenbergetal.,2008;MahonandCara-\n(LLMs) have generated an explosion of inter- mazza,2008;Montero-Melisetal.,2022;Ostarek\nestintheirunderlyingcapabilitiesandlimitations and Bottini, 2021), the evidence points to some\n(Thirunavukarasuetal.,2023). Oneoft-citedlimita- degreeofcross-talkbetweenlinguisticandsenso-\ntionofcontemporaryLLMsisthattheyaretrained rimotorneuralsystems.\nonlinguisticinputalone(BenderandKoller,2020),\nandthus,unlikehumans,lackaccesstoembodied\nMuchofthisevidencecomesfromthesentence-\nexperience—seen by some as a prerequisite for picture verification task paradigm (Stanfield and\nZwaan, 2001). In this task, participants read a\nlanguageunderstanding(Bisketal.,2020;Harnad,\nshortsentence(e.g.,“Hehammeredthenailinto\n1990;MolloandMillière,2023). MultimodalLarge\nthewall”),thenseeapictureofanobject(e.g.,a\nLanguageModels(MLLMsDriessetal.,2023;Gird-\nnail)andmustdecidewhethertheobjectwasmen-\nharetal.,2023;Huangetal.,2023)—whichlearnto\ntionedintheprecedingsentence. Crucially,when\nassociatelinguisticrepresentationswithdatafrom\nothermodalities—maybeapartialsolutiontothis\ntheimageoftheobjectmatchestheorientation(or\nshape, color, etc.) implied by the sentence (e.g.,\nsymbol grounding problem (Harnad, 1990). Yet\nthe nail is horizontal rather than vertical), partici-\ndespiteimpressiveperformancebyMLLMs(Doso-\npants are faster and more accurate in their deci-\nvitskiyetal.,2021), littleisknownabouthowdis-\nsions (Stanfield and Zwaan, 2001; Pecher et al.,\ntinctmodalities(e.g.,languageandvision)arein-\n2009; Connell, 2007). Because the object is the\ntegrated withinamodel’srepresentationalspace,\nsame(e.g.,anegg),humansmustbeinferringvi-\nastheyappeartobeinhumans.\nsualfeaturesbasedonpropertiesoftheeventitself\nWeaddressthisgapbyturningtoananalogous\n(e.g.,aneggcookinginaskillet).\ndebate about the extent to which human seman-\nticrepresentationsaregroundedinsensorimotor In the current work, we applied these method-\nexperience(Barsalou,1999). Theembodiedsimu- ologicalinsightstoimproveourunderstandingof\nlationhypothesis(Bergen,2015;Glenberg,2010) MLLMs. We ask whether MLLM’s internal repre-\narguesthatlanguageunderstandinginvolvesthe sentationsoflinguisticinput(e.g.,\"Hehammered\nactivation of grounded representations, i.e. that the nail into the wall\") are more similar to repre-\nthesameneuraltissuerecruitedtoperceiveorpar- sentations of images that match visual features\nticipateinanevent(e.g.,kickingasoccerball)is implied by that input than those that do not. To\nalsoengagedtounderstandlanguageaboutthat addressthisquestion,weadaptedmaterialsfrom\nevent(e.g.,“Shekickedtheball”). Indeed,awide threepsycholinguisticstudiesthatprovideevidence\nbodyofexperimentalevidencesuggeststhatsome forsimulationoftheimpliedorientation(Stanfield\ndegree of sensorimotor activation occurs during and Zwaan, 2001), shape (Pecher et al., 2009),\nlanguage processing (Zwaan and Pecher, 2012; andcolor(Connell,2007)ofobjects. Notethatthis\n\napproachdiffersfromastandardclassificationtask: tion,convertingthemintoprobabilitiesofthemodel\nratherthanclassifyingimagesonthebasisofwhich associatingeachimagewithagivensentence:\nobjectstheycontain(e.g.,“acupofcoffee”)orex-\nplicitfeaturesofthoseobjects(e.g.,“ablackcupof exp(S ·I )\np = i j\ncoffee”),weareaskingwhethertheMLLMactivates ij (cid:80)2\nexp(S ·I )\nimplicit featuresthatcouldbeinferredfromamore k=1 i k\nholisticeventrepresentation(e.g.,“Joannenever whereS istheembeddingforsentencei,I is\ni j\ntook milk in her coffee” implies that the coffee is theembeddingforimagej,andp isthesoftmax\nij\nblack). probabilitythatsentenceimatcheswithimagej.\nTostatisticallyevaluatethemodel’sperformance,\nweconductedat-testtocomparetheprobabilities\n2. Methods\nofmatching(e.g.,p andp )againstmismatching\n11 22\n(e.g.,p andp )sentence-imagepairs. Asignif-\n12 21\n2.1. Materials icantresult,wherethematchingprobabilitiesare\ngreaterthanmismatchingones,wouldindicatethat\nWeusedstimulifromthreeexperimentsthatmea-\nthe MLLM’s representations are sensitive to the\nsured visual simulation in human participants.\nvisualpropertiesimpliedbythelinguisticinput.\nItems were organized as quadruplets, consisting\nofapairofimagesandapairofsentences. Sen-\ntencepairsdifferedbyimplyingthatanobjecthad\n2.3. Vision-Language Models\nacertainvisualproperty(shape,color,ororien-\ntation). Eachoftheimagesinapairmatchedthe WeevaluatefourdifferentCLIP-basedVisionTrans-\nimpliedvisualfeatureinoneofthesentences(and formerswithdifferentnumbersofparametersand\nthereforemismatchedtheother,seeFigure1). trainingregimesinordertotestthegeneralizability\n60quadrupletsfromPecheretal.(2009)varied androbustnessofimpliedvisualfeatureeffects.\ntheimpliedshapeofanobject. Asentencesuch TheVisionTransformer(ViT)architectureadapts\nas“Therewasanegginthe[refrigerator/skillet]”im- the Transformer to handle visual data (Dosovit-\npliedthattheeggwaseitherinitsshellorcracked skiy et al., 2021). The ViT divides an image into\nopen. A pair of black-and-white images of eggs fixed-size non-overlapping patches that are then\nmatchedoneofthesesentencesbydisplayingthe linearly embedded into input vectors. A classifi-\nrelevantvisualfeature. Connell(2007)collected12 cation head is attached to the output to produce\nquadrupletsthatvarytheimpliedcolorofanob- the final prediction. Despite their simplicity and\nject. “Joanne[never/always]tookmilkinhercoffee” lackofinductivebiases(e.g.,convolutionallayers),\nimplies black/brown coffee. The images differed ViTs have achieved competitive performance on\nonlyincolor. Finally,StanfieldandZwaan(2001) various visual tasks, especially when pre-trained\ncollected24quadrupletsofsentencesimplyingdif- onlargedatasets(Dosovitskiyetal.,2021;Schuh-\nferentorientationsofanitem,andline-drawings mannetal.,2022).\nthatwererotatedtomatchtheimpliedorientation. CLIP (Contrastive Language–Image Pre-\nForinstance“Derekswunghisbatastheballap- training)employscontrastivelearningtoassociate\nproached”suggestsahorizontalbat,while“Derek imageswithtextdescriptions(Radfordetal.,2021).\nheldhisbathighastheballapproached”suggests ThemodeljointlytrainsaViTimageencoderand\naverticalbat. a text encoder to predict the correct pairings of\n(image, text) pairs. This allows CLIP to learn a\nsharedsemanticspacebetweenimagesandtext.\n2.2. Model Evaluation\nWeevaluatefourpre-trainedCLIPmodels:\nToprobeMLLMs,weimplementedacomputational ViT-B/32: Thebasemodelfrom(Radfordetal.,\nanalogueofthesentence-pictureverificationtask. 2021). ViT-B/32usesapatchsizeof32pxandhas\nOurprimaryquestionwaswhetheramodel’srep- 120M parameters. It was trained on 400 million\nresentation of a given linguistic input (e.g., \"He 224x224pixelimage-textpairsover32epochs.\nhammeredthenailintothewall\")wasmoresimilar ViT-L/14: Thebest-performingmodelfrom(Rad-\ntoitsrepresentationofanimagethatmatchedan ford et al., 2021, described in the paper as ViT-\nimpliedvisualfeature(e.g. horizontalorientation) L/14@336px). ViT-L/14usesapatchsizeof14px\ncompared to an image that did not (e.g. a verti- and has 430M parameters. It was pre-trained in\ncalnail). Foreachsentence-imagepair,wefound thesamemannerasViT-B/32andthenfine-tuned\nthedotproductbetweentheMLLMembeddingof at336pxforoneadditionalepoch.\nthe sentence and the image. This value quanti- ViT-H/14: A larger model based on the CLIP\nfiesthesimilaritybetweenthelinguisticandvisual architecture (Ilharco et al., 2021). ViT-H/14 has\nrepresentationswithinthemodel. Thedotproduct 1BparametersandwastrainedontheLAION2B\nvalueswerethenpassedthroughasoftmaxfunc- datasetfor16epochs(Schuhmannetal.,2022).\n\nFigure1: Thedatasetconsistedofpairsofsentencesandimages,formingquadruplets. Eachsentencein\napairimpliedthatanobjecthadacertainvisualproperty(e.g. browncolor). Eachimpliedvisualproperty\nwasmatchedbyoneofthepairofimages. Theimpliedvisualpropertiesincludedshape(Left, Pecher\netal.,2009),color(Center, Connell,2007),andorientation(Right, StanfieldandZwaan,2001).\nImageBind: an MLLM that learns a joint em- manipulatedvisualfeatureslikeorientation,orthat\nbedding across six modalities, including images, thesefeaturesaredifficulttoidentifyintheimage\ntext,audio,depth,thermal,andIMUdata(Girdhar stimuliused. Totestthispossibility,weranafollow-\netal.,2023). Internally,aTransformerarchitecture up“manipulationcheck”todeterminewhetherthe\nisusedforallmodalities. Theimageandtexten- MLLMsweresensitivetoorientationandcolorwhen\ncodersarebasedontheViT-H/14model. theywereexplicitlymentionedinthetext. Theanal-\nysiswasvirtuallyidenticaltotheprimaryanalysis\nabove, except that we used a sentence template\n3. Results\nthatexplicitly describedspecificvisualfeaturesof\ntheobjectinquestion,e.g.,“Itwasa[COLOR][OB-\nWe tested whether MLLMs were sensitive to the\nJECT]”.WethenaskedwhethertheMLLMscould\nimplied visual features in the sentence using a t-\nsuccessfullymatchsentenceswithexplicitvisual\ntest. The test compared the probability assigned\nfeatures(e.g.,“Itwasaredtrafficlight”vs. “Itwas\ntoimagesthatmatchedtheimpliedvisualfeatures\nagreentrafficlight”).\nversusthosethatdidnot. Allofthemodels,except\nAllmodelstestedshowedaneffectofbothcolor\nfor the smallest (ViT-B/32), showed a significant\neffect of shape. ImageBind showed the largest\n(p<.01)andorientation(p<.01). Thatis,mod-\nelsassignedhigherprobabilitytoimageswithvisual\neffect: t(238)=4.65,p<0.001. ViT-B/32showed\nan effect in the expected direction but it did not\nfeaturesthatmatchedthoseexplicitlymentioned in\nthesentence. ThisindicatesthattheMLLMsare\nreachsignificance: t(238)=1.81,p=0.072.\nsensitivetocolorandorientation,andthatstim-\nTheresultsforColorweremorevaried. Neither\nulusqualityissufficienttoidentifythesefeatures.\nthe ViT-B/32 and ViT-L/14 models showed a sig-\nnificanteffectofmatchbetweenthecolorimplied\nby a sentence and the color of an image. Both 4. Discussion\nViT-H/14 (t(46) = 2.16,p < 0.05) and ImageBind\n(t(46) = 2.85,p < 0.01 demonstrated sensitivity OurcentralquestionwaswhetherMLLMsshowed\ntoimpliedcolorpropertiesalthoughtheseeffects effects that have been taken as evidence of em-\nwerelessrobustthanforshape. bodiedsimulationinhumans(StanfieldandZwaan,\nNoneofthemodelsshowedsignificantsensitivity 2001). WeaskedwhetherMLLMsweresensitiveto\nto implied orientation from linguistic cues. The specificvisualfeatures(shape,color,andorienta-\nlargestnumericaleffectwasshownbyImageBind: tion)thatwereimplied butnotexplicitlymentioned\nt(94)=1.09,p=0.278(seeTable4). byaverbaldescriptionofanevent. Wefoundrobust\nevidence of simulation for implied shape, mixed\nevidenceforsimulationofimpliedcolor,andno\n3.1. Follow-up Analysis of Explicit\nevidenceofsimulationforimpliedorientation.\nFeatures\nImportantly,noneofthesevisualfeatureswere\nOne potential explanation for the null results re- explicitlymentionedinthesentences. Thus,ifan\nportedaboveisthatMLLMsareinsensitivetothe MLLMexhibitssensitivitytoimpliedshape,itsug-\n\nFigure2: Comparisonofmeanprobabilityvaluesassignedtoimagesthateithermatched(bluebars)or\ndidnotmatch(redbars)impliedvisualfeaturesofasentence. FourVisionTransformerModels(ViT-B/32,\nViT-L/14,ViT-H/14,andImageBind)),wereevaluatedacrossthreedatasets(Shape,Orientation,and\nColor). Errorbarsdenote95%bootstrappedconfidenceintervals.\ngests that the model is activating event-specific in the relationship between images and descrip-\nrepresentationsoftheobjectsmentionedinasen- tions. Orientationcanbeinfluencedbyrotationor\ntence. In humans, an analogous effect is taken viewpointsandcolorsimilarlyvarieswithlighting.\nasevidenceofembodiedsimulation(Stanfieldand Implicitindicationsofthesefeaturesintextlabels\nZwaan, 2001; Bergen, 2015). The findings here maythereforebelessreliablethanindicationsof\nsuggest that such an effect can be produced via moreinvariantfeaturessuchasshape. Futurework\nexposuretolarge-scalestatisticalassociationsbe- could ask whether color and orientation are less\ntweenpatternsinimagesandpatternsintext. integrated withlinguisticrepresentationsinMLLMs,\norsimplyhardertoinferfromtextdescriptions.\nModel Shape Color Orientation\nFuture studies could also explore whether\nViT-B/32 0.072 0.112 0.965\nMLLMssimulatemodalitiesbeyondvision. There\nViT-L/14 <0.001 0.240 0.510 isevidencethathumansactivateothersensorimo-\nViT-H/14 <0.001 0.036 0.323 tor modalities, such as auditory volume (Winter\nImageBind <0.001 0.006 0.278 andBergen,2012)andmotoraction(Fischerand\nZwaan,2008),thoughevidenceforothermodalities\nTable1: p-valuesfromt-testsmeasuringtheeffect\nlikeolfactionislimited(SpeedandMajid,2018).\nofmatchingimpliedvisualfeaturesbetweenlabels\nand images. All models except ViT-B/32 show a Finally,thereisconsiderabledebatewithinpsy-\nsignificanteffectforShape. ViT-H/14andImage- cholinguistics over whether embodied simulation\nBindbothshowsignificanteffectsforColor. None playsafunctional roleinlanguagecomprehension,\nofthemodelsshowaneffectofOrientation. orwhetheritisepiphenomenal(OstarekandBot-\ntini,2021;MahonandCaramazza,2008;Glenberg\nItisunclearwhyMLLMsdidnotappeartosimu- etal.,2008). Futureworkcouldcontributetothisde-\nlateorientation(orcolor,insomecases). Critically, batebyusingMLLMsas“subjects”: specifically,re-\nwheneitherfeaturewasexplicit inthetext,amatch searcherscould“lesion”representationsoffeatures\neffectwasobtained(seeSection3.1);thissuggests like shape and ask whether this causally affects\nthenulleffectswerenotduetooverallinsensitivity processing of sentences implying object shape.\ntothosevisualfeatures. Instead,MLLMsappearto This would join the broader “neuroconnectionist”\nactivatesomeimplicitvisualfeaturesmorereadily research program that aims to unify research on\nthanothers. Thisvariationcouldbedrivenbynoise humancognitionandonmodelsinspiredbycogni-\n\ntion(Doerigetal.,2023). Louise Connell. 2007. Representing object\ncolour in language comprehension. Cognition,\n102(3):476–485.\n5. Conclusion\nAdrienDoerig,RowanPSommers,KatjaSeeliger,\nWefoundthatMLLMsaresensitivetowhethervi- BlakeRichards,JenannIsmael,GraceWLind-\nsual features that are implied by a sentence are say,KonradPKording,TaliaKonkle,MarcelAJ\nmatchedinanimage,aphenomenontakenasevi- VanGerven,NikolausKriegeskorte,etal.2023.\ndenceofembodiedsimulationinhumans. The neuroconnectionist research programme.\nNatureReviewsNeuroscience,pages1–20.\n6. Ethical Considerations and\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nLimitations Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani,\nThestudyislimitedinthatitonlyevaluatesVision\nMatthiasMinderer,GeorgHeigold,SylvainGelly,\nTransformers. Other VLM architectures may pro-\nJakob Uszkoreit, and Neil Houlsby. 2021. An\nduce different associations between text and im-\nImageisWorth16x16Words: Transformersfor\nages. Thenumberofitemsforsomeofthedatasets\nImageRecognitionatScale.\nwassmall. Somemodelsmayhaveshownsignif-\nicantmatcheffectswithalargernumberofitems. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey\nOnepotentiallimitationofthestudyisthatthetasks Lynch, Aakanksha Chowdhery, Brian Ichter,\ngiventohumanandLLMparticipantsarenotquite AyzaanWahid,JonathanTompson,QuanVuong,\nanalogous. Inthepicture-verificationtask,thepar- Tianhe Yu, et al. 2023. Palm-e: An embod-\nticipantisawarethattheimpliedvisualfeaturesare iedmultimodallanguagemodel. arXivpreprint\nirrelevant: theirtaskistoidentifywhethertheobject arXiv:2303.03378.\nwaspresentinthesentence. Themodelscannot\nbe so instructed: the measure of association be- MartinHFischerandRolfAZwaan.2008. Embod-\ntweenthesentenceandimagerepresentationswill iedlanguage: Areviewoftheroleofthemotor\nbe based on all features that were useful to the systeminlanguagecomprehension. Quarterly\nmodelduringCLIPpre-training. Nevertheless,the journalofexperimentalpsychology,61(6):825–\nresults show that models are sensitive to these 850.\nimpliedfeaturesevenwhentheyarenotexplicitly\nmentioned. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu,\nMannatSingh,KalyanVasudevAlwala,Armand\nJoulin,andIshanMisra.2023. Imagebind: One\n7. Bibliographical References embeddingspacetobindthemall. InProceed-\ningsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition,pages15180–\n15190.\nLawrence W Barsalou. 1999. Perceptual sym-\nbol systems. Behavioral and brain sciences, Arthur M Glenberg. 2010. Embodiment as a uni-\n22(4):577–660. fyingperspectiveforpsychology. Wileyinterdis-\nciplinaryreviews: Cognitivescience,1(4):586–\nEmilyMBenderandAlexanderKoller.2020. Climb- 596.\ningtowardsnlu: Onmeaning,form,andunder-\nstanding in the age of data. In Proceedings of ArthurMGlenberg,MarcSato,andLuigiCattaneo.\nthe 58th annual meeting of the association for 2008. Use-induced motor plasticity affects the\ncomputationallinguistics,pages5185–5198. processing of abstract and concrete language.\nCurrentBiology,18(7):R290–R291.\nBenjaminBergen.2015. Embodiment,simulation\nand meaning. In The Routledge handbook of StevanHarnad.1990. Thesymbolgroundingprob-\nsemantics,pages142–157.Routledge. lem. Physica D: Nonlinear Phenomena, 42(1-\n3):335–346.\nYonatanBisk,AriHoltzman,JesseThomason,Ja-\ncobAndreas,YoshuaBengio,JoyceChai,Mirella ShaohanHuang,LiDong,WenhuiWang,YaruHao,\nLapata,AngelikiLazaridou,JonathanMay,Alek- Saksham Singhal, Shuming Ma, Tengchao Lv,\nsandrNisnevich,etal.2020.Experiencegrounds LeiCui,OwaisKhanMohammed,QiangLiu,etal.\nlanguage. In Proceedings of the 2020 Confer- 2023. Language is not all you need: Aligning\nenceonEmpiricalMethodsinNaturalLanguage perceptionwithlanguagemodels. arXivpreprint\nProcessing(EMNLP),pages8718–8735. arXiv:2302.14045.\n\nGabriel Ilharco, Mitchell Wortsman, Ross Wight- TingFangTan,andDanielShuWeiTing.2023.\nman, Cade Gordon, Nicholas Carlini, Rohan Large language models in medicine. Nature\nTaori,AchalDave,VaishaalShankar,Hongseok medicine,pages1–11.\nNamkoong,JohnMiller,HannanehHajishirzi,Ali\nBodo Winter and Benjamin Bergen. 2012. Lan-\nFarhadi,andLudwigSchmidt.2021. Openclip.\nguagecomprehendersrepresentobjectdistance\nIfyouusethissoftware,pleaseciteitasbelow.\nbothvisuallyandauditorily. LanguageandCog-\nBradfordZMahonandAlfonsoCaramazza.2008. nition,4(1):1–16.\nAcriticallookattheembodiedcognitionhypoth-\nRolf A. Zwaan and Diane Pecher. 2012. Revis-\nesisandanewproposalforgroundingconcep-\niting Mental Simulation in Language Compre-\ntualcontent. Journalofphysiology-Paris,102(1-\n3):59–70.\nhension: SixReplicationAttempts. PLOSONE,\n7(12):e51382.\nDimitri Coelho Mollo and Raphaël Millière. 2023.\nThe vector grounding problem. arXiv preprint\narXiv:2304.01481.\nGuillermo Montero-Melis, Jeroen Van Paridon,\nMarkusOstarek,andEmanuelBylund.2022. No\nevidenceforembodiment: Themotorsystemis\nnotneededtokeepactionverbsinworkingmem-\nory. cortex,150:108–125.\nMarkus Ostarek and Roberto Bottini. 2021.\nTowards strong inference in research on\nembodiment–possibilities and limitations of\ncausalparadigms. JournalofCognition,4(1).\nDianePecher,SaskiavanDantzig,RolfAZwaan,\nandRenéZeelenberg.2009. Shortarticle: Lan-\nguagecomprehendersretainimpliedshapeand\norientation of objects. Quarterly Journal of Ex-\nperimentalPsychology,62(6):1108–1114.\nAlec Radford, Jong Wook Kim, Chris Hallacy,\nAdityaRamesh,GabrielGoh,SandhiniAgarwal,\nGirishSastry,AmandaAskell,PamelaMishkin,\nJackClark,GretchenKrueger,andIlyaSutskever.\n2021.LearningTransferableVisualModelsFrom\nNaturalLanguageSupervision.\nChristoph Schuhmann, Romain Beaumont,\nRichardVencu,CadeGordon,RossWightman,\nMehdi Cherti, Theo Coombes, Aarush Katta,\nClayton Mullis, and Mitchell Wortsman. 2022.\nLaion-5b: An open large-scale dataset for\ntraining next generation image-text models.\nAdvances in Neural Information Processing\nSystems,35:25278–25294.\nLauraJSpeedandAsifaMajid.2018.Anexception\ntomentalsimulation: Noevidenceforembodied\nodorlanguage. CognitiveScience,42(4):1146–\n1178.\nRobertAStanfieldandRolfAZwaan.2001. The\neffectofimpliedorientationderivedfromverbal\ncontext on picture recognition. Psychological\nscience,12(2):153–156.\nArun James Thirunavukarasu, Darren Shu Jeng\nTing, Kabilan Elangovan, Laura Gutierrez,",
  "char_count": 21148,
  "truncated": false,
  "structure": {
    "title": "Multimodal Language Models Show Evidence of Embodied",
    "authors": [
      "R. Jones",
      "Multimodal Language",
      "Introduction Winterand",
      "Models Show",
      "Sean Trott"
    ],
    "abstract": "Multimodallargelanguagemodels(MLLMs)aregainingpopularityaspartialsolutionstothe“symbolgrounding\nproblem”facedbylanguagemodelstrainedontextalone. However,littleisknownaboutwhetherandhowthese\nmultiplemodalitiesareintegrated. Wedrawinspirationfromanalogousworkinhumanpsycholinguisticsonembodied\nsimulation,i.e.,thehypothesisthatlanguagecomprehensionisgrounded insensorimotorrepresentations. Weshow\nthatMLLMsaresensitivetoimplicit visualfeatureslikeobjectshape(e.g.,“Theeggwasintheskillet”impliesa\nfryingeggratherthanoneinashell). ThissuggeststhatMLLMsactivateimplicitinformationaboutobjectshape\nwhenitisimpliedbyaverbaldescriptionofanevent. Wefindmixedresultsforcolorandorientation,andruleout\nthepossibilitythatthisisduetomodels’insensitivitytothosefeaturesinourdatasetoverall. Wesuggestthatboth\nhumanpsycholinguisticsandcomputationalmodelsoflanguagecouldbenefitfromcross-pollination,e.g.,withthe\npotentialtoestablishwhethergroundedrepresentationsplayafunctional roleinlanguageprocessing.\nKeywords:grounding,multimodallanguagemodels,embodiment",
    "sections": [
      {
        "title": "1. Introduction WinterandBergen,2012). Whilethereisongoing",
        "position": 1243
      },
      {
        "title": "2. Methods",
        "position": 5073
      },
      {
        "title": "3. Results",
        "position": 9888
      },
      {
        "title": "5. Conclusion",
        "position": 15087
      },
      {
        "title": "6. Ethical Considerations and",
        "position": 15509
      },
      {
        "title": "7. Bibliographical References embeddingspacetobindthemall. InProceed-",
        "position": 17191
      }
    ],
    "keywords": [],
    "references_found": true
  },
  "extraction_method": "pdfplumber"
}