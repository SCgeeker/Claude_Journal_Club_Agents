{
  "file_path": "D:\\core\\Research\\Program_verse\\+\\pdf\\Rommers-2013.pdf",
  "file_name": "Rommers-2013.pdf",
  "full_text": "Psychological Science\nhttp://pss.sagepub.com/\nObject Shape and Orientation Do Not Routinely Influence Performance During Language Processing\nJoost Rommers, Antje S. Meyer and Falk Huettig\nPsychological Science 2013 24: 2218 originally published online 24 September 2013\nDOI: 10.1177/0956797613490746\nThe online version of this article can be found at:\nhttp://pss.sagepub.com/content/24/11/2218\nPublished by:\nhttp://www.sagepublications.com\nOn behalf of:\nAssociation for Psychological Science\nAdditional services and information for Psychological Science can be found at:\nEmail Alerts: http://pss.sagepub.com/cgi/alerts\nSubscriptions: http://pss.sagepub.com/subscriptions\nReprints: http://www.sagepub.com/journalsReprints.nav\nPermissions: http://www.sagepub.com/journalsPermissions.nav\n>> Version of Record - Nov 8, 2013\nOnlineFirst Version of Record - Sep 24, 2013\nWhat is This?\nDDoowwnnllooaaddeedd ffrroomm ppssss..ssaaggeeppuubb..ccoomm aatt MMaaxx PPllaanncckk IInnssttiittuutt oonn NNoovveemmbbeerr 1111,, 22001133\n\n490746\nresearch-article2013 PSSXXX10.1177/0956797613490746Rommers et al.Visual Representations\nResearch Article\nPsychological Science\nObject Shape and Orientation Do Not 24(11) 2218 –2225\n© The Author(s) 2013\nReprints and permissions:\nRoutinely Influence Performance During\nsagepub.com/journalsPermissions.nav\nDOI: 10.1177/0956797613490746\nLanguage Processing pss.sagepub.com\nJoost Rommers1,2, Antje S. Meyer1,3, and Falk Huettig1,3\n1Psychology of Language Department, Max Planck Institute for Psycholinguistics, Nijmegen,\nThe Netherlands; 2International Max Planck Research School for Language Sciences,\nNijmegen, The Netherlands; and 3Donders Institute for Brain, Cognition and Behaviour,\nRadboud University Nijmegen\nAbstract\nThe role of visual representations during language processing remains unclear: They could be activated as a necessary\npart of the comprehension process, or they could be less crucial and influence performance in a task-dependent\nmanner. In the present experiments, participants read sentences about an object. The sentences implied that the\nobject had a specific shape or orientation. They then either named a picture of that object (Experiments 1 and 3) or\ndecided whether the object had been mentioned in the sentence (Experiment 2). Orientation information did not\nreliably influence performance in any of the experiments. Shape representations influenced performance most strongly\nwhen participants were asked to compare a sentence with a picture or when they were explicitly asked to use mental\nimagery while reading the sentences. Thus, in contrast to previous claims, implied visual information often does not\ncontribute substantially to the comprehension process during normal reading.\nKeywords\npsycholinguistics, language\nReceived 8/8/12; Revision accepted 4/23/13\nMuch research has suggested that listeners and readers Mahon & Caramazza, 2008; for an analogous debate in\nactivate visual and motor representations of objects that mental imagery, see Kosslyn, 1994; Pylyshyn, 1981).\nare referred to in spoken or written form (for a review, Regardless of the modal or amodal format of concep-\nsee Zwaan, 2004). Since the publication of Barsalou’s tual representations, the evidence for the activation of\n(1999) seminal article, many of these findings have been visual representations during language processing\ninterpreted in terms of embodied cognition, which is the remains intriguing. Do perceptual representations deserve\nview that high-level cognitive processes such as lan- a more explicit role in models of language processing\nguage, memory, and thought involve reenactment or sim- than is currently the case? To answer this question, the\nulation of perception and action states. This idea contrasts conditions under which such representations become\nsharply with amodal theories of knowledge (e.g., Kintsch, activated must be determined.\n2008), in which the format of high-level cognitive pro- One hypothesis is that the activation of visual repre-\ncesses is dissociated from perceptual processes. However, sentations is a crucial component of language process-\namodal propositions or features can in principle also be ing: Whenever one processes a word that refers to an\nused to capture perceptual representations, and theories object (e.g., tomato), one activates a visual representation\nof embodied cognition assume that “a simulator produces\nsimulations that are always partial and sketchy, never\nCorresponding Author:\ncomplete” (Barsalou, 1999, p. 586). It is therefore uncer-\nJoost Rommers, Max Planck Institute for Psycholinguistics, P. O. Box\ntain whether any experimental evidence could falsify\n310, 6500 AH Nijmegen, The Netherlands\neither theory with regard to representational format (see E-mail: joost.rommers@mpi.nl\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\nVisual Representations 2219\nthat specifies its shape (e.g., round), and this representa- hypothesis have also suggested that the activation of per-\ntion forms a necessary part of the situation model assem- ceptual representations in linguistic or conceptual tasks\nbled during comprehension. Thus, visual representations may depend on context (e.g., van Dam, van Dijk,\nmay routinely be activated during language processing Bekkering, & Rueschemeyer, 2012). For instance, Pecher,\n(e.g., Wassenburg & Zwaan, 2010). Zeelenberg, and Barsalou (2003) observed that property\nThis claim is supported by studies showing that visual verification on a given trial (e.g., APPLE-green) was faster\nrepresentations were activated even though they were and more accurate after a trial that involved the same\npresumably not needed for the task. For instance, in a sensory modality (e.g., DIAMOND-sparkle) than after a\nsentence-picture verification study, Zwaan, Stanfield, and trial that involved a different modality (e.g., AIRPLANE-\nYaxley (2002) presented participants with sentences noisy). Furthermore, Solomon and Barsalou (2004) found\nabout objects that implied that the objects had a particu- that performance in property verification depended on\nlar shape (e.g., “The ranger saw the eagle in the sky” or perceptual variables only when thorough processing was\n“The ranger saw the eagle in the nest,” which implies that encouraged through the presence of associatively related\nthe eagle had its wings spread or tucked in, respectively). fillers. Moreover, studies of priming between shape-\nParticipants more quickly decided that a subsequently related word pairs have yielded inconsistent results. In a\nviewed object (e.g., an eagle) had been mentioned in the lexical decision task, Schreuder, Flores d’Arcais, and\nsentence when its shape corresponded to that implied in Glazenborg (1984) observed perceptual priming (e.g.,\nthe sentence than when these shapes mismatched. This ball-apple; both are round objects), but Pecher,\nsuggests that visual representations of shape were acti- Zeelenberg, and Raaijmakers (1998) failed to replicate\nvated. The same pattern was seen when the verification the priming effect, except when visual representations\ntask was replaced by picture naming. had been emphasized by shape-decision tasks before the\nSimilarly, for spatial orientation, Stanfield and Zwaan experiment. Recall that our aim was not to evaluate theo-\n(2001) found that participants were faster to indicate that ries of embodied and disembodied cognition but to\nan object (e.g., a vertically oriented nail) had been men- investigate the orthogonal issue of whether visual repre-\ntioned in a preceding sentence when its orientation sentations routinely influence performance.\nmatched that implied in the sentence (e.g., “The carpen- Other support for context dependence comes from\nter hammered the nail into the floor”) than when these eye-tracking studies using the visual-world paradigm, in\norientations mismatched (e.g., “The carpenter hammered which participants listen to spoken sentences while view-\nthe nail into the wall”). Recently, Zwaan and Pecher ing arrays of objects. In these studies, on hearing or antic-\n(2012) replicated the orientation and shape effects in an ipating a spoken word (e.g., snake), participants rapidly\nonline study that used the sentence-picture verification shifted their gaze to visually similar objects (e.g., a cable;\ntask. Pecher, van Dantzig, Zwaan, and Zeelenberg (2009) Dahan & Tanenhaus, 2005; Huettig & Altmann, 2007;\nobserved the same effects in a memory task, in which Rommers, Meyer, Praamstra, & Huettig, 2013). However,\nparticipants read a set of sentences and then judged this did not happen when the objects were replaced with\nwhether subsequently presented images had been men- printed words (Huettig & McQueen, 2011). Regarding\ntioned in those sentences. Findings showed that partici- color representations, Connell and Lynott (2009) observed\npants’ judgments were influenced by the orientation and that naming colors of colored words was easier when the\nshape implied in sentences read 45 min earlier. This sug- color of the typeface was congruent with that of the\ngests that representations of shape and orientation were object described (e.g., bear in a brown typeface) than\nactivated during sentence reading and retained over time. when it was not. However, Connell (2007) found that\nAn alternative hypothesis is that visual representations responses to picture probes were faster when the colors\nare not crucial for language comprehension. Visual rep- did not match than when they did match. Both facilita-\nresentations become activated during language process- tion (Zwaan et al., 2002) and interference (Richardson,\ning, but this may be a by-product of the way information Spivey, Barsalou, & McRae, 2003) have also been reported\ncascades through the cognitive system (Mahon & for orientation. Finally, Kang, Yap, Tse, and Kurby (2011)\nCaramazza, 2008). Moreover, whether or to what extent failed to replicate an effect of object size reported by\nvisual representations are activated may depend on the Sereno, O’Donnell, and Sereno (2009).\nparticular situation in which a concept is instantiated. In the present experiments, we investigated whether\nAccording to this hypothesis, the activation of visual rep- information about the shape and orientation of objects\nresentations can be limited to certain task situations. mentioned in written sentences routinely influences per-\nAlthough it has often been claimed that perceptual or formance. The studies discussed previously differed in\nmotor representations become activated automatically or many ways, including the materials used, the timing of\nroutinely (e.g., Pulvermüller, 2005; Wassenburg & Zwaan, the stimuli, and the tasks. We used the same materials in\n2010), some proponents of the embodied cognition all of the present experiments but varied the task.\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\n2220 Rommers et al.\nParticipants read sentences and named pictures sentence appeared in the center of the screen in white,\n(Experiment 1), explicitly compared sentences with pic- 15-point Arial font. After participants pressed a button to\ntures (Experiment 2), or used imagery before naming pic- indicate that they had read the sentence, a white fixation\ntures (Experiment 3). If the activation of visual cross appeared on the screen for 250 ms, followed by\nrepresentations is an inherent part of the comprehension a picture of an object for 3 s. Participants named the\nprocess, it should influence performance across a range object in the picture, and their speech was recorded.\nof processing tasks. If visual representations influence After every eight trials, there was an optional break.\nperformance in a task-dependent manner, larger effects Participants were instructed to read each sentence care-\nmight be observed in Experiments 2 and 3 than in fully and to name each picture as quickly and accurately\nExperiment 1. as possible.\nExperiment 1\nAnalysis\nParticipants\nA speech-waveform editor was used to manually mea-\nsure naming latencies. Responses different from the\nFifty-two native speakers of Dutch (45 women, 7 men)\nobject name mentioned in the sentence were discarded.\nwith an average age of 20 years (range = 17–26 years)\nOne of the orientation items had to be excluded because\nwere paid for their participation. All had normal or\nof an error in the construction of the materials. The data\ncorrected-to-normal vision and reported no language\nanalysis followed that of Zwaan et al. (2002). For both the\nimpairments.\norientation and the shape items, the response latencies\nwere aggregated to medians for each participant and\nStimuli and design\nentered in a 2 (condition: match, mismatch) × 2 (picture\nThe stimuli consisted of the 92 black-and-white picture- version: e.g., horizontal, vertical) × 4 (list: 1–4) repeated\nsentence quadruplets used in Pecher, van Dantzig, measures analysis of variance (ANOVA), with list as a\nZwaan, and Zeelenberg (2009). Each quadruplet con- between-subjects variable (because of the counterbal-\nsisted of two sentences and two black-and-white pic- anced design, no item analyses are reported; see\ntures, with each picture matching one sentence. In 52 of Raaijmakers, Schrijnemakers, & Gremmen, 1999).\nthe quadruplets, the orientation of the objects (horizontal Because Experiments 1 to 3 yielded null effects\nor vertical) was manipulated such that they either that are surprising given previous findings, we further\nmatched or mismatched the orientation implied in the examined the effects seen in each experiment in two\nsentences. In the remaining 40 quadruplets, the shapes of ways. First, because traditional p values never allow\nthe objects were manipulated to either match or mis- one to accept the null hypothesis, we computed an\nmatch the shapes implied in the sentences, as in the stud- approximation to Bayesian information criterion (BIC)\nies discussed previously. Each participant saw one of four posterior probabilities (p ) from the ANOVA following\nBIC\nlists, in which one sentence-picture combination from Wagenmakers (2007) and Masson (2011). In the case of a\neach quadruplet occurred. In addition, each list included null effect, we report the evidence for the null hypothe-\n92 filler items in which the sentence did not imply a par- sis, p (H |D). In the case of a significant effect, we report\nBIC 0\nticular shape or orientation and did not mention the pic- the evidence for the alternative hypothesis, p (H |D),\nBIC 1\nture (fillers were the same across all lists). The pictures but note that these two probabilities sum to 1. Second,\nvaried in size (width range = 63–720 pixels, height range we conducted a more powerful analysis using linear\n= 63–540 pixels). The items occurred in random order, mixed-effects regression models (Baayen, Davidson, &\nand the same condition (match, mismatch, and filler) did Bates, 2008). The responses were log-transformed to\nnot occur more than three times in succession. reduce skewness and analyzed with a model that included\nthe fixed factor condition (match, mismatch) and random\nintercepts and slopes by participant, picture, and sen-\nApparatus and procedure\ntence. We used the R (Version 10.2.1; R Development\nParticipants were tested individually in a sound-damped Core Team, 2009) libraries lme4 (Version 0.999375-34)\nbooth, seated in front of a 17-in. iiyama HM703UT moni- and languageR (Version 1.0). The match condition was\ntor (Iiyama, Japan), a custom-built button box, and a mapped onto the intercept. We used a likelihood-ratio\nSennheiser microphone (Wedemark, Germany). Presen- test to compare this model and a model without the fixed\ntation software (Version 14.1; Neurobehavioral Systems, effect of condition but with the same random-effects\nAlbany, CA) was used to display the images on the moni- structure. Error rates were low (see Table S1 in the\ntor. Each trial started with a 200-ms black screen. Then a Supplemental Material available online).\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\nVisual Representations 2221\nResults and discussion (back-transforming the beta to a time yielded a value of\n8 ms), χ2(1) = 1.480, p = .224.\nSeparate analyses were carried out for the orientation\nFor the shape items, we discarded 228 incorrect\nand shape items. For the orientation items, we discarded\nresponses and 24 trials because of a technical error (11.6%\n128 incorrect responses and two trials because of a tech-\nof all trials). The 16-ms difference in reaction time between\nnical error (5% of all trials). The 9-ms difference in reac-\nthe match and mismatch conditions (Fig. 1) was not\ntion times between the match and mismatch conditions significant, F(1, 48) = 0.759, p = .388, η 2 = .016, and\n(see Fig. 1) was not significant, F(1, 48) = 0.999, p = .323, p\nthere was positive evidence for the null hypothesis,\nη 2 = .020. There was a Condition × List interaction, F(3,\np p (H |D) = .827. There was a trend for a Condition × List\n48) = 3.920, p = .014, η p 2 = .197, and a three-way Condition in B t I e C rac 0 tion, F(3, 48) = 2.511, p = .070, η 2 = .136, and a\n× Picture Version × List interaction, F(3, 48) = 2.781, p = p\nCondition × Picture Version × List interaction, F(3, 48) =\n. d 0 i 5 ti 1 o , n η p w 2 a = s .1 .8 4 0 8 8 . ; T a h c e c o p B rd IC i ( n H g 0 |D to ) R fo a r f t t e h r e y ’ m s a (1 in 9 9 e 5 f ) fe c c l t a o ss f i c fi o c n a - - 4.352, p = .009, η p 2 = .214. Only the mixed-effects model\nshowed an effect of condition, β = 0.02418 (back-\ntion, this value constitutes positive evidence that there transformed: 19 ms), χ2(1) = 5.540, p = .019. A joint analy-\nwas no effect of orientation. The mixed-effects model\nsis of the orientation and shape medians in a 2 (item type:\nalso did not indicate an effect of condition, β = 0.0084\norientation, shape) × 2 (condition: match, mismatch)\nANOVA yielded no Item Type × Condition interaction, F <\n1, which suggests that the effects were comparably small.\nIn sum, using the same analyses (ANOVAs on medi-\nans) and sample size that Zwaan et al. (2002) used, we\nOrientation Items found that neither the orientation nor the shape manipu-\nlation had a significant effect on naming latencies. This is\nsurprising, given the congruency effects found for both\n800\nmanipulations by Pecher, van Dantzig, Zwaan, and\nZeelenberg (2009), who used the same materials in a\n750 memory task, and the findings by Zwaan et al. (2002,\nExperiment 2), who used materials similar to our shape\n700 items in a naming task and found a 33-ms difference\nbetween responses for matching and mismatching items.\n650 However, in the mixed-effects model, a small effect\nappeared for shape. In the next experiment, we exam-\nined whether stronger effects would be obtained with the\n600\nsame materials in a sentence-picture verification task, in\nwhich participants were explicitly required to relate the\nsentences to the pictures (following Stanfield & Zwaan,\nShape Items\n2001, and Zwaan et al., 2002, Experiment 1).\n800\nExperiment 2\nParticipants\n750\nForty-four native speakers of Dutch (38 women, 8 men)\n700 with an average age of 21 years (range = 18–26 years)\nwere paid for their participation. All had normal or\n650 corrected-to-normal vision and reported no language\nimpairments. None had taken part in Experiment 1.\n600\nStimuli\n1 (Naming) 2 (Verification) 3 (Imagery +\nNaming) The stimuli from Experiment 1 were used.\nExperiment\nProcedure\nExperiment 1 was the same as Experiment 2 except for\nthe task. Participants were asked to indicate as quickly\nand accurately as possible whether the depicted object\n)sm(\nemiT\nnoitcaeR\nMatch Condition\nMismatch Condition\n)sm(\nemiT\nnoitcaeR\nFig. 1. Average reaction time (based on by-participant medians) in\nExperiments 1, 2, and 3 as a function of whether pictures matched\nor mismatched the descriptions given in sentences. Results are shown\nseparately for orientation items (top) and shape items (bottom). Error\nbars indicate standard errors.\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\n2222 Rommers et al.\nhad been mentioned in the preceding sentence by press- Experiment 3\ning a button on a button box. A green button on the left\nParticipants\nindicated yes and a red button on the right indicated no.\nWhen the participant pressed one of the buttons, the pic-\nEighty-eight native speakers of Dutch (74 women, 14\nture disappeared from the screen.\nmen) with an average age of 20 years (range = 18–28\nyears) were paid for their participation. All had normal or\nResults and discussion corrected-to-normal vision and reported no language\nimpairments. None had participated in Experiments 1\nThe analysis was the same as for Experiment 1. For the\nor 2.\norientation items, 54 incorrect trials were discarded (2.4%\nof all trials). The ANOVAs on median reaction times\nStimuli\nshowed that the mean difference of 1 ms between the\nmatch and mismatch conditions (see Fig. 1) was not signifi- The stimuli from Experiments 1 and 2 were used.\ncant, F(1, 40) = 0.627, p = .433, η2 = .015, and there was\np\npositive evidence for the null hypothesis, p (H |D) =\nBIC 0 Procedure\n.825. There was a trend toward a Condition × Picture\nVersion × List interaction, F(3, 40) = 2.624, p = .064, η 2 = The procedure was the same as in Experiment 1, except\np\n.164. In the mixed-effects model, the effect of condition that participants were asked to create a visual image of\nalso did not reach significance, β = 0.02263 (back- the situation described in each sentence during reading,\ntransformed: 14 ms), χ2(1) = 3.224, p = .073. before they pressed the button to continue. To remind\nFor the shape items, 63 incorrect trials (3.5% of all tri- participants of this instruction, we replaced every other\nals) were discarded. On average, participants were 50 ms break screen with an imagery rating screen on which\nslower in the mismatch condition than in the match con- participants were asked to indicate how well they had\ndition (Fig. 1), F(1, 40) = 33.455, p < .001, η 2 = .455, with been able to create visual images, using a scale from 1\np\nvery strong evidence for the alternative hypothesis, (very badly; my images were vague, dark, or even absent)\np (H |D) = .999. Condition further interacted with list, to 10 (very well; my images were as bright and lively as\nBIC 1\nF(3, 40) = 4.300, p = .010, η 2 = .244, and with picture normal visual perception).\np\nversion, F(1, 40) = 11.443, p = .002, η 2 = .222. There\np\nwas a Condition × Picture Version × List interaction,\nResults and discussion\nF(3, 40) = 13.420, p < .001, η 2 = .502. In the mixed-effects\np\nmodel, there was also a significant effect of condition; The analysis was the same as for Experiment 1. For the\nβ = 0.06724 (back-transformed: 44 ms), χ2(1) = 3.224, p < orientation items, we discarded 193 incorrect responses\n.001. A joint analysis of the orientation and shape medi- (4.3% of all trials). The 7-ms advantage for the match\nans yielded a Type × Condition interaction, F(1, 43) = condition (Fig. 1) was not significant, F(1, 84) = 1.632,\n11.964, p = .001, η 2 = .218, indicating that the shape p = .205, η 2 = .019, with positive evidence for the null\np p\neffect was larger than the orientation effect. hypothesis, p (H |D) = .825. There was a Condition ×\nBIC 0\nThe shape-congruency effect was larger in the verifi- List interaction, F(3, 84) = 3.476, p = .020, η 2 = .110, and\np\ncation task (Experiment 2; 50 ms) than in the naming a Condition × Picture Version × List interaction, F(3, 84) =\ntask (Experiment 1; 16 ms). In a joint analysis of the 5.659, p = .001, η 2 = .168. The mixed-effects model did\np\nmedian reaction times from Experiments 1 and 2, this not indicate an effect of condition, β = 0.006347 (back-\nwas confirmed by a Condition × Experiment interaction, transformed: 5 ms), χ2(1) = 0.995, p = .318.\nF(1, 94) = 6.977, p = .010, η 2 = .069. This finding sup- For the shape items, we discarded 330 incorrect\np\nports task dependence, because the participants appar- responses (9.4% of all trials). On average, participants\nently activated and used visual representations of shape were slower by 20 ms to name the pictures in the mis-\nmore systematically in the verification task than in the match condition than in the match condition (Fig. 1),\nnaming task. In the verification task, the participants had which yielded a main effect of condition, F(1, 84) =\nto compare the content of the sentences with the depicted 10.108, p = .002, η 2 = .107, with positive evidence for the\np\nobjects; this can be done by using visual representations. alternative hypothesis, p (H |D) = .941. There was a\nBIC 1\nWe hypothesized that if task relevance does explain the Condition × List interaction, F(3, 84) = 3.721, p = .014,\ngreater influence of visual representations in sentence- η 2 = .117, and a Condition × Picture Version × List inter-\np\npicture verification compared with naming, one would action, F(3, 84) = 7.261, p < .001, η 2 = .206. The mixed-\np\nexpect to see a shape-congruency effect and possibly an effects model also indicated an effect of condition,\norientation-congruency effect in a naming task in which β = 0.029034 (back-transformed: 23 ms), χ2(1) = 9.100,\nreaders were explicitly instructed to recruit visual p = .003. Thus, in contrast to Experiment 1, Experiment 3\nrepresentations. showed a clear effect of implied shape in an object-\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\nVisual Representations 2223\nnaming task, though in joint analyses it was not signifi-\ncantly larger than the orientation effect—Type × Condition\ninteraction: F(1, 87) = 1.460, p = .230, η 2 = .017, or larger\np\nthan the shape effect in Experiment 1—Condition × 10\nExperiment interaction: F(1, 138) = 0.006, p = .937, η 2 =\np\n.000. This result is suggestive of the idea that recruiting\nmental imagery during sentence reading facilitated pic-\nture recognition, thereby shortening naming latencies.1 8\nExperiment 4\nIn the previous experiments, object-shape information 6\ninfluenced performance in a task-dependent manner, but\norientation representations barely influenced perfor-\nmance at all. Does this reflect an inherently smaller influ-\n4\nence of orientation representations relative to shape\nrepresentations on response latencies? In Experiment 4, a\nrating study, we examined whether the two item sets dif-\nfered in how well the visual representations implied by\n2\nthe sentences corresponded to the pictures on the screen.\nParticipants Orientation\nItem Type\nForty native speakers of Dutch (33 women, 7 men) with\nan average age of 20 years (range = 17–23 years) were\nrecruited from the same participant pool as the previous\nparticipants and paid for their participation. None of\nthem had participated in Experiments 1, 2, or 3.\n4.6 points higher in the match condition than in the mis-\nmatch condition, F(1, 36) = 247.044, p < .001, η 2 = .873.\nStimuli and design p\nFor the shape items, ratings were 3.3 points higher in\nThe stimuli from the previous experiments were used. the match condition than in the mismatch condition,\nThe 40 participants were divided into four groups of 10, F(1, 36) = 173.664, p < .001, η 2 = .828. A joint analysis\np\nand each group was randomly assigned to one of the showed that the rating difference between the match and\nfour lists. These lists contained the same items as the lists mismatch conditions was larger for the orientation items\nin the previous experiments but without the fillers. than for the shape items—Type × Condition interaction:\nThe shape and orientation items were now presented F(1, 36) = 29.796, p < .001, η 2 = .453. This means that the\np\nin separate test blocks because they required different pattern of results—shape-congruency effects but no reli-\njudgments. Block order was counterbalanced among able orientation-congruency effects—cannot be ascribed\nparticipants. to poorer quality of the orientation items compared with\nthe shape items.\nProcedure\nGeneral Discussion\nWe used the online experiment package WebExp to per-\nform the experiment (Keller, Gunasekharan, Mayo, & We compared the activation of two types of visual repre-\nCorley, 2009). On each trial, a sentence was presented sentations, shape and orientation, across three different\nabove a picture. Participants were asked to look carefully task settings. Our research was motivated by the question\nat each sentence and picture and to rate the fit between of whether the striking demonstrations of the activation\nthe sentence and the picture in terms of object shape or of visual representations during language tasks (e.g.,\norientation (depending on the block) using a scale from Stanfield & Zwaan, 2001; Zwaan et al., 2002) reflect rou-\n1 (poorest fit) to 10 (best fit). tine or task-dependent processes. There were two major\nfindings.\nFirst, effects of implied orientation seem to be very\nResults and discussion\ndifficult to obtain. We cannot resolve this discrepancy\nANOVAs were performed on by-participant mean ratings between earlier studies, which found significant effects,\n(Fig. 2). For the orientation items, the average rating was and our experiments, which failed to find orientation\n)01−1(\ngnitaR\nMatch Condition\nMismatch Condition\nShape\nFig. 2. Mean rating as a function of item type and condition in Experi-\nment 4. Error bars indicate standard errors of the mean.\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\n2224 Rommers et al.\neffects. In the present experiments, effects of orientation influence of shape representations is mediated by task\nwere absent not only in naming tasks (in which this fac- demands and probably by the use of imagery. During\ntor had not been tested before) but also in sentence- everyday reading tasks, implied visual information often\npicture verification, in which we did observe a clear does not contribute substantially to the comprehension\neffect of shape. The ratings obtained in Experiment 4 process.\nconfirmed that the items in the orientation set were well\nchosen. The larger effects for shape than for orientation Author Contributions\ntherefore cannot be attributed to a difference in item\nAll authors contributed to the study design. Data collection and\nquality; instead, they are likely to depend on cognitive\ndata analysis were performed by J. Rommers under the supervi-\nfactors, such as the importance of shape in object recog-\nsion of F. Huettig and A. S. Meyer, and all authors contributed\nnition (Biederman & Cooper, 1991). Orientation is more to interpretation of data. J. Rommers drafted the manuscript,\ndependent on viewpoint and is thus less characteristic of and F. Huettig and A. S. Meyer provided critical revisions. All\nobjects. Although our results do not exclude the possibil- authors approved the final version of the manuscript for\nity that orientation representations could be routinely submission.\nactivated (Wassenburg & Zwaan, 2010), they do cast\ndoubt on claims that orientation representations routinely Acknowledgments\ninfluence performance.\nWe thank Ronald Fisher and John Nagengast for technical sup-\nSecond, the results advance our understanding of the\nport, and we thank the research assistants of our lab for testing\nrole of visual representations in language processing by\nsome of the participants and measuring speech onsets. We are\nshowing that the influence of shape representations grateful to Diane Pecher for providing the experimental materi-\ndepends on the task. Shape influences were weaker dur- als and suggesting the Bayesian analyses, and we thank her and\ning naming (Experiment 1) than during sentence-picture an anonymous reviewer for their comments.\nverification (Experiment 2) and when participants were\ninstructed to use imagery (Experiment 3). The influence Declaration of Conflicting Interests\nof implied shape representations seems to occur on\nThe authors declared that they had no conflicts of interest with\ndemand rather than being an inherent consequence of\nrespect to their authorship or the publication of this article.\nthe reading process.\nBecause the imagery instructions were the only differ-\nSupplemental Material\nence between Experiments 1 and 3, our data suggest that\nAdditional supporting material may be found at http://pss\nthe use of imagery can be a mediating factor between\n.sagepub.com/content/by/supplemental-data\nlanguage processing and the activation of visual repre-\nsentations. In conceptual-processing research, this idea\nhas previously been rejected because of the absence of Note\ncorrelations between the size of the modality-switch 1. We previously ran a version of Experiment 3 with 24 partici-\neffect discussed in our introduction (Pecher et al., 2003) pants (replaced by a larger sample at the request of a reviewer)\nand visual-imagery measures (Pecher, van Dantzig, & that yielded similar results: There was an effect of shape (30 ms),\nSchifferstein, 2009). In the present study, we took an but not of orientation (−2 ms), which supports our conclusions.\nexperimental approach, and our results support the\ninvolvement of imagery. It is noteworthy that in a joint References\nanalysis of Experiments 1 and 3, we observed no\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-\nExperiment × Condition interaction. One interpretation\neffects modeling with crossed random effects for subjects\nof this pattern of results is that some participants in and items. Journal of Memory and Language, 59, 390–412.\nExperiment 1 spontaneously used imagery without Barsalou, L. W. (1999). Perceptual symbol systems. Behavioral\nexplicit instruction to do so. Stanfield and Zwaan (2001, & Brain Sciences, 22, 577–609.\np. 156) mentioned that in a related unpublished study Biederman, I., & Cooper, E. E. (1991). Evidence for complete\n(Stanfield, 2000), 25% of the participants reported trying translational and reflectional invariance in visual object\nto actively generate images, which is consistent with the priming. Perception, 20, 585–593.\nConnell, L. (2007). Representing object colour in language com-\nuse of spontaneous imagery. Thus, the relationship\nprehension. Cognition, 102, 476–485.\nbetween imagery and the activation of visual representa-\nConnell, L., & Lynott, D. (2009). Is a bear white in the woods?\ntions is clearly worth further investigation.\nParallel representation of implied object color during lan-\nIn sum, our findings paint a different picture of the\nguage comprehension. Psychonomic Bulletin & Review, 16,\nrole of visual representations than did previous studies.\n573–577.\nThey suggest that orientation representations play only a Dahan, D., & Tanenhaus, M. K. (2005). Looking at the rope\nminor role during language comprehension and that the when looking for the snake: Conceptually mediated eye\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013\n\nVisual Representations 2225\nmovements during spoken-word recognition. Psychonomic Raaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F.\nBulletin & Review, 12, 453–459. (1999). How to deal with “the language-as-fixed-effect fal-\nHuettig, F., & Altmann, G. T. M. (2007). Visual-shape competi- lacy”: Common misconceptions and alternative solutions.\ntion during language-mediated attention is based on lexical Journal of Memory and Language, 41, 416–426.\ninput and not modulated by contextual appropriateness. Raftery, A. E. (1995). Bayesian model selection in social\nVisual Cognition, 15, 985–1018. research. In P. V. Marsden (Ed.), Sociological methodology\nHuettig, F., & McQueen, J. M. (2011). The nature of the visual 1995 (pp. 111–196). Cambridge, England: Blackwell.\nenvironment induces implicit biases during language-medi- Richardson, D. C., Spivey, M. J., Barsalou, L. W., & McRae,\nated visual search. Memory & Cognition, 39, 1068–1084. K. (2003). Spatial representations activated during real-\nKang, S. H. K., Yap, M. J., Tse, C. S., & Kurby, C. A. (2011). time comprehension of verbs. Cognitive Science, 27, 767–\nSemantic size does not matter: “Bigger” words are not 780.\nrecognized faster. Quarterly Journal of Experimental Rommers, J., Meyer, A. S., Praamstra, P., & Huettig, F. (2013).\nPsychology, 64, 1041–1047. The contents of predictions in sentence comprehension:\nKeller, F., Gunasekharan, S., Mayo, N., & Corley, M. (2009). Activation of the shape of objects before they are referred\nTiming accuracy of Web experiments: A case study using to. Neuropsychologia, 51, 437–447.\nthe WebExp software package. Behavior Research Methods, Schreuder, R., Flores d’Arcais, G. B., & Glazenborg, G. (1984).\n41, 1–12. Effects of perceptual and conceptual similarity in semantic\nKintsch, W. (2008). Symbol systems and perceptual representa- priming. Psychological Research, 45, 339–354.\ntions. In M. De Vega, A. Glenberg, & A. Graesser (Eds.), Sereno, S. C., O’Donnell, P. J., & Sereno, M. E. (2009). Size\nSymbols and embodiment (pp. 145–164). Oxford, England: matters: Bigger is faster. Quarterly Journal of Experimental\nOxford University Press. Psychology, 62, 1115–1122.\nKosslyn, S. M. (1994). Image and brain: The resolution of the Solomon, K. O., & Barsalou, L. W. (2004). Perceptual simulation\nimagery debate. Cambridge, MA: MIT Press. in property verification. Memory & Cognition, 32, 244–259.\nMahon, B. Z., & Caramazza, A. (2008). A critical look at the Stanfield, R. A. (2000). The effects of verbal context on picture\nembodied cognition hypothesis and a new proposal for recognition. Unpublished master’s thesis, Florida State\ngrounding conceptual content. Journal of Physiology Paris, University, Tallahassee.\n102, 59–70. Stanfield, R. A., & Zwaan, R. A. (2001). The effect of implied\nMasson, M. E. J. (2011). A tutorial on a practical Bayesian alter- orientation derived from verbal context on picture recogni-\nnative to null-hypothesis significance testing. Behavior tion. Psychological Science, 12, 153–156.\nResearch Methods, 43, 679–690. van Dam, W. O., van Dijk, M., Bekkering, H., & Rueschemeyer,\nPecher, D., van Dantzig, S., & Schifferstein, H. N. J. (2009). S.-A. (2012). Flexibility in embodied lexical-semantic\nConcepts are not represented by imagery. Psychonomic representations. Human Brain Mapping, 33, 2322–\nBulletin & Review, 16, 914–919. 2333.\nPecher, D., van Dantzig, S., Zwaan, R. A., & Zeelenberg, R. Wagenmakers, E.-J. (2007). A practical solution to the pervasive\n(2009). Language comprehenders retain implied shape and problems of p values. Psychonomic Bulletin & Review, 14,\norientation of objects. Quarterly Journal of Experimental 779–804.\nPsychology, 62, 1108–1114. Wassenburg, S. I., & Zwaan, R. A. (2010). Readers routinely\nPecher, D., Zeelenberg, R., & Barsalou, L. W. (2003). Verifying represent implied object rotation: The role of visual expe-\ndifferent-modality properties for concepts produces switch- rience. Quarterly Journal of Experimental Psychology, 63,\ning costs. Psychological Science, 14, 119–124. 1665–1670.\nPecher, D., Zeelenberg, R., & Raaijmakers, J. G. W. (1998). Zwaan, R. A. (2004). The immersed experiencer: Toward an\nDoes pizza prime coin? Perceptual priming in lexical deci- embodied theory of language comprehension. In B. H.\nsion and pronunciation. Journal of Memory and Language, Ross (Ed.), The psychology of learning and motivation (pp.\n38, 401–418. 35–62). New York, NY: Academic Press.\nPulvermüller, F. (2005). Brain mechanisms linking language Zwaan, R. A., & Pecher, D. (2012). Revisiting mental simulation\nand action. Nature Reviews Neuroscience, 6, 576–582. in language comprehension: Six replication attempts. PLoS\nPylyshyn, Z. W. (1981). The imagery debate: Analogue media ONE, 7(12), e51382. Retrieved from http://www.plosone\nversus tacit knowledge. Psychological Review, 88, 16–45. .org/article/info:doi/10.1371/journal.pone.0051382\nR Development Core Team. (2009). R: A language and envi- Zwaan, R. A., Stanfield, R. A., & Yaxley, R. H. (2002). Language\nronment for statistical computing. Vienna, Austria: R comprehenders mentally represent the shapes of objects.\nFoundation for Statistical Computing. Psychological Science, 13, 168–171.\nDownloaded from pss.sagepub.com at Max Planck Institut on November 11, 2013",
  "char_count": 40141,
  "truncated": false,
  "structure": {
    "title": "Psychological Science",
    "authors": [
      "Object Shape",
      "S. Meyer",
      "Joost Rommers",
      "During Language",
      "Orientation Do",
      "First Version",
      "Not Routinely",
      "Influence Performance",
      "Falk Huettig",
      "Email Alerts"
    ],
    "abstract": "The role of visual representations during language processing remains unclear: They could be activated as a necessary\npart of the comprehension process, or they could be less crucial and influence performance in a task-dependent\nmanner. In the present experiments, participants read sentences about an object. The sentences implied that the\nobject had a specific shape or orientation. They then either named a picture of that object (Experiments 1 and 3) or\ndecided whether the object had been mentioned in the sentence (Experiment 2). Orientation information did not\nreliably influence performance in any of the experiments. Shape representations influenced performance most strongly\nwhen participants were asked to compare a sentence with a picture or when they were explicitly asked to use mental\nimagery while reading the sentences. Thus, in contrast to previous claims, implied visual information often does not\ncontribute substantially to the comprehension process during normal reading.\nKeywords\npsycholinguistics, language\nReceived 8/8/12; Revision accepted 4/23/13\nMuch research has suggested that listeners and readers Mahon & Caramazza, 2008; for an analogous debate in\nactivate visual and motor representations of objects that mental imagery, see Kosslyn, 1994; Pylyshyn, 1981).\nare referred to in spoken or written form (for a review, Regardless of the modal or amodal format of concep-\nsee Zwaan, 2004). Since the publication of Barsalou’s tual representations, the evidence for the activation of\n(1999) seminal article, many of these findings have been visual representations during language processing\ninterpreted in terms of embodied cognition, which is the remains intriguing. Do perceptual representations deserve\nview that high-level cognitive processes such as lan- a more explicit role in models of language processing\nguage, memory, and thought involve reenactment or sim- than is currently the case? To answer this question, the\nulation of perception and action states. This i",
    "sections": [
      {
        "title": "8 ms), χ2(1) = 1.480, p = .224.",
        "position": 16164
      },
      {
        "title": "4\nence of orientation representations relative to shape",
        "position": 26199
      },
      {
        "title": "2\nthe sentences corresponded to the pictures on the screen.",
        "position": 26426
      }
    ],
    "keywords": [],
    "references_found": true
  },
  "extraction_method": "pdfplumber"
}