"""
æŠ•å½±ç‰‡ç”Ÿæˆå™¨æ¨¡çµ„
åŸºæ–¼Journal Clubçš„å¤šé¢¨æ ¼å­¸è¡“ç°¡å ±ç”Ÿæˆ
"""

import re
import requests
import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import time
import yaml as yaml_lib

try:
    from jinja2 import Template
    JINJA2_AVAILABLE = True
except ImportError:
    JINJA2_AVAILABLE = False

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

# å˜—è©¦å°å…¥æ¨¡å‹ç›£æ§æ¨¡å¡Š
try:
    from src.utils.model_monitor import ModelMonitor
    MONITOR_AVAILABLE = True
except ImportError:
    try:
        # å‚™ç”¨å°å…¥è·¯å¾‘
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from src.utils.model_monitor import ModelMonitor
        MONITOR_AVAILABLE = True
    except ImportError:
        MONITOR_AVAILABLE = False
        ModelMonitor = None

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
    from pptx.enum.text import PP_ALIGN
    from pptx.dml.color import RGBColor
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False

# å¯é¸çš„LLMå¾Œç«¯
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False


class SlideMaker:
    """
    æŠ•å½±ç‰‡ç”Ÿæˆå™¨
    æ”¯æ´7ç¨®å­¸è¡“é¢¨æ ¼ã€5ç¨®è©³ç´°ç¨‹åº¦ã€3ç¨®èªè¨€
    """

    def __init__(self,
                 template_path: Optional[str] = None,
                 styles_config: Optional[str] = None,
                 llm_provider: str = "auto",
                 ollama_url: str = "http://localhost:11434",
                 api_key: Optional[str] = None,
                 selection_strategy: str = "balanced"):
        """
        åˆå§‹åŒ–æŠ•å½±ç‰‡ç”Ÿæˆå™¨

        Args:
            template_path: Jinja2æ¨¡æ¿è·¯å¾‘
            styles_config: é¢¨æ ¼é…ç½®YAMLè·¯å¾‘
            llm_provider: LLMæä¾›è€… (auto/ollama/openai/google/anthropic)
            ollama_url: Ollama APIåœ°å€
            api_key: APIé‡‘é‘°ï¼ˆOpenAI/Google/Anthropicç”¨ï¼‰
            selection_strategy: æ¨¡å‹é¸æ“‡ç­–ç•¥ (balanced/quality_first/cost_first/speed_first)
        """
        if not JINJA2_AVAILABLE:
            raise ImportError("Jinja2 not installed. Run: pip install jinja2")

        if not YAML_AVAILABLE:
            raise ImportError("PyYAML not installed. Run: pip install pyyaml")

        if not PPTX_AVAILABLE:
            raise ImportError("python-pptx not installed. Run: pip install python-pptx")

        self.llm_provider = llm_provider.lower()
        self.ollama_url = ollama_url
        self.api_key = api_key or os.getenv('LLM_API_KEY')
        self.selection_strategy = selection_strategy

        # åˆå§‹åŒ–æ¨¡å‹ç›£æ§å™¨
        self.model_monitor = None
        if MONITOR_AVAILABLE:
            try:
                self.model_monitor = ModelMonitor()
            except Exception as e:
                print(f"âš ï¸  æ¨¡å‹ç›£æ§å™¨åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
                self.model_monitor = None

        # è¼‰å…¥æ¨¡å‹é¸æ“‡é…ç½®
        self.model_config = self._load_model_config()

        # è¼‰å…¥æ¨¡æ¿
        if template_path is None:
            template_path = Path(__file__).parent.parent.parent / "templates" / "prompts" / "journal_club_template.jinja2"

        with open(template_path, 'r', encoding='utf-8') as f:
            self.template = Template(f.read())

        # è¼‰å…¥é¢¨æ ¼é…ç½®
        if styles_config is None:
            styles_config = Path(__file__).parent.parent.parent / "templates" / "styles" / "academic_styles.yaml"

        # é›²ç«¯æ¨¡å‹æ¨è–¦é…ç½®
        self.CLOUD_MODEL_RECOMMENDATIONS = {
            'zettelkasten': 'minimax-m2:cloud',  # 230Bï¼Œæ·±åº¦æ€è€ƒå’Œæ¦‚å¿µæå–
            'research_methods': 'minimax-m2:cloud',  # éœ€è¦åš´è¬¹åˆ†æ
            'literature_review': 'minimax-m2:cloud',  # éœ€è¦ç¶œåˆåˆ†æ
            'modern_academic': 'gpt-oss:20b-cloud',  # å¹³è¡¡æ•ˆèƒ½
            'teaching': 'phi3.5:cloud',  # å¿«é€Ÿæ¸…æ™°çš„æ•™å­¸å…§å®¹
            'default': 'gpt-oss:20b-cloud'  # é€šç”¨é è¨­
        }

        with open(styles_config, 'r', encoding='utf-8') as f:
            self.styles_config = yaml.safe_load(f)

        # åˆå§‹åŒ–LLMå®¢æˆ¶ç«¯
        self._init_llm_clients()

    def get_style_info(self, style: str) -> Dict[str, Any]:
        """ç²å–é¢¨æ ¼ä¿¡æ¯"""
        styles = self.styles_config.get('styles', {})
        if style not in styles:
            available = ', '.join(styles.keys())
            raise ValueError(f"Unknown style: {style}. Available: {available}")
        return styles[style]

    def get_detail_info(self, detail_level: str) -> Dict[str, Any]:
        """ç²å–è©³ç´°ç¨‹åº¦ä¿¡æ¯"""
        details = self.styles_config.get('detail_levels', {})
        if detail_level not in details:
            available = ', '.join(details.keys())
            raise ValueError(f"Unknown detail level: {detail_level}. Available: {available}")
        return details[detail_level]

    def get_language_info(self, language: str) -> Dict[str, Any]:
        """ç²å–èªè¨€ä¿¡æ¯"""
        languages = self.styles_config.get('languages', {})
        if language not in languages:
            available = ', '.join(languages.keys())
            raise ValueError(f"Unknown language: {language}. Available: {available}")
        return languages[language]

    def generate_prompt(self,
                       topic: str,
                       style: str = "modern_academic",
                       detail_level: str = "standard",
                       language: str = "chinese",
                       slide_count: int = 15,
                       pdf_content: Optional[str] = None,
                       custom_requirements: Optional[str] = None) -> str:
        """
        ç”ŸæˆLLMæç¤ºè©

        Args:
            topic: ç°¡å ±ä¸»é¡Œ
            style: å­¸è¡“é¢¨æ ¼
            detail_level: è©³ç´°ç¨‹åº¦
            language: èªè¨€
            slide_count: æŠ•å½±ç‰‡æ•¸é‡
            pdf_content: PDFå…§å®¹ï¼ˆå¯é¸ï¼‰
            custom_requirements: è‡ªè¨‚è¦æ±‚ï¼ˆå¯é¸ï¼‰

        Returns:
            ç”Ÿæˆçš„æç¤ºè©
        """
        style_info = self.get_style_info(style)
        detail_info = self.get_detail_info(detail_level)
        lang_info = self.get_language_info(language)

        prompt = self.template.render(
            topic=topic,
            slide_count=slide_count,
            style_name=style_info['name'],
            style_description=style_info['description'],
            detail_name=detail_info['name'],
            detail_description=detail_info['description'],
            detail_level=detail_level,
            language=language,
            pdf_content=pdf_content,
            custom_requirements=custom_requirements
        )

        return prompt

    def _init_llm_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ¶ç«¯ï¼ˆåƒ…åˆå§‹åŒ–æŒ‡å®šçš„providerï¼‰"""
        # åªæœ‰åœ¨ auto æ¨¡å¼æˆ–æ˜ç¢ºæŒ‡å®šæ™‚æ‰åˆå§‹åŒ–å°æ‡‰çš„å®¢æˆ¶ç«¯
        # é€™é¿å…äº†ä¸å¿…è¦çš„ API åˆå§‹åŒ–å’ŒéŒ¯èª¤

        # Google Gemini
        if (self.llm_provider in ["auto", "google"]) and GOOGLE_AVAILABLE and (self.api_key or os.getenv('GOOGLE_API_KEY')):
            try:
                genai.configure(api_key=self.api_key or os.getenv('GOOGLE_API_KEY'))
                self.google_client = genai
            except Exception:
                self.google_client = None
        else:
            self.google_client = None

        # OpenAI
        if (self.llm_provider in ["auto", "openai"]) and OPENAI_AVAILABLE and (self.api_key or os.getenv('OPENAI_API_KEY')):
            try:
                self.openai_client = openai.OpenAI(api_key=self.api_key or os.getenv('OPENAI_API_KEY'))
            except Exception:
                self.openai_client = None
        else:
            self.openai_client = None

        # Anthropic Claude
        if (self.llm_provider in ["auto", "anthropic"]) and ANTHROPIC_AVAILABLE and (self.api_key or os.getenv('ANTHROPIC_API_KEY')):
            try:
                self.anthropic_client = anthropic.Anthropic(api_key=self.api_key or os.getenv('ANTHROPIC_API_KEY'))
            except Exception:
                self.anthropic_client = None
        else:
            self.anthropic_client = None

        # OpenRouter: ä¸éœ€è¦å®¢æˆ¶ç«¯åˆå§‹åŒ–ï¼Œç›´æ¥ä½¿ç”¨ requests + OPENROUTER_API_KEY
        # æª¢æ¸¬åœ¨ _detect_available_providers() ä¸­é€šéç’°å¢ƒè®Šæ•¸å®Œæˆ

    def _load_model_config(self) -> Dict:
        """è¼‰å…¥æ¨¡å‹é¸æ“‡é…ç½®"""
        config_path = Path(__file__).parent.parent.parent / "config" / "model_selection.yaml"
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
            except Exception as e:
                print(f"âš ï¸  è¼‰å…¥æ¨¡å‹é…ç½®å¤±æ•—ï¼š{e}")
        return {}

    def _select_best_model(self, task_type: Optional[str] = None,
                          style: Optional[str] = None) -> Tuple[str, str]:
        """
        æ™ºèƒ½é¸æ“‡æœ€ä½³æ¨¡å‹

        Args:
            task_type: ä»»å‹™é¡å‹ï¼ˆzettelkasten/academic_slidesç­‰ï¼‰
            style: å­¸è¡“é¢¨æ ¼

        Returns:
            (provider, model_name) å…ƒçµ„
        """
        if not self.model_config:
            # æ²’æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜èª
            return self._get_default_model()

        # ç²å–å¯ç”¨çš„æä¾›è€…
        available_providers = self._detect_available_providers()
        if not available_providers:
            raise RuntimeError("æ²’æœ‰å¯ç”¨çš„LLMæä¾›è€…")

        # æ ¹æ“šé¢¨æ ¼ç²å–æ¨è–¦æ¨¡å‹
        candidates = []
        if style and "style_model_mapping" in self.model_config:
            style_mapping = self.model_config["style_model_mapping"].get(style, {})
            candidates.extend(style_mapping.get("preferred", []))
            candidates.extend(style_mapping.get("fallback", []))
            avoid = style_mapping.get("avoid", [])
        elif task_type and "task_model_mapping" in self.model_config:
            task_mapping = self.model_config["task_model_mapping"].get(task_type, {})
            candidates.extend(task_mapping.get("preferred", []))
            candidates.extend(task_mapping.get("fallback", []))
            avoid = task_mapping.get("avoid", [])
        else:
            # ä½¿ç”¨å„ªå…ˆç´šé †åº
            models = self.model_config.get("models", {})
            sorted_models = sorted(models.items(),
                                 key=lambda x: x[1].get("priority", 999))
            candidates = [m[0] for m in sorted_models]
            avoid = []

        # é¸æ“‡ç¬¬ä¸€å€‹å¯ç”¨çš„æ¨¡å‹
        for model_id in candidates:
            if model_id in avoid:
                continue

            model_info = self.model_config.get("models", {}).get(model_id)
            if not model_info:
                continue

            provider = model_info.get("provider")
            model_name = model_info.get("model_name")

            # æª¢æŸ¥æä¾›è€…æ˜¯å¦å¯ç”¨
            if provider not in available_providers:
                continue

            # æª¢æŸ¥é…é¡ï¼ˆå¦‚æœæœ‰ç›£æ§å™¨ï¼‰
            if self.model_monitor:
                quota_status = self.model_monitor.check_quota_status(provider, model_name)
                if quota_status.get("exceeded", False):
                    continue

                # æª¢æŸ¥æˆæœ¬é™åˆ¶
                cost_status = self.model_monitor.check_cost_status()
                if cost_status.get("controlled") and cost_status["session"].get("exceeded"):
                    # æˆæœ¬è¶…é™ï¼Œé¸æ“‡å…è²»æ¨¡å‹
                    if not model_info.get("free_quota", False):
                        continue

            print(f"ğŸ¯ æ™ºèƒ½é¸æ“‡æ¨¡å‹ï¼š{model_id} ({provider}/{model_name})")
            return provider, model_name

        # æ²’æœ‰æ‰¾åˆ°åˆé©çš„ï¼Œä½¿ç”¨é»˜èª
        return self._get_default_model()

    def _get_default_model(self) -> Tuple[str, str]:
        """ç²å–é»˜èªæ¨¡å‹"""
        available = self._detect_available_providers()
        if "google" in available:
            return "google", "gemini-2.0-flash-exp"
        elif "anthropic" in available:
            return "anthropic", "claude-3-haiku-20240307"
        elif "ollama" in available:
            return "ollama", "gpt-oss:20b-cloud"
        else:
            raise RuntimeError("æ²’æœ‰å¯ç”¨çš„LLMæä¾›è€…")

    def _check_ollama_health(self) -> bool:
        """æª¢æŸ¥Ollamaæœå‹™å¥åº·ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception:
            return False

    def _detect_available_providers(self) -> List[str]:
        """åµæ¸¬å¯ç”¨çš„LLMæä¾›è€…"""
        providers = []

        # æª¢æŸ¥ Ollama
        if self._check_ollama_health():
            providers.append('ollama')

        # æª¢æŸ¥ Google
        if self.google_client:
            providers.append('google')

        # æª¢æŸ¥ OpenAI
        if self.openai_client:
            providers.append('openai')

        # æª¢æŸ¥ Anthropic
        if self.anthropic_client:
            providers.append('anthropic')

        # æª¢æŸ¥ OpenRouter
        if os.getenv('OPENROUTER_API_KEY'):
            providers.append('openrouter')

        return providers

    def call_llm(self,
                 prompt: str,
                 model: Optional[str] = None,
                 provider: Optional[str] = None,
                 timeout: int = 300,
                 task_type: Optional[str] = None,
                 style: Optional[str] = None,
                 max_tokens: int = 4096) -> Tuple[str, str]:
        """
        çµ±ä¸€çš„LLMèª¿ç”¨æ¥å£ï¼Œæ”¯æ´å¤šå¾Œç«¯å’Œè‡ªå‹•fallback

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±
            provider: æŒ‡å®šLLMæä¾›è€…ï¼ˆå¯é¸ï¼ŒNoneå‰‡è‡ªå‹•é¸æ“‡ï¼‰
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            task_type: ä»»å‹™é¡å‹ï¼ˆç”¨æ–¼æ™ºèƒ½é¸æ“‡ï¼‰
            style: å­¸è¡“é¢¨æ ¼ï¼ˆç”¨æ–¼æ™ºèƒ½é¸æ“‡ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ tokens æ•¸ï¼ˆåƒ… OpenRouterï¼Œé»˜èª 4096ï¼‰

        Returns:
            (ç”Ÿæˆçš„å…§å®¹, ä½¿ç”¨çš„provider)
        """
        # æ±ºå®šä½¿ç”¨çš„providerå’Œmodel
        actual_model = model
        if provider is None:
            if self.llm_provider == "auto":
                # ä½¿ç”¨æ™ºèƒ½é¸æ“‡
                selected_provider, selected_model = self._select_best_model(task_type, style)
                provider = selected_provider
                if not model:  # å¦‚æœæ²’æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨æ™ºèƒ½é¸æ“‡çš„
                    actual_model = selected_model
                print(f"ğŸ¤– æ™ºèƒ½é¸æ“‡ï¼š{provider}/{actual_model}")
            else:
                provider = self.llm_provider

        # æº–å‚™fallback chain
        fallback_chain = [provider]
        available = self._detect_available_providers()
        for p in available:
            if p not in fallback_chain:
                fallback_chain.append(p)

        # å˜—è©¦èª¿ç”¨LLMï¼ˆå¸¶fallbackï¼‰
        last_error = None
        for attempt_provider in fallback_chain:
            start_time = time.time()
            try:
                # æ ¹æ“šæä¾›è€…èª¿ç”¨å°æ‡‰çš„æ–¹æ³•
                if attempt_provider == 'ollama':
                    used_model = actual_model or "gpt-oss:20b-cloud"
                    result = self.call_ollama(prompt, used_model, timeout)
                elif attempt_provider == 'google':
                    used_model = actual_model or "gemini-2.0-flash-exp"
                    result = self.call_google(prompt, used_model)
                elif attempt_provider == 'openai':
                    used_model = actual_model or "gpt-3.5-turbo"
                    result = self.call_openai(prompt, used_model)
                elif attempt_provider == 'anthropic':
                    used_model = actual_model or "claude-3-haiku-20240307"
                    result = self.call_anthropic(prompt, used_model)
                elif attempt_provider == 'openrouter':
                    used_model = actual_model or "anthropic/claude-3.5-sonnet"
                    result = self.call_openrouter(prompt, used_model, timeout, max_tokens)
                else:
                    continue

                # è¨ˆç®—éŸ¿æ‡‰æ™‚é–“
                response_time = time.time() - start_time

                # è¿½è¹¤ä½¿ç”¨æƒ…æ³ï¼ˆå¦‚æœæœ‰ç›£æ§å™¨ï¼‰
                if self.model_monitor:
                    # ä¼°ç®—tokenæ•¸ï¼ˆç°¡å–®ä¼°ç®—ï¼šå­—å…ƒæ•¸/4ï¼‰
                    estimated_tokens = (len(prompt) + len(result)) // 4
                    usage_info = self.model_monitor.track_usage(
                        model_name=used_model,
                        provider=attempt_provider,
                        tokens_used=estimated_tokens,
                        response_time=response_time,
                        success=True,
                        task_type=task_type
                    )

                    # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ›æ¨¡å‹
                    if usage_info.get("quota_status", {}).get("warning"):
                        print(f"âš ï¸  é…é¡è­¦å‘Šï¼š{used_model} ä½¿ç”¨é‡æ¥è¿‘é™åˆ¶")
                    if usage_info.get("cost_status", {}).get("session", {}).get("warning"):
                        print(f"ğŸ’° æˆæœ¬è­¦å‘Šï¼šç•¶å‰æœƒè©±æˆæœ¬ ${usage_info['session_cost']:.2f}")

                return result, attempt_provider

            except Exception as e:
                # è¿½è¹¤å¤±æ•—ï¼ˆå¦‚æœæœ‰ç›£æ§å™¨ï¼‰
                if self.model_monitor:
                    response_time = time.time() - start_time
                    self.model_monitor.track_usage(
                        model_name=actual_model or "unknown",
                        provider=attempt_provider,
                        tokens_used=0,
                        response_time=response_time,
                        success=False,
                        task_type=task_type,
                        error_message=str(e)
                    )

                last_error = e
                if len(fallback_chain) > 1:
                    print(f"âš ï¸  {attempt_provider} å¤±æ•—ï¼š{str(e)}")
                    if attempt_provider != fallback_chain[-1]:
                        print(f"ğŸ”„ å˜—è©¦fallbackåˆ°ä¸‹ä¸€å€‹provider...")
                continue

        # æ‰€æœ‰provideréƒ½å¤±æ•—
        raise RuntimeError(f"æ‰€æœ‰LLMæä¾›è€…éƒ½å¤±æ•—ã€‚æœ€å¾ŒéŒ¯èª¤ï¼š{last_error}")

    def call_ollama(self,
                   prompt: str,
                   model: str = "gpt-oss:20b-cloud",
                   timeout: int = 300) -> str:
        """
        èª¿ç”¨Ollama APIç”Ÿæˆå…§å®¹ï¼ˆæ”¯æ´æœ¬åœ°å’Œé›²ç«¯æ¨¡å‹ï¼‰

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±ï¼ˆæ”¯æ´ :cloud å¾Œç¶´çš„é›²ç«¯æ¨¡å‹ï¼‰
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            ç”Ÿæˆçš„å…§å®¹

        æ”¯æ´çš„é›²ç«¯æ¨¡å‹ï¼š
            - minimax-m2:cloud (230B, æ”¯æ´thinking) - åƒ…CLI
            - gpt-oss:20b-cloud (20B)
            - qwen2.5-coder:cloud (3.1B, ç¨‹å¼ç¢¼åˆ†æ)
            - phi3.5:cloud (3.8B, é€šç”¨ä»»å‹™)
        """
        # ç‰¹æ®Šè™•ç† MiniMax-M2:cloud - ä½¿ç”¨CLIæ–¹å¼
        if model.lower() == "minimax-m2:cloud":
            import subprocess
            import tempfile
            import os

            try:
                # å°‡promptå¯«å…¥è‡¨æ™‚æ–‡ä»¶ä»¥é¿å…å‘½ä»¤è¡Œé•·åº¦é™åˆ¶
                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as f:
                    f.write(prompt)
                    temp_file = f.name

                # ä½¿ç”¨ollama CLIé‹è¡Œ
                cmd = f'type "{temp_file}" | ollama run {model}'

                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    errors='ignore',  # å¿½ç•¥ç·¨ç¢¼éŒ¯èª¤
                    timeout=timeout
                )

                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_file)
                except:
                    pass

                if result.returncode != 0:
                    raise RuntimeError(f"Ollama CLI failed: {result.stderr}")

                # è¿”å›è¼¸å‡ºï¼Œç§»é™¤thinkingéƒ¨åˆ†å’ŒANSIæ§åˆ¶å­—ç¬¦
                output = result.stdout

                # ç§»é™¤ANSIè½‰ç¾©åºåˆ—
                import re
                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                output = ansi_escape.sub('', output)

                # æå–thinkingä¹‹å¾Œçš„å¯¦éš›å›æ‡‰
                if "Thinking..." in output:
                    # æ‰¾åˆ°"...done thinking."ä¹‹å¾Œçš„å…§å®¹
                    if "...done thinking." in output:
                        parts = output.split("...done thinking.")
                        if len(parts) > 1:
                            output = parts[1].strip()
                    elif "done thinking" in output:
                        parts = output.split("done thinking")
                        if len(parts) > 1:
                            output = parts[1].strip()

                # æ¸…ç†æ®˜ç•™çš„æ§åˆ¶å­—ç¬¦
                output = re.sub(r'\[.*?\]', '', output)
                output = re.sub(r'[â ™â ¹â ¸â ´â ¦â §â â ‹]', '', output)
                output = output.strip()

                return output.strip()

            except subprocess.TimeoutExpired:
                raise RuntimeError(f"Ollama CLI timeout after {timeout} seconds")
            except Exception as e:
                raise RuntimeError(f"Ollama CLI execution failed: {e}")

        # å…¶ä»–æ¨¡å‹ä½¿ç”¨APIæ–¹å¼
        api_endpoint = f"{self.ollama_url}/api/generate"

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }

        try:
            response = requests.post(
                api_endpoint,
                json=payload,
                timeout=timeout
            )
            response.raise_for_status()

            result = response.json()
            return result.get('response', '')

        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Ollama API call failed: {e}")

    def call_google(self,
                   prompt: str,
                   model: str = "gemini-pro") -> str:
        """
        èª¿ç”¨Google Gemini APIç”Ÿæˆå…§å®¹

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±

        Returns:
            ç”Ÿæˆçš„å…§å®¹
        """
        if not self.google_client:
            raise RuntimeError("Google Gemini not available. Install: pip install google-generativeai")

        try:
            model_instance = self.google_client.GenerativeModel(model)
            response = model_instance.generate_content(prompt)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Google Gemini API call failed: {e}")

    def call_openai(self,
                   prompt: str,
                   model: str = "gpt-3.5-turbo") -> str:
        """
        èª¿ç”¨OpenAI APIç”Ÿæˆå…§å®¹

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±

        Returns:
            ç”Ÿæˆçš„å…§å®¹
        """
        if not self.openai_client:
            raise RuntimeError("OpenAI not available. Install: pip install openai")

        try:
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            raise RuntimeError(f"OpenAI API call failed: {e}")

    def call_anthropic(self,
                      prompt: str,
                      model: str = "claude-3-sonnet-20240229") -> str:
        """
        èª¿ç”¨Anthropic Claude APIç”Ÿæˆå…§å®¹

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±

        Returns:
            ç”Ÿæˆçš„å…§å®¹
        """
        if not self.anthropic_client:
            raise RuntimeError("Anthropic Claude not available. Install: pip install anthropic")

        try:
            response = self.anthropic_client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text
        except Exception as e:
            raise RuntimeError(f"Anthropic Claude API call failed: {e}")

    def call_openrouter(self,
                       prompt: str,
                       model: str = "anthropic/claude-3.5-sonnet",
                       timeout: int = 300,
                       max_tokens: int = 4096) -> str:
        """
        èª¿ç”¨ OpenRouter API ç”Ÿæˆå…§å®¹

        Args:
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±ï¼ˆä¾‹å¦‚: anthropic/claude-3.5-sonnetï¼‰
            timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆ tokens æ•¸ï¼ˆé»˜èª 4096ï¼‰

        Returns:
            ç”Ÿæˆçš„å…§å®¹

        æ”¯æ´çš„æ¨¡å‹ç¯„ä¾‹ï¼š
            - anthropic/claude-3.5-sonnet (æ¨è–¦ç”¨æ–¼ Zettelkasten)
            - anthropic/claude-3-haiku (å¿«é€Ÿç¶“æ¿Ÿ)
            - google/gemini-2.0-flash-exp (å…è²»)
        """
        api_key = os.getenv('OPENROUTER_API_KEY')
        if not api_key:
            raise ValueError("OPENROUTER_API_KEY not set. Please add it to .env file")

        url = "https://openrouter.ai/api/v1/chat/completions"

        headers = {
            "Authorization": f"Bearer {api_key}",
            "HTTP-Referer": "https://github.com/claude-lit-workflow",
            "X-Title": "Claude Lit Workflow - Zettelkasten Generator"
        }

        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens
        }

        try:
            response = requests.post(url, headers=headers, json=data, timeout=timeout)
            response.raise_for_status()

            result = response.json()
            return result['choices'][0]['message']['content']

        except requests.exceptions.Timeout:
            raise RuntimeError(f"OpenRouter API call timeout after {timeout}s")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"OpenRouter API call failed: {e}")
        except (KeyError, IndexError) as e:
            raise RuntimeError(f"Failed to parse OpenRouter response: {e}")

    def parse_slides(self, content: str) -> List[Dict[str, str]]:
        """
        è§£æLLMç”Ÿæˆçš„æŠ•å½±ç‰‡å…§å®¹

        Args:
            content: LLMç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            æŠ•å½±ç‰‡åˆ—è¡¨ï¼Œæ¯å€‹æŠ•å½±ç‰‡åŒ…å« title å’Œ content
        """
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²æŠ•å½±ç‰‡
        pattern = r'===([^=]+)==='
        parts = re.split(pattern, content)

        slides = []

        # è·³éç¬¬ä¸€éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯å‰ç½®æ–‡æœ¬ï¼‰
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                title = parts[i].strip()
                slide_content = parts[i + 1].strip()

                slides.append({
                    'title': title,
                    'content': slide_content,
                    'type': 'title' if 'æ¨™é¡Œé ' in title or 'Title' in title else 'content'
                })

        return slides

    def create_markdown(self,
                       slides: List[Dict[str, str]],
                       output_path: str,
                       title: Optional[str] = None,
                       style: str = "modern_academic") -> str:
        """
        å‰µå»ºMarkdownç°¡å ±æ–‡ä»¶ï¼ˆæ”¯æ´Marp/reveal.jsæ ¼å¼ï¼‰

        Args:
            slides: æŠ•å½±ç‰‡æ•¸æ“š
            output_path: è¼¸å‡ºè·¯å¾‘
            title: ç°¡å ±æ¨™é¡Œ
            style: å­¸è¡“é¢¨æ ¼

        Returns:
            è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        """
        # è¼‰å…¥Markdownç°¡å ±æ¨¡æ¿
        md_template_path = Path(__file__).parent.parent.parent / "templates" / "markdown" / "academic_slides.jinja2"

        with open(md_template_path, 'r', encoding='utf-8') as f:
            md_template = Template(f.read())

        # æº–å‚™æ•¸æ“š
        main_title = title or "å­¸è¡“ç°¡å ±"
        subtitle = ""
        content_slides = []

        for slide_data in slides:
            if slide_data['type'] == 'title':
                # è§£ææ¨™é¡Œé 
                lines = slide_data['content'].split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('æ¨™é¡Œï¼š') or line.startswith('Title:'):
                        main_title = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    elif line.startswith('å‰¯æ¨™é¡Œï¼š') or line.startswith('Subtitle:'):
                        subtitle = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
            else:
                # è§£æå…§å®¹é 
                content_lines = slide_data['content'].split('\n')
                slide_title = slide_data['title']
                items = []

                for line in content_lines:
                    line = line.strip()
                    if not line:
                        continue
                    if line.startswith('æ¨™é¡Œï¼š') or line.startswith('Title:'):
                        slide_title = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    elif line.startswith('å…§å®¹ï¼š') or line.startswith('Content:'):
                        continue
                    else:
                        # æ¸…ç†é …ç›®ç¬¦è™Ÿ
                        line = re.sub(r'^[â€¢\-\*]\s*', '', line)
                        if line:
                            items.append(line)

                content_slides.append({
                    'title': slide_title,
                    'items': items,
                    'notes': ''
                })

        # æ¸²æŸ“æ¨¡æ¿
        current_date = datetime.now().strftime("%Y-%m-%d")

        markdown_content = md_template.render(
            main_title=main_title,
            subtitle=subtitle,
            slides=content_slides,
            header_text=style,
            footer_text=current_date,
            authors='',
            date=current_date,
            contact=''
        )

        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        return str(output_path)

    def create_pptx(self,
                   slides: List[Dict[str, str]],
                   output_path: str,
                   title: Optional[str] = None) -> str:
        """
        å‰µå»ºPowerPointæ–‡ä»¶

        Args:
            slides: æŠ•å½±ç‰‡æ•¸æ“š
            output_path: è¼¸å‡ºè·¯å¾‘
            title: ç°¡å ±æ¨™é¡Œï¼ˆå¯é¸ï¼‰

        Returns:
            è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        """
        prs = Presentation()

        # è¨­ç½®æŠ•å½±ç‰‡å¤§å°ï¼ˆ16:9ï¼‰
        prs.slide_width = Inches(10)
        prs.slide_height = Inches(5.625)

        for slide_data in slides:
            if slide_data['type'] == 'title':
                # æ¨™é¡Œé 
                slide = prs.slides.add_slide(prs.slide_layouts[0])

                # è§£ææ¨™é¡Œå’Œå‰¯æ¨™é¡Œ
                lines = slide_data['content'].split('\n')
                main_title = ""
                subtitle = ""

                for line in lines:
                    line = line.strip()
                    if line.startswith('æ¨™é¡Œï¼š') or line.startswith('Title:'):
                        main_title = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    elif line.startswith('å‰¯æ¨™é¡Œï¼š') or line.startswith('Subtitle:'):
                        subtitle = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()

                if slide.shapes.title:
                    slide.shapes.title.text = main_title or slide_data['title']

                if len(slide.placeholders) > 1:
                    slide.placeholders[1].text = subtitle

            else:
                # å…§å®¹é 
                slide = prs.slides.add_slide(prs.slide_layouts[1])

                # è§£æå…§å®¹ä»¥æå–çœŸæ­£çš„æ¨™é¡Œ
                content_lines = slide_data['content'].split('\n')
                actual_title = slide_data['title']  # é è¨­å€¼
                content_items = []

                for line in content_lines:
                    line = line.strip()
                    if not line:
                        continue

                    # æå–æ¨™é¡Œ
                    if line.startswith('æ¨™é¡Œï¼š') or line.startswith('Title:'):
                        actual_title = line.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                    # è·³é "å…§å®¹ï¼š" æ¨™è¨˜è¡Œ
                    elif line.startswith('å…§å®¹ï¼š') or line.startswith('Content:'):
                        continue
                    # æ”¶é›†å¯¦éš›å…§å®¹
                    else:
                        content_items.append(line)

                # è¨­ç½®æ¨™é¡Œ
                if slide.shapes.title:
                    slide.shapes.title.text = actual_title

                # æ·»åŠ å…§å®¹
                if len(slide.placeholders) > 1:
                    text_frame = slide.placeholders[1].text_frame
                    text_frame.clear()

                    # å•Ÿç”¨æ–‡å­—è‡ªå‹•èª¿æ•´å’Œæ›è¡Œ
                    text_frame.word_wrap = True

                    # è¨ˆç®—ç¸½å…§å®¹é•·åº¦ä»¥æ±ºå®šå­—é«”å¤§å°
                    total_content_length = sum(len(item) for item in content_items)
                    item_count = len(content_items)

                    # æ™ºèƒ½å­—é«”å¤§å°æ±ºç­–
                    if total_content_length > 1000 or item_count > 8:
                        base_font_size = Pt(11)
                        line_spacing = 0.9
                    elif total_content_length > 800 or item_count > 6:
                        base_font_size = Pt(12)
                        line_spacing = 1.0
                    elif total_content_length > 600 or item_count > 5:
                        base_font_size = Pt(14)
                        line_spacing = 1.1
                    elif total_content_length > 400:
                        base_font_size = Pt(16)
                        line_spacing = 1.2
                    else:
                        base_font_size = Pt(18)
                        line_spacing = 1.3

                    for line in content_items:
                        # ç§»é™¤é …ç›®ç¬¦è™Ÿæ¨™è¨˜
                        line = re.sub(r'^[â€¢\-\*]\s*', '', line)
                        # ç§»é™¤ç²—é«”æ¨™è¨˜ **text**
                        line = line.replace('**', '')

                        if line:
                            p = text_frame.add_paragraph()
                            p.text = line
                            p.level = 0
                            p.font.size = base_font_size
                            p.space_before = Pt(3)
                            p.space_after = Pt(3)
                            p.line_spacing = line_spacing

        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        prs.save(str(output_path))

        return str(output_path)

    def generate_slides(self,
                       topic: str,
                       style: str = "modern_academic",
                       detail_level: str = "standard",
                       language: str = "chinese",
                       slide_count: int = 15,
                       output_path: Optional[str] = None,
                       output_format: str = "pptx",
                       pdf_content: Optional[str] = None,
                       custom_requirements: Optional[str] = None,
                       model: str = "gemma2:latest") -> Dict[str, Any]:
        """
        å®Œæ•´çš„æŠ•å½±ç‰‡ç”Ÿæˆæµç¨‹

        Args:
            topic: ç°¡å ±ä¸»é¡Œ
            style: å­¸è¡“é¢¨æ ¼
            detail_level: è©³ç´°ç¨‹åº¦
            language: èªè¨€
            slide_count: æŠ•å½±ç‰‡æ•¸é‡
            output_path: è¼¸å‡ºè·¯å¾‘
            output_format: è¼¸å‡ºæ ¼å¼ (pptx/markdown/both)
            pdf_content: PDFå…§å®¹
            custom_requirements: è‡ªè¨‚è¦æ±‚
            model: Ollamaæ¨¡å‹

        Returns:
            åŒ…å«çµæœä¿¡æ¯çš„å­—å…¸
        """
        # 1. ç”Ÿæˆæç¤ºè©
        prompt = self.generate_prompt(
            topic=topic,
            style=style,
            detail_level=detail_level,
            language=language,
            slide_count=slide_count,
            pdf_content=pdf_content,
            custom_requirements=custom_requirements
        )

        # 2. èª¿ç”¨LLMï¼ˆè‡ªå‹•é¸æ“‡å¯ç”¨çš„providerï¼‰
        print("ğŸ¤– æ­£åœ¨ç”ŸæˆæŠ•å½±ç‰‡å…§å®¹...")
        llm_output, used_provider = self.call_llm(prompt, model=model)
        print(f"âœ… ä½¿ç”¨ {used_provider} ç”Ÿæˆå®Œæˆ")

        # 3. è§£ææŠ•å½±ç‰‡
        print("ğŸ“Š æ­£åœ¨è§£ææŠ•å½±ç‰‡çµæ§‹...")
        slides = self.parse_slides(llm_output)

        if not slides:
            raise ValueError("ç„¡æ³•è§£ææŠ•å½±ç‰‡å…§å®¹ï¼Œè«‹æª¢æŸ¥LLMè¼¸å‡ºæ ¼å¼")

        # 4. ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = f"output/{topic}_{style}_{timestamp}"
        else:
            base_name = str(Path(output_path).with_suffix(''))

        output_files = []

        # ç”ŸæˆPPTX
        if output_format in ["pptx", "both"]:
            pptx_path = f"{base_name}.pptx"
            print("ğŸ’¾ æ­£åœ¨å‰µå»ºPowerPointæ–‡ä»¶...")
            pptx_file = self.create_pptx(slides, pptx_path, title=topic)
            output_files.append(pptx_file)

        # ç”ŸæˆMarkdown
        if output_format in ["markdown", "both"]:
            md_path = f"{base_name}.md"
            print("ğŸ“ æ­£åœ¨å‰µå»ºMarkdownç°¡å ±...")
            md_file = self.create_markdown(slides, md_path, title=topic, style=style)
            output_files.append(md_file)

        return {
            'success': True,
            'output_path': output_files[0] if len(output_files) == 1 else output_files,
            'output_files': output_files,
            'output_format': output_format,
            'slide_count': len(slides),
            'style': style,
            'detail_level': detail_level,
            'language': language,
            'llm_provider': used_provider,
            'llm_output': llm_output[:500]  # ä¿å­˜å‰500å­—å…ƒä½œç‚ºé è¦½
        }


# ä¾¿æ·å‡½æ•¸
def make_slides(topic: str,
               pdf_path: Optional[str] = None,
               style: str = "modern_academic",
               detail_level: str = "standard",
               language: str = "chinese",
               slide_count: int = 15,
               output_path: Optional[str] = None,
               model: str = "gemma2:latest") -> str:
    """
    ä¾¿æ·å‡½æ•¸ï¼šç”ŸæˆæŠ•å½±ç‰‡

    Args:
        topic: ç°¡å ±ä¸»é¡Œ
        pdf_path: PDFæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        style: å­¸è¡“é¢¨æ ¼
        detail_level: è©³ç´°ç¨‹åº¦
        language: èªè¨€
        slide_count: æŠ•å½±ç‰‡æ•¸é‡
        output_path: è¼¸å‡ºè·¯å¾‘
        model: Ollamaæ¨¡å‹

    Returns:
        è¼¸å‡ºPPTXæ–‡ä»¶è·¯å¾‘
    """
    maker = SlideMaker()

    # å¦‚æœæä¾›PDFè·¯å¾‘ï¼Œæå–å…§å®¹
    pdf_content = None
    if pdf_path:
        from ..extractors import PDFExtractor
        extractor = PDFExtractor(max_chars=10000)  # Journal Clubé™åˆ¶
        result = extractor.extract(pdf_path)
        pdf_content = result['full_text']

    result = maker.generate_slides(
        topic=topic,
        style=style,
        detail_level=detail_level,
        language=language,
        slide_count=slide_count,
        output_path=output_path,
        pdf_content=pdf_content,
        model=model
    )

    return result['output_path']


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    import sys

    if len(sys.argv) > 1:
        topic = sys.argv[1]
        print(f"æ¸¬è©¦ç”ŸæˆæŠ•å½±ç‰‡: {topic}")

        maker = SlideMaker()
        result = maker.generate_slides(
            topic=topic,
            style="modern_academic",
            slide_count=5  # æ¸¬è©¦ç”¨å°‘é‡æŠ•å½±ç‰‡
        )

        print(f"\nâœ… å®Œæˆï¼")
        print(f"è¼¸å‡º: {result['output_path']}")
        print(f"æŠ•å½±ç‰‡æ•¸: {result['slide_count']}")
    else:
        print("ç”¨æ³•: python slide_maker.py <ä¸»é¡Œ>")
