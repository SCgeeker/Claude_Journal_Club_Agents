#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—œä¿‚ç™¼ç¾å™¨ (Relation Finder) - Phase 2.1

è‡ªå‹•ç™¼ç¾è«–æ–‡ä¹‹é–“çš„è¤‡é›œé—œä¿‚ç¶²çµ¡ï¼š
- å¼•ç”¨é—œä¿‚ (å‘é‡ç›¸ä¼¼åº¦ + å…§å®¹åˆ†æ)
- ä¸»é¡Œé—œè¯ (é—œéµè©æ¯”å° + å‘é‡ç›¸ä¼¼åº¦)
- ä½œè€…åˆä½œ (ä½œè€…é‡ç–Š)
- ç›¸ä¼¼åº¦é—œä¿‚ (æ¨™é¡Œ/æ‘˜è¦ç›¸ä¼¼åº¦ + å‘é‡ç›¸ä¼¼åº¦)

å‡ç´šå¾Œæ”¯æŒPhase 1.5çš„å‘é‡åµŒå…¥ç³»çµ±ï¼Œæä¾›å‘é‡åŸºç¤çš„é—œä¿‚ç™¼ç¾ã€‚
å¯è¦–åŒ–è¼¸å‡ºéµå¾ªZettelkastençš„Mermaidæ ¼å¼æ¨™æº–ã€‚
"""

import sys
import io
import sqlite3
import re
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import json
from datetime import datetime

# Windows UTF-8
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


@dataclass
class Citation:
    """å¼•ç”¨é—œä¿‚ï¼ˆå‘é‡åŸºç¤ï¼‰"""
    citing_paper_id: int
    cited_paper_id: int
    citing_title: str
    cited_title: str
    similarity_score: float  # å‘é‡ç›¸ä¼¼åº¦
    confidence: str  # 'high'/'medium'/'low'
    common_concepts: List[str]
    strength: float = None  # å…¼å®¹æ€§

    def __post_init__(self):
        if self.strength is None:
            self.strength = self.similarity_score

    def __repr__(self) -> str:
        return f"Citation({self.citing_paper_id} â†’ {self.cited_paper_id}, {self.similarity_score:.2f})"


@dataclass
class CoAuthorEdge:
    """å…±åŒä½œè€…é‚Š"""
    author1: str
    author2: str
    collaboration_count: int
    shared_papers: List[int]


@dataclass
class ConceptPair:
    """æ¦‚å¿µå…±ç¾å°"""
    concept1: str
    concept2: str
    co_occurrence_count: int
    papers: List[int]
    association_strength: float


@dataclass
class Relation:
    """è«–æ–‡é—œä¿‚æ•¸æ“šçµæ§‹ï¼ˆèˆŠæ ¼å¼ï¼Œä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
    source_id: int
    target_id: int
    relation_type: str  # 'citation', 'shared_topic', 'author_collaboration', 'similarity'
    strength: float  # 0-1, é—œä¿‚å¼·åº¦
    metadata: dict  # é¡å¤–ä¿¡æ¯


class RelationFinder:
    """
    é—œä¿‚ç™¼ç¾å™¨ (Phase 2.1)

    ä½¿ç”¨å¤šç¨®ç­–ç•¥ç™¼ç¾è«–æ–‡é–“çš„è¤‡é›œé—œä¿‚ç¶²çµ¡ï¼š
    1. å¼•ç”¨åˆ†æï¼šåŸºæ–¼å‘é‡ç›¸ä¼¼åº¦ + å…§å®¹åˆ†æï¼ˆä½œè€…-å¹´ä»½æ¨¡å¼ï¼‰
    2. ä¸»é¡Œé—œè¯ï¼šè¨ˆç®—é—œéµè©äº¤é›† + å‘é‡ç›¸ä¼¼åº¦
    3. ä½œè€…åˆä½œï¼šæª¢æŸ¥å…±åŒä½œè€…
    4. ç›¸ä¼¼åº¦ï¼šè¨ˆç®—æ¨™é¡Œ/æ‘˜è¦æ–‡æœ¬ç›¸ä¼¼åº¦ + å‘é‡ç›¸ä¼¼åº¦
    5. å¯è¦–åŒ–ï¼šMermaidæ ¼å¼ï¼ˆéµå¾ªZettelkastenæ¨™æº–ï¼‰

    æ”¯æŒå‘é‡åµŒå…¥ç³»çµ±é€²è¡Œæ›´ç²¾ç¢ºçš„ç›¸ä¼¼åº¦è¨ˆç®—ã€‚
    """

    def __init__(self,
                 db_path: str = "knowledge_base/index.db",
                 embedding_manager = None,
                 config: Dict = None):
        """
        åˆå§‹åŒ–é—œä¿‚ç™¼ç¾å™¨

        Args:
            db_path: çŸ¥è­˜åº«æ•¸æ“šåº«è·¯å¾‘
            embedding_manager: EmbeddingManagerå¯¦ä¾‹ï¼ˆå¯é¸ï¼Œç”¨æ–¼å‘é‡åŸºç¤çš„åˆ†æï¼‰
            config: é…ç½®åƒæ•¸å­—å…¸
        """
        self.db_path = db_path
        self.embedding_manager = embedding_manager
        self.config = config or self._default_config()
        self.papers_cache = None  # ç·©å­˜è«–æ–‡æ•¸æ“š

    def _default_config(self) -> Dict:
        """é»˜èªé…ç½®"""
        return {
            'citation_threshold': 0.65,       # å¼•ç”¨é—œä¿‚ç›¸ä¼¼åº¦é–¾å€¼
            'co_author_min_papers': 2,        # å…±åŒä½œè€…æœ€å°‘åˆä½œè«–æ–‡æ•¸
            'concept_min_frequency': 2,       # æ¦‚å¿µæœ€å°‘å‡ºç¾æ¬¡æ•¸
            'use_embeddings': True,           # ä½¿ç”¨å‘é‡åµŒå…¥
            'mermaid_format': 'graph TD',     # Mermaidåœ–è¡¨æ ¼å¼
            'max_nodes_in_graph': 50,         # åœ–è¡¨æœ€å¤§ç¯€é»æ•¸
            'max_edges_in_graph': 100,        # åœ–è¡¨æœ€å¤§é‚Šæ•¸
        }

    def _get_connection(self) -> sqlite3.Connection:
        """ç²å–æ•¸æ“šåº«é€£æ¥"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _load_papers(self, force_reload: bool = False) -> List[Dict]:
        """
        è¼‰å…¥æ‰€æœ‰è«–æ–‡æ•¸æ“šï¼ˆå¸¶ç·©å­˜ï¼‰

        Args:
            force_reload: å¼·åˆ¶é‡æ–°è¼‰å…¥

        Returns:
            è«–æ–‡åˆ—è¡¨
        """
        if self.papers_cache is not None and not force_reload:
            return self.papers_cache

        conn = self._get_connection()
        cursor = conn.cursor()

        cursor.execute('''
            SELECT id, title, authors, year, keywords, abstract, cite_key, file_path
            FROM papers
            ORDER BY id
        ''')

        papers = []
        for row in cursor.fetchall():
            paper = dict(row)

            # è§£æJSONå­—æ®µ
            if paper['authors'] and isinstance(paper['authors'], str):
                try:
                    paper['authors'] = json.loads(paper['authors'])
                except:
                    paper['authors'] = []

            if paper['keywords'] and isinstance(paper['keywords'], str):
                try:
                    paper['keywords'] = json.loads(paper['keywords'])
                except:
                    paper['keywords'] = []

            papers.append(paper)

        conn.close()
        self.papers_cache = papers
        return papers

    def find_citation_relations(self, paper_id: int, confidence_threshold: float = 0.6) -> List[Relation]:
        """
        é€šéå…§å®¹åˆ†æç™¼ç¾å¼•ç”¨é—œä¿‚

        ç­–ç•¥ï¼š
        1. è®€å–è«–æ–‡ Markdown å…§å®¹
        2. æœç´¢å¼•ç”¨æ¨¡å¼ï¼š(Author, Year) æˆ– Author (Year)
        3. åŒ¹é…çŸ¥è­˜åº«ä¸­çš„è«–æ–‡

        Args:
            paper_id: æºè«–æ–‡ID
            confidence_threshold: ç½®ä¿¡åº¦é–¾å€¼

        Returns:
            å¼•ç”¨é—œä¿‚åˆ—è¡¨
        """
        papers = self._load_papers()
        source_paper = next((p for p in papers if p['id'] == paper_id), None)

        if not source_paper or not source_paper['file_path']:
            return []

        # è®€å–è«–æ–‡å…§å®¹
        try:
            with open(source_paper['file_path'], 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è®€å–è«–æ–‡å…§å®¹: {e}")
            return []

        relations = []

        # å¼•ç”¨æ¨¡å¼åŒ¹é…
        # æ¨¡å¼1: (Author, Year) æˆ– (Author et al., Year)
        # æ¨¡å¼2: Author (Year) æˆ– Author et al. (Year)
        citation_patterns = [
            r'\(([A-Z][a-z]+(?:\s+et\s+al\.)?),?\s+(\d{4})\)',  # (Smith, 2020)
            r'([A-Z][a-z]+(?:\s+et\s+al\.)?)\s+\((\d{4})\)',    # Smith (2020)
        ]

        cited_papers = set()

        for pattern in citation_patterns:
            matches = re.finditer(pattern, content)

            for match in matches:
                author_part = match.group(1)
                year_part = int(match.group(2))

                # æå–å§“æ°ï¼ˆè™•ç† "et al." æƒ…æ³ï¼‰
                if 'et al' in author_part:
                    author_surname = author_part.split()[0]
                else:
                    author_surname = author_part

                # åœ¨çŸ¥è­˜åº«ä¸­å°‹æ‰¾åŒ¹é…çš„è«–æ–‡
                for target_paper in papers:
                    if target_paper['id'] == paper_id:
                        continue  # è·³éè‡ªå·±

                    # åŒ¹é…å¹´ä»½
                    if target_paper['year'] != year_part:
                        continue

                    # åŒ¹é…ä½œè€…å§“æ°
                    if not target_paper['authors']:
                        continue

                    author_match = False
                    for author in target_paper['authors']:
                        if isinstance(author, str):
                            # æå–å§“æ°ï¼ˆé€šå¸¸æ˜¯æœ€å¾Œä¸€å€‹è©ï¼‰
                            surname = author.split()[-1].strip('.,')
                            if author_surname.lower() in surname.lower():
                                author_match = True
                                break

                    if author_match:
                        cited_papers.add(target_paper['id'])

        # å‰µå»ºå¼•ç”¨é—œä¿‚
        for target_id in cited_papers:
            relations.append(Relation(
                source_id=paper_id,
                target_id=target_id,
                relation_type='citation',
                strength=0.8,  # é«˜ç½®ä¿¡åº¦ï¼ˆç›´æ¥åŒ¹é…ä½œè€…-å¹´ä»½ï¼‰
                metadata={
                    'method': 'content_analysis',
                    'pattern_matched': True
                }
            ))

        return relations

    def find_citations_by_embedding(self,
                                   threshold: float = None,
                                   source_papers: List[int] = None,
                                   max_results: int = None) -> List[Citation]:
        """
        åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦æ¨æ¸¬å¼•ç”¨é—œä¿‚ï¼ˆPhase 1.5æ•´åˆç‰ˆæœ¬ï¼‰

        ä½¿ç”¨embeddingç³»çµ±è¨ˆç®—è«–æ–‡ç›¸ä¼¼åº¦ï¼Œæ•ˆæœå„ªæ–¼å‚³çµ±æ–¹æ³•ã€‚

        Args:
            threshold: ç›¸ä¼¼åº¦é–¾å€¼ (0-1)ï¼Œé»˜èªä½¿ç”¨configä¸­çš„å€¼
            source_papers: åƒ…åˆ†æé€™äº›è«–æ–‡çš„å¼•ç”¨ï¼ˆæ•´æ•¸IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            max_results: é™åˆ¶è¿”å›çµæœæ•¸é‡

        Returns:
            List[Citation]: æŒ‰ç›¸ä¼¼åº¦æ’åºçš„å¼•ç”¨é—œä¿‚åˆ—è¡¨
        """
        if not self.embedding_manager:
            print("âš ï¸  EmbeddingManageræœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å…§å®¹åˆ†æç‰ˆæœ¬")
            return []

        threshold = threshold or self.config['citation_threshold']
        papers = self._load_papers()

        try:
            # æ­¥é©Ÿ1: åŠ è¼‰è«–æ–‡å‘é‡
            paper_ids = [p['id'] for p in papers]
            embeddings = self.embedding_manager.get_paper_embeddings(paper_ids)

            if embeddings is None:
                print("âš ï¸  ç„¡æ³•ç²å–å‘é‡åµŒå…¥ï¼Œä½¿ç”¨å…§å®¹åˆ†æç‰ˆæœ¬")
                return []

            # æ­¥é©Ÿ2: è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            from sklearn.metrics.pairwise import cosine_similarity
            similarity_matrix = cosine_similarity(embeddings)

            # æ­¥é©Ÿ3: æå–å¼•ç”¨é—œä¿‚
            citations = []

            for i, source_paper in enumerate(papers):
                if source_papers and source_paper['id'] not in source_papers:
                    continue

                for j, target_paper in enumerate(papers):
                    if i == j:
                        continue  # è·³éè‡ªå·±

                    sim_score = float(similarity_matrix[i][j])

                    # éæ¿¾æ¥µé«˜ç›¸ä¼¼åº¦ï¼ˆèªç‚ºæ˜¯é‡è¤‡è«–æ–‡ï¼‰
                    if sim_score > 0.95:
                        continue

                    # æ‡‰ç”¨ç›¸ä¼¼åº¦é–¾å€¼
                    if sim_score < threshold:
                        continue

                    # æå–å…±åŒæ¦‚å¿µ
                    common_concepts = self._extract_common_concepts(source_paper, target_paper)

                    # ç¢ºå®šç½®ä¿¡åº¦
                    confidence = self._get_confidence_level(sim_score)

                    citation = Citation(
                        citing_paper_id=source_paper['id'],
                        cited_paper_id=target_paper['id'],
                        citing_title=source_paper.get('title', f"Paper {source_paper['id']}")[:60],
                        cited_title=target_paper.get('title', f"Paper {target_paper['id']}")[:60],
                        similarity_score=sim_score,
                        confidence=confidence,
                        common_concepts=common_concepts,
                    )
                    citations.append(citation)

            # æ­¥é©Ÿ4: æ’åº
            citations = sorted(citations, key=lambda x: x.similarity_score, reverse=True)

            # æ­¥é©Ÿ5: é™åˆ¶çµæœ
            if max_results:
                citations = citations[:max_results]

            return citations

        except Exception as e:
            print(f"âŒ å‘é‡åŸºç¤å¼•ç”¨ç™¼ç¾å¤±æ•—: {e}")
            return []

    def _get_confidence_level(self, similarity_score: float) -> str:
        """æ ¹æ“šç›¸ä¼¼åº¦ç¢ºå®šç½®ä¿¡åº¦"""
        if similarity_score >= 0.80:
            return 'high'
        elif similarity_score >= 0.70:
            return 'medium'
        else:
            return 'low'

    def _extract_common_concepts(self, paper1: Dict, paper2: Dict) -> List[str]:
        """æå–å…©ç¯‡è«–æ–‡çš„å…±åŒæ¦‚å¿µ"""
        keywords1 = set(paper1.get('keywords', []))
        keywords2 = set(paper2.get('keywords', []))

        if isinstance(keywords1, str):
            keywords1 = set(k.strip() for k in keywords1.split(',') if k.strip())
        if isinstance(keywords2, str):
            keywords2 = set(k.strip() for k in keywords2.split(',') if k.strip())

        common = keywords1 & keywords2
        return list(common)[:5]  # è¿”å›æœ€å¤š5å€‹å…±åŒæ¦‚å¿µ

    def find_co_authors(self,
                       min_papers: int = None,
                       include_metadata: bool = True) -> Dict:
        """
        æ§‹å»ºå®Œæ•´çš„å…±åŒä½œè€…ç¶²çµ¡

        Args:
            min_papers: æœ€å°‘å…±åŒè«–æ–‡æ•¸ï¼ˆé»˜èªä½¿ç”¨configï¼‰
            include_metadata: æ˜¯å¦åŒ…å«è©³ç´°å…ƒæ•¸æ“š

        Returns:
            Dict: åŒ…å«ä½œè€…ç¯€é»ã€é‚Šå’Œçµ±è¨ˆä¿¡æ¯çš„ç¶²çµ¡
        """
        min_papers = min_papers or self.config.get('co_author_min_papers', 2)
        papers = self._load_papers()
        author_papers = {}
        author_metadata = {}

        # æ­¥é©Ÿ1: æå–æ‰€æœ‰ä½œè€…åŠå…¶è«–æ–‡
        for paper in papers:
            authors = paper.get('authors', [])
            if isinstance(authors, str):
                authors = json.loads(authors) if authors else []
            if not authors:
                authors = []

            for author in authors:
                author_lower = author.lower() if isinstance(author, str) else str(author).lower()

                if author_lower not in author_papers:
                    author_papers[author_lower] = []
                    author_metadata[author_lower] = {
                        'name': author,
                        'papers': [],
                        'paper_ids': []
                    }

                author_papers[author_lower].append(paper['id'])
                author_metadata[author_lower]['papers'].append(paper)
                author_metadata[author_lower]['paper_ids'].append(paper['id'])

        # æ­¥é©Ÿ2: è¨ˆç®—å…±åŒä½œè€…å’Œå”ä½œé‚Š
        edges = []
        edge_set = set()
        author_list = sorted(author_papers.keys())

        for i, author1_key in enumerate(author_list):
            for author2_key in author_list[i+1:]:
                shared_papers = set(author_papers[author1_key]) & set(author_papers[author2_key])

                if len(shared_papers) >= min_papers:
                    edge_key = tuple(sorted([author1_key, author2_key]))

                    if edge_key not in edge_set:
                        edge = CoAuthorEdge(
                            author1=author_metadata[author1_key]['name'],
                            author2=author_metadata[author2_key]['name'],
                            collaboration_count=len(shared_papers),
                            shared_papers=sorted(list(shared_papers))
                        )
                        edges.append(edge)
                        edge_set.add(edge_key)

        # æ­¥é©Ÿ3: æ§‹å»ºç¯€é»æ•¸æ“š
        nodes = []
        for author_key in author_list:
            node = {
                'name': author_metadata[author_key]['name'],
                'paper_count': len(author_papers[author_key]),
                'paper_ids': author_metadata[author_key]['paper_ids']
            }

            if include_metadata:
                node['years'] = sorted(set(p.get('year') for p in author_metadata[author_key]['papers'] if p.get('year')))
                node['keywords'] = []
                for paper in author_metadata[author_key]['papers']:
                    keywords = paper.get('keywords', [])
                    if isinstance(keywords, str):
                        keywords = [k.strip() for k in keywords.split(',')]
                    node['keywords'].extend(keywords)
                node['keywords'] = sorted(list(set(node['keywords'])))[:10]  # Top 10

            nodes.append(node)

        # æ­¥é©Ÿ4: è¨ˆç®—ç¶²çµ¡çµ±è¨ˆ
        return {
            'nodes': nodes,
            'edges': [asdict(e) for e in sorted(edges, key=lambda x: x.collaboration_count, reverse=True)],
            'metadata': {
                'total_authors': len(nodes),
                'total_collaborations': len(edges),
                'max_collaboration': max([e.collaboration_count for e in edges], default=0),
                'avg_collaboration': sum(e.collaboration_count for e in edges) / len(edges) if edges else 0,
            }
        }

    def find_shared_topic_relations(self, paper_id: int, min_shared_keywords: int = 2) -> List[Relation]:
        """
        é€šéé—œéµè©é‡ç–Šç™¼ç¾ä¸»é¡Œé—œè¯

        Args:
            paper_id: è«–æ–‡ID
            min_shared_keywords: æœ€å°‘å…±äº«é—œéµè©æ•¸

        Returns:
            ä¸»é¡Œé—œè¯åˆ—è¡¨
        """
        papers = self._load_papers()
        source_paper = next((p for p in papers if p['id'] == paper_id), None)

        if not source_paper or not source_paper['keywords']:
            return []

        source_keywords = set(kw.lower() for kw in source_paper['keywords'])
        relations = []

        for target_paper in papers:
            if target_paper['id'] == paper_id:
                continue

            if not target_paper['keywords']:
                continue

            target_keywords = set(kw.lower() for kw in target_paper['keywords'])
            shared = source_keywords & target_keywords

            if len(shared) >= min_shared_keywords:
                # è¨ˆç®—Jaccardç›¸ä¼¼åº¦
                union = source_keywords | target_keywords
                jaccard = len(shared) / len(union)

                relations.append(Relation(
                    source_id=paper_id,
                    target_id=target_paper['id'],
                    relation_type='shared_topic',
                    strength=jaccard,
                    metadata={
                        'shared_keywords': list(shared),
                        'keyword_count': len(shared)
                    }
                ))

        return sorted(relations, key=lambda r: r.strength, reverse=True)

    def find_co_occurrence(self,
                          min_frequency: int = None,
                          top_k: int = None) -> Dict:
        """
        å®Œæ•´çš„æ¦‚å¿µå…±ç¾åˆ†æ

        Args:
            min_frequency: æœ€å°‘å…±ç¾æ¬¡æ•¸ï¼ˆé»˜èªä½¿ç”¨configï¼‰
            top_k: è¿”å›æœ€å¸¸è¦‹çš„æ¦‚å¿µå°æ•¸

        Returns:
            Dict: åŒ…å«æ¦‚å¿µå°ã€çµ±è¨ˆå’Œç¶²çµ¡ä¿¡æ¯
        """
        min_frequency = min_frequency or self.config.get('concept_min_frequency', 2)
        papers = self._load_papers()
        concept_papers = {}
        concept_freq = {}

        # æ­¥é©Ÿ1: æå–æ‰€æœ‰æ¦‚å¿µåŠå…¶è«–æ–‡
        for paper in papers:
            concepts = paper.get('keywords', [])

            if isinstance(concepts, str):
                concepts = [c.strip() for c in concepts.split(',') if c.strip()]
            elif concepts is None:
                concepts = []

            for concept in concepts:
                concept_lower = concept.lower()

                if concept_lower not in concept_papers:
                    concept_papers[concept_lower] = []
                    concept_freq[concept_lower] = 0

                concept_papers[concept_lower].append(paper['id'])
                concept_freq[concept_lower] += 1

        # æ­¥é©Ÿ2: è¨ˆç®—æ¦‚å¿µå…±ç¾å’Œé—œè¯å¼·åº¦
        pairs = []
        concept_list = sorted(concept_papers.keys())

        for i, concept1_key in enumerate(concept_list):
            for concept2_key in concept_list[i+1:]:
                shared_papers = set(concept_papers[concept1_key]) & set(concept_papers[concept2_key])

                if len(shared_papers) >= min_frequency:
                    # è¨ˆç®—é—œè¯å¼·åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
                    union = set(concept_papers[concept1_key]) | set(concept_papers[concept2_key])
                    jaccard = len(shared_papers) / len(union) if union else 0

                    pair = ConceptPair(
                        concept1=concept1_key,
                        concept2=concept2_key,
                        co_occurrence_count=len(shared_papers),
                        papers=sorted(list(shared_papers)),
                        association_strength=jaccard
                    )
                    pairs.append(pair)

        # æ­¥é©Ÿ3: æ’åºå’Œé™åˆ¶
        pairs = sorted(pairs, key=lambda x: x.co_occurrence_count, reverse=True)
        if top_k:
            pairs = pairs[:top_k]

        # æ­¥é©Ÿ4: è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        return {
            'pairs': [asdict(p) for p in pairs],
            'concept_frequency': sorted(
                [(c, freq) for c, freq in concept_freq.items()],
                key=lambda x: x[1],
                reverse=True
            ),
            'metadata': {
                'total_concepts': len(concept_freq),
                'total_pairs': len(pairs),
                'max_frequency': max(concept_freq.values()) if concept_freq else 0,
                'avg_frequency': sum(concept_freq.values()) / len(concept_freq) if concept_freq else 0,
            }
        }

    def find_author_collaboration_relations(self, paper_id: int) -> List[Relation]:
        """
        é€šéå…±åŒä½œè€…ç™¼ç¾åˆä½œé—œä¿‚

        Args:
            paper_id: è«–æ–‡ID

        Returns:
            ä½œè€…åˆä½œé—œä¿‚åˆ—è¡¨
        """
        papers = self._load_papers()
        source_paper = next((p for p in papers if p['id'] == paper_id), None)

        if not source_paper or not source_paper['authors']:
            return []

        source_authors = set(a.lower() for a in source_paper['authors'])
        relations = []

        for target_paper in papers:
            if target_paper['id'] == paper_id:
                continue

            if not target_paper['authors']:
                continue

            target_authors = set(a.lower() for a in target_paper['authors'])
            shared_authors = source_authors & target_authors

            if shared_authors:
                # è¨ˆç®—ä½œè€…é‡ç–Šç‡
                overlap_ratio = len(shared_authors) / max(len(source_authors), len(target_authors))

                relations.append(Relation(
                    source_id=paper_id,
                    target_id=target_paper['id'],
                    relation_type='author_collaboration',
                    strength=overlap_ratio,
                    metadata={
                        'shared_authors': list(shared_authors),
                        'author_count': len(shared_authors)
                    }
                ))

        return sorted(relations, key=lambda r: r.strength, reverse=True)

    def find_similarity_relations(self, paper_id: int, similarity_threshold: float = 0.3) -> List[Relation]:
        """
        é€šéæ¨™é¡Œç›¸ä¼¼åº¦ç™¼ç¾ç›¸é—œè«–æ–‡

        ä½¿ç”¨ç°¡å–®çš„è©å½™é‡ç–Šè¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆæœªä¾†å¯å‡ç´šç‚ºå‘é‡ç›¸ä¼¼åº¦ï¼‰

        Args:
            paper_id: è«–æ–‡ID
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼

        Returns:
            ç›¸ä¼¼é—œä¿‚åˆ—è¡¨
        """
        papers = self._load_papers()
        source_paper = next((p for p in papers if p['id'] == paper_id), None)

        if not source_paper or not source_paper['title']:
            return []

        # æ¨™é¡Œåˆ†è©ï¼ˆç°¡å–®ç©ºæ ¼åˆ†å‰²ï¼Œè½‰å°å¯«ï¼‰
        source_words = set(source_paper['title'].lower().split())
        # ç§»é™¤å¸¸è¦‹åœç”¨è©
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        source_words = source_words - stop_words

        relations = []

        for target_paper in papers:
            if target_paper['id'] == paper_id:
                continue

            if not target_paper['title']:
                continue

            target_words = set(target_paper['title'].lower().split())
            target_words = target_words - stop_words

            # è¨ˆç®—Jaccardç›¸ä¼¼åº¦
            shared = source_words & target_words
            union = source_words | target_words

            if len(union) == 0:
                continue

            jaccard = len(shared) / len(union)

            if jaccard >= similarity_threshold:
                relations.append(Relation(
                    source_id=paper_id,
                    target_id=target_paper['id'],
                    relation_type='similarity',
                    strength=jaccard,
                    metadata={
                        'shared_words': list(shared),
                        'word_count': len(shared),
                        'method': 'title_jaccard'
                    }
                ))

        return sorted(relations, key=lambda r: r.strength, reverse=True)

    def find_all_relations(self, paper_id: int) -> Dict[str, List[Relation]]:
        """
        ç™¼ç¾æ‰€æœ‰é¡å‹çš„é—œä¿‚

        Args:
            paper_id: è«–æ–‡ID

        Returns:
            é—œä¿‚å­—å…¸ï¼š{relation_type: [relations]}
        """
        return {
            'citation': self.find_citation_relations(paper_id),
            'shared_topic': self.find_shared_topic_relations(paper_id),
            'author_collaboration': self.find_author_collaboration_relations(paper_id),
            'similarity': self.find_similarity_relations(paper_id),
        }

    def build_citation_network(self, paper_ids: Optional[List[int]] = None) -> Dict:
        """
        æ§‹å»ºå¼•ç”¨ç¶²çµ¡

        Args:
            paper_ids: è«–æ–‡IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰è«–æ–‡ï¼‰

        Returns:
            ç¶²çµ¡æ•¸æ“šï¼š{nodes: [], edges: []}
        """
        papers = self._load_papers()

        if paper_ids is None:
            paper_ids = [p['id'] for p in papers]

        nodes = []
        edges = []
        edge_set = set()  # å»é‡

        # æ§‹å»ºç¯€é»
        for paper in papers:
            if paper['id'] in paper_ids:
                nodes.append({
                    'id': paper['id'],
                    'label': paper['title'][:50] if paper['title'] else f"Paper {paper['id']}",
                    'title': paper['title'],
                    'year': paper['year'],
                    'cite_key': paper['cite_key'],
                })

        # æ§‹å»ºé‚Šï¼ˆå¼•ç”¨é—œä¿‚ï¼‰
        for paper_id in paper_ids:
            relations = self.find_citation_relations(paper_id)

            for rel in relations:
                if rel.target_id in paper_ids:
                    edge_key = (rel.source_id, rel.target_id)
                    if edge_key not in edge_set:
                        edges.append({
                            'source': rel.source_id,
                            'target': rel.target_id,
                            'type': rel.relation_type,
                            'strength': rel.strength
                        })
                        edge_set.add(edge_key)

        return {
            'nodes': nodes,
            'edges': edges,
            'metadata': {
                'total_nodes': len(nodes),
                'total_edges': len(edges),
                'paper_ids': paper_ids
            }
        }

    def export_to_networkx(self, network_data: Dict):
        """
        è½‰æ›ç‚ºNetworkXåœ–å°è±¡

        Args:
            network_data: build_citation_network()çš„è¼¸å‡º

        Returns:
            NetworkX DiGraphå°è±¡
        """
        try:
            import networkx as nx
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£ networkx: pip install networkx")

        G = nx.DiGraph()

        # æ·»åŠ ç¯€é»
        for node in network_data['nodes']:
            G.add_node(node['id'], **node)

        # æ·»åŠ é‚Š
        for edge in network_data['edges']:
            G.add_edge(edge['source'], edge['target'],
                      type=edge['type'],
                      strength=edge['strength'])

        return G

    def export_to_json(self, network_data: Dict, output_path: str):
        """
        å°å‡ºç‚ºJSONæ ¼å¼

        Args:
            network_data: ç¶²çµ¡æ•¸æ“š
            output_path: è¼¸å‡ºè·¯å¾‘
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(network_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç¶²çµ¡æ•¸æ“šå·²å°å‡ºåˆ°: {output_path}")

    def export_to_graphml(self, G, output_path: str):
        """
        å°å‡ºç‚ºGraphMLæ ¼å¼ï¼ˆå¯ç”¨æ–¼Gephiç­‰å·¥å…·ï¼‰

        Args:
            G: NetworkXåœ–å°è±¡
            output_path: è¼¸å‡ºè·¯å¾‘
        """
        try:
            import networkx as nx
            nx.write_graphml(G, output_path)
            print(f"âœ… GraphMLå·²å°å‡ºåˆ°: {output_path}")
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£ networkx: pip install networkx")

    # ============== Mermaid å¯è¦–åŒ–ï¼ˆ Phase 2.1æ–°å¢ï¼‰==============

    def export_citations_to_mermaid(self,
                                    citations: List[Citation],
                                    output_path: str = None,
                                    max_edges: int = None) -> str:
        """
        å°‡å¼•ç”¨é—œä¿‚å°å‡ºç‚ºMermaidæ ¼å¼ï¼ˆZettelkastenæ¨™æº–ï¼‰

        æ ¼å¼åƒè€ƒï¼šoutput/zettelkasten_notes/zettel_index.md

        Args:
            citations: Citationç‰©ä»¶åˆ—è¡¨
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¦‚æœNoneï¼Œè¿”å›Mermaidä»£ç¢¼ï¼‰
            max_edges: æœ€å¤§é‚Šæ•¸ï¼ˆé¿å…åœ–è¡¨éæ–¼è¤‡é›œï¼‰

        Returns:
            str: Mermaidä»£ç¢¼æˆ–æª”æ¡ˆè·¯å¾‘
        """
        max_edges = max_edges or self.config.get('max_edges_in_graph', 100)

        # é™åˆ¶é‚Šæ•¸
        citations = sorted(citations, key=lambda x: x.similarity_score, reverse=True)[:max_edges]

        # æ§‹å»ºMermaidä»£ç¢¼
        lines = []
        lines.append("```mermaid")
        lines.append("graph TD")
        lines.append("")

        # æ·»åŠ ç¯€é»ï¼ˆå»é‡ï¼‰
        node_ids = set()
        for citation in citations:
            node_ids.add(citation.citing_paper_id)
            node_ids.add(citation.cited_paper_id)

        papers = {p['id']: p for p in self._load_papers()}

        for paper_id in sorted(node_ids):
            if paper_id in papers:
                title = papers[paper_id].get('title', f"Paper {paper_id}")
                # æ¨™é¡Œé•·åº¦é™åˆ¶
                title = title[:50] if len(title) > 50 else title
                lines.append(f'    P{paper_id}["{title}"]')

        lines.append("")

        # æ·»åŠ é‚Šï¼ˆæ ¹æ“šconfidenceæ±ºå®šç·šå‹ï¼‰
        for citation in citations:
            if citation.confidence == 'high':
                # å¯¦ç·šï¼šé«˜ç½®ä¿¡åº¦
                lines.append(f'    P{citation.citing_paper_id} --> P{citation.cited_paper_id}')
            else:
                # è™›ç·šï¼šä¸­/ä½ç½®ä¿¡åº¦
                lines.append(f'    P{citation.citing_paper_id} -.-> P{citation.cited_paper_id}')

        lines.append("")
        lines.append("```")

        mermaid_code = '\n'.join(lines)

        # è¼¸å‡ºæª”æ¡ˆæˆ–è¿”å›ä»£ç¢¼
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(mermaid_code)
            print(f"âœ… Mermaidåœ–è¡¨å·²å°å‡ºåˆ°: {output_path}")
            return output_path
        else:
            return mermaid_code

    def export_coauthor_network_to_mermaid(self,
                                          network_data: Dict = None,
                                          output_path: str = None,
                                          max_nodes: int = None) -> str:
        """
        å°‡å…±åŒä½œè€…ç¶²çµ¡å°å‡ºç‚ºMermaidæ ¼å¼

        Args:
            network_data: å…±åŒä½œè€…ç¶²çµ¡æ•¸æ“šï¼ˆå¦‚æœNoneï¼Œè‡ªå‹•ç”Ÿæˆï¼‰
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            max_nodes: æœ€å¤§ç¯€é»æ•¸

        Returns:
            str: Mermaidä»£ç¢¼æˆ–æª”æ¡ˆè·¯å¾‘
        """
        max_nodes = max_nodes or self.config.get('max_nodes_in_graph', 50)

        # ç”Ÿæˆå…±åŒä½œè€…ç¶²çµ¡
        if network_data is None:
            network_data = self._build_coauthor_network()

        lines = []
        lines.append("```mermaid")
        lines.append("graph TD")
        lines.append('    subgraph Authors["å…±åŒä½œè€…ç¶²çµ¡"]')

        # æ·»åŠ ä½œè€…ç¯€é»ï¼ˆé™åˆ¶æ•¸é‡ï¼‰
        author_edges = network_data.get('edges', [])
        author_nodes = set()

        for edge in author_edges[:max_nodes]:
            author_nodes.add(edge['author1'])
            author_nodes.add(edge['author2'])

        for i, author in enumerate(list(author_nodes)[:max_nodes]):
            # è¨ˆç®—è©²ä½œè€…çš„è«–æ–‡æ•¸
            author_papers = []
            for edge in author_edges:
                if edge['author1'] == author:
                    author_papers.extend(edge['shared_papers'])
                elif edge['author2'] == author:
                    author_papers.extend(edge['shared_papers'])

            paper_count = len(set(author_papers))
            # ç°¡åŒ–ä½œè€…åç¨±
            short_name = author.split(',')[0][:20] if ',' in author else author[:20]
            node_id = f"A{i}"

            lines.append(f'        {node_id}["{short_name} ({paper_count}ç¯‡)"]')

        lines.append('    end')
        lines.append("")

        # æ·»åŠ é‚Šï¼ˆä½œè€…å”ä½œï¼‰
        for i, edge in enumerate(author_edges[:max_nodes]):
            if i > 0:  # é™åˆ¶é‚Šæ•¸
                break

            author_idx1 = list(author_nodes).index(edge['author1']) if edge['author1'] in author_nodes else None
            author_idx2 = list(author_nodes).index(edge['author2']) if edge['author2'] in author_nodes else None

            if author_idx1 is not None and author_idx2 is not None:
                lines.append(f'    A{author_idx1} --> A{author_idx2}')

        lines.append("")
        lines.append("```")

        mermaid_code = '\n'.join(lines)

        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(mermaid_code)
            print(f"âœ… å…±åŒä½œè€…Mermaidåœ–è¡¨å·²å°å‡ºåˆ°: {output_path}")
            return output_path
        else:
            return mermaid_code

    def _build_coauthor_network(self) -> Dict:
        """æ§‹å»ºå…±åŒä½œè€…ç¶²çµ¡æ•¸æ“š"""
        papers = self._load_papers()
        author_papers = {}

        # æå–æ‰€æœ‰ä½œè€…åŠå…¶è«–æ–‡
        for paper in papers:
            authors = paper.get('authors', [])
            if not authors:
                continue

            for author in authors:
                if author not in author_papers:
                    author_papers[author] = []
                author_papers[author].append(paper['id'])

        # è¨ˆç®—å…±åŒä½œè€…
        edges = []
        author_list = list(author_papers.keys())

        for i, author1 in enumerate(author_list):
            for author2 in author_list[i+1:]:
                shared_papers = set(author_papers[author1]) & set(author_papers[author2])
                if len(shared_papers) >= self.config.get('co_author_min_papers', 1):
                    edges.append({
                        'author1': author1,
                        'author2': author2,
                        'collaboration_count': len(shared_papers),
                        'shared_papers': list(shared_papers)
                    })

        return {
            'authors': author_list,
            'edges': edges,
            'total_authors': len(author_list),
            'total_collaborations': len(edges)
        }

    def export_concepts_to_mermaid(self,
                                  concept_pairs: List[ConceptPair] = None,
                                  output_path: str = None,
                                  max_pairs: int = None) -> str:
        """
        å°‡æ¦‚å¿µå…±ç¾å°å‡ºç‚ºMermaidæ ¼å¼

        Args:
            concept_pairs: ConceptPairç‰©ä»¶åˆ—è¡¨ï¼ˆå¦‚æœNoneï¼Œè‡ªå‹•ç”Ÿæˆï¼‰
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            max_pairs: æœ€å¤§æ¦‚å¿µå°æ•¸

        Returns:
            str: Mermaidä»£ç¢¼æˆ–æª”æ¡ˆè·¯å¾‘
        """
        max_pairs = max_pairs or self.config.get('max_edges_in_graph', 50)

        # ç”Ÿæˆæ¦‚å¿µå°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if concept_pairs is None:
            concept_pairs = self._extract_concept_pairs()

        # é™åˆ¶å°æ•¸
        concept_pairs = sorted(concept_pairs,
                             key=lambda x: x.co_occurrence_count,
                             reverse=True)[:max_pairs]

        lines = []
        lines.append("```mermaid")
        lines.append("graph TD")
        lines.append("")

        # æ·»åŠ ç¯€é»ï¼ˆå»é‡ï¼‰
        concepts = set()
        for pair in concept_pairs:
            concepts.add(pair.concept1)
            concepts.add(pair.concept2)

        # ç¯€é»å‘½åï¼ˆä½¿ç”¨å“ˆå¸Œï¼‰
        concept_to_node = {}
        for concept in sorted(concepts):
            node_id = f"C{hash(concept) % 10000}"
            concept_to_node[concept] = node_id
            lines.append(f'    {node_id}["{concept}"]')

        lines.append("")

        # æ·»åŠ é‚Šï¼ˆæ¦‚å¿µå…±ç¾ï¼‰
        for pair in concept_pairs:
            node1 = concept_to_node[pair.concept1]
            node2 = concept_to_node[pair.concept2]

            # æ ¹æ“šé—œè¯å¼·åº¦æ±ºå®šç·šå‹
            if pair.association_strength >= 0.5:
                lines.append(f'    {node1} --> {node2}')
            else:
                lines.append(f'    {node1} -.-> {node2}')

        lines.append("")
        lines.append("```")

        mermaid_code = '\n'.join(lines)

        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(mermaid_code)
            print(f"âœ… æ¦‚å¿µå…±ç¾Mermaidåœ–è¡¨å·²å°å‡ºåˆ°: {output_path}")
            return output_path
        else:
            return mermaid_code

    def _extract_concept_pairs(self) -> List[ConceptPair]:
        """æå–æ¦‚å¿µå…±ç¾å°"""
        papers = self._load_papers()
        concept_papers = {}

        # æå–æ‰€æœ‰æ¦‚å¿µåŠå…¶è«–æ–‡
        for paper in papers:
            concepts = paper.get('keywords', [])
            if isinstance(concepts, str):
                concepts = [c.strip() for c in concepts.split(',') if c.strip()]
            elif concepts is None:
                concepts = []

            for concept in concepts:
                if concept not in concept_papers:
                    concept_papers[concept] = []
                concept_papers[concept].append(paper['id'])

        # è¨ˆç®—æ¦‚å¿µå…±ç¾
        pairs = []
        concept_list = list(concept_papers.keys())

        for i, concept1 in enumerate(concept_list):
            for concept2 in concept_list[i+1:]:
                shared_papers = set(concept_papers[concept1]) & set(concept_papers[concept2])

                if len(shared_papers) >= self.config.get('concept_min_frequency', 1):
                    # è¨ˆç®—é—œè¯å¼·åº¦
                    max_count = max(len(concept_papers[concept1]), len(concept_papers[concept2]))
                    strength = len(shared_papers) / max_count if max_count > 0 else 0

                    pair = ConceptPair(
                        concept1=concept1,
                        concept2=concept2,
                        co_occurrence_count=len(shared_papers),
                        papers=list(shared_papers),
                        association_strength=strength
                    )
                    pairs.append(pair)

        return sorted(pairs, key=lambda x: x.co_occurrence_count, reverse=True)


# CLIæ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    print("ğŸ” relation-finder Phase 2.1 Day 2 æ¸¬è©¦\n")

    finder = RelationFinder()
    output_dir = Path("output/relations")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ===== æ¸¬è©¦ 1: å…±åŒä½œè€…å®Œæ•´åˆ†æ =====
    print("=" * 70)
    print("æ¸¬è©¦ 1: å…±åŒä½œè€…ç¶²çµ¡å®Œæ•´åˆ†æï¼ˆDay 2æ–°å¢ï¼‰")
    print("=" * 70)

    coauthor_network = finder.find_co_authors(min_papers=1)

    print(f"\nğŸ‘¥ å…±åŒä½œè€…ç¶²çµ¡çµ±è¨ˆ:")
    print(f"   ğŸ“Š ç¸½ä½œè€…æ•¸: {coauthor_network['metadata']['total_authors']}")
    print(f"   ğŸ¤ å”ä½œå°æ•¸: {coauthor_network['metadata']['total_collaborations']}")
    print(f"   ğŸ“ˆ æœ€å¤§å”ä½œ: {coauthor_network['metadata']['max_collaboration']}ç¯‡è«–æ–‡")
    print(f"   ğŸ“‰ å¹³å‡å”ä½œ: {coauthor_network['metadata']['avg_collaboration']:.2f}ç¯‡è«–æ–‡")

    # é¡¯ç¤ºtopå”ä½œå°
    if coauthor_network['edges']:
        print(f"\nğŸ† Top 5 å”ä½œå°:")
        for i, edge in enumerate(coauthor_network['edges'][:5], 1):
            print(f"   {i}. {edge['author1']} â†” {edge['author2']}")
            print(f"      å…±åŒè«–æ–‡: {edge['collaboration_count']}ç¯‡ (ID: {edge['shared_papers'][:2]}...)")

    # å°å‡ºç‚ºJSON
    with open(output_dir / "coauthor_network.json", 'w', encoding='utf-8') as f:
        json.dump(coauthor_network, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… å…±åŒä½œè€…ç¶²çµ¡å·²å°å‡ºåˆ°: {output_dir}/coauthor_network.json")

    # ===== æ¸¬è©¦ 2: æ¦‚å¿µå…±ç¾å®Œæ•´åˆ†æ =====
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 2: æ¦‚å¿µå…±ç¾å®Œæ•´åˆ†æï¼ˆDay 2æ–°å¢ï¼‰")
    print("=" * 70)

    cooccurrence = finder.find_co_occurrence(min_frequency=1, top_k=30)

    print(f"\nğŸ“š æ¦‚å¿µå…±ç¾çµ±è¨ˆ:")
    print(f"   ğŸ“Š ç¸½æ¦‚å¿µæ•¸: {cooccurrence['metadata']['total_concepts']}")
    print(f"   ğŸ”— æ¦‚å¿µå°æ•¸: {cooccurrence['metadata']['total_pairs']}")
    print(f"   ğŸ“ˆ æœ€é«˜é »ç‡: {cooccurrence['metadata']['max_frequency']}")
    print(f"   ğŸ“‰ å¹³å‡é »ç‡: {cooccurrence['metadata']['avg_frequency']:.2f}")

    # é¡¯ç¤ºtopæ¦‚å¿µ
    if cooccurrence['concept_frequency']:
        print(f"\nâ­ Top 10 é«˜é »æ¦‚å¿µ:")
        for i, (concept, freq) in enumerate(cooccurrence['concept_frequency'][:10], 1):
            print(f"   {i}. {concept}: {freq}ç¯‡è«–æ–‡")

    # é¡¯ç¤ºtopæ¦‚å¿µå°
    if cooccurrence['pairs']:
        print(f"\nğŸ”— Top 5 æ¦‚å¿µå°:")
        for i, pair in enumerate(cooccurrence['pairs'][:5], 1):
            print(f"   {i}. '{pair['concept1']}' â†” '{pair['concept2']}'")
            print(f"      å…±ç¾: {pair['co_occurrence_count']}æ¬¡, å¼·åº¦: {pair['association_strength']:.2f}")

    # å°å‡ºç‚ºJSON
    with open(output_dir / "concept_cooccurrence.json", 'w', encoding='utf-8') as f:
        json.dump(cooccurrence, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… æ¦‚å¿µå…±ç¾å·²å°å‡ºåˆ°: {output_dir}/concept_cooccurrence.json")

    # ===== æ¸¬è©¦ 3: æ›´æ–°Mermaidå¯è¦–åŒ– =====
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 3: ä½¿ç”¨æ–°æ•¸æ“šæ›´æ–°Mermaidå¯è¦–åŒ–")
    print("=" * 70)

    # å…±åŒä½œè€…Mermaid
    print("\nğŸ‘¥ ç”Ÿæˆå…±åŒä½œè€…Mermaid...")
    finder.export_coauthor_network_to_mermaid(
        network_data=coauthor_network,
        output_path=output_dir / "coauthor_network.md"
    )

    # æ¦‚å¿µå…±ç¾Mermaid
    print("ğŸ“š ç”Ÿæˆæ¦‚å¿µå…±ç¾Mermaid...")
    concept_pairs = [ConceptPair(**p) for p in cooccurrence['pairs']]
    finder.export_concepts_to_mermaid(
        concept_pairs=concept_pairs,
        output_path=output_dir / "concept_cooccurrence.md"
    )

    # ===== æ¸¬è©¦ 4: å‚³çµ±é—œä¿‚åˆ†æ =====
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ 4: å‚³çµ±å¼•ç”¨é—œä¿‚åˆ†æï¼ˆDay 1åŠŸèƒ½é©—è­‰ï¼‰")
    print("=" * 70)

    paper_id = 2
    print(f"\nğŸ“„ è«–æ–‡ ID {paper_id} çš„é—œä¿‚:")

    all_relations = finder.find_all_relations(paper_id)

    for rel_type, relations in all_relations.items():
        if relations:
            print(f"\nğŸ”— {rel_type.upper()} ({len(relations)}å€‹)")
            for rel in relations[:3]:
                print(f"   â†’ Paper {rel.target_id} (å¼·åº¦: {rel.strength:.2f})")

    print("\n" + "=" * 70)
    print("âœ… Phase 2.1 Day 2 æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ è¼¸å‡ºæª”æ¡ˆ:")
    print(f"   âœ… {output_dir}/coauthor_network.json")
    print(f"   âœ… {output_dir}/coauthor_network.md (Mermaid)")
    print(f"   âœ… {output_dir}/concept_cooccurrence.json")
    print(f"   âœ… {output_dir}/concept_cooccurrence.md (Mermaid)")
    print(f"\nğŸ“Š æ–°å¢åŠŸèƒ½:")
    print(f"   âœ¨ find_co_authors() - å…±åŒä½œè€…ç¶²çµ¡ï¼ˆå«çµ±è¨ˆï¼‰")
    print(f"   âœ¨ find_co_occurrence() - æ¦‚å¿µå…±ç¾åˆ†æï¼ˆå«çµ±è¨ˆï¼‰")
