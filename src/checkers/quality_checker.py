#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è«–æ–‡è³ªé‡æª¢æŸ¥å™¨
æª¢æŸ¥çŸ¥è­˜åº«ä¸­è«–æ–‡çš„å…ƒæ•¸æ“šè³ªé‡ä¸¦æä¾›ä¿®å¾©å»ºè­°
"""

import re
import json
import yaml
import requests
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from difflib import SequenceMatcher
import sys

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from knowledge_base import KnowledgeBaseManager


@dataclass
class QualityIssue:
    """è³ªé‡å•é¡Œ"""
    field: str  # title, authors, year, abstract, keywords
    severity: str  # critical, warning, info
    issue_type: str  # invalid_pattern, missing, suspicious, format_error
    description: str
    current_value: Any
    suggested_fix: Optional[Any] = None
    auto_fixable: bool = False


@dataclass
class QualityReport:
    """è³ªé‡æª¢æŸ¥å ±å‘Š"""
    paper_id: int
    title: str
    overall_score: float = 0.0
    quality_level: str = "unknown"  # excellent, good, acceptable, poor, critical
    issues: List[QualityIssue] = field(default_factory=list)
    metadata_scores: Dict[str, float] = field(default_factory=dict)
    checked_at: str = field(default_factory=lambda: datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

    def get_critical_issues(self) -> List[QualityIssue]:
        """ç²å–åš´é‡å•é¡Œ"""
        return [issue for issue in self.issues if issue.severity == "critical"]

    def get_warnings(self) -> List[QualityIssue]:
        """ç²å–è­¦å‘Š"""
        return [issue for issue in self.issues if issue.severity == "warning"]

    def get_info(self) -> List[QualityIssue]:
        """ç²å–ä¿¡æ¯"""
        return [issue for issue in self.issues if issue.severity == "info"]

    def has_critical_issues(self) -> bool:
        """æ˜¯å¦æœ‰åš´é‡å•é¡Œ"""
        return len(self.get_critical_issues()) > 0

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "paper_id": self.paper_id,
            "title": self.title,
            "overall_score": self.overall_score,
            "quality_level": self.quality_level,
            "checked_at": self.checked_at,
            "metadata_scores": self.metadata_scores,
            "issues": {
                "critical": [
                    {
                        "field": i.field,
                        "type": i.issue_type,
                        "description": i.description,
                        "current_value": i.current_value,
                        "suggested_fix": i.suggested_fix,
                        "auto_fixable": i.auto_fixable
                    }
                    for i in self.get_critical_issues()
                ],
                "warnings": [
                    {
                        "field": i.field,
                        "type": i.issue_type,
                        "description": i.description,
                        "current_value": i.current_value
                    }
                    for i in self.get_warnings()
                ],
                "info": [
                    {
                        "field": i.field,
                        "description": i.description
                    }
                    for i in self.get_info()
                ]
            }
        }

    def to_text(self, detail_level: str = "standard") -> str:
        """è½‰æ›ç‚ºæ–‡æœ¬å ±å‘Š"""
        lines = []
        lines.append("=" * 70)
        lines.append(f"è«–æ–‡è³ªé‡æª¢æŸ¥å ±å‘Š")
        lines.append("=" * 70)
        lines.append(f"è«–æ–‡ID: {self.paper_id}")
        lines.append(f"æ¨™é¡Œ: {self.title}")
        lines.append(f"æª¢æŸ¥æ™‚é–“: {self.checked_at}")
        lines.append("")
        lines.append(f"æ•´é«”è©•åˆ†: {self.overall_score:.1f}/100")
        lines.append(f"è³ªé‡ç­‰ç´š: {self._get_quality_level_label(self.quality_level)}")
        lines.append("")

        if detail_level in ["standard", "comprehensive"]:
            lines.append("å…ƒæ•¸æ“šè©•åˆ†:")
            for field, score in self.metadata_scores.items():
                lines.append(f"  - {field}: {score:.1f}")
            lines.append("")

        critical = self.get_critical_issues()
        if critical:
            lines.append(f"ğŸš¨ åš´é‡å•é¡Œ ({len(critical)} å€‹):")
            for i, issue in enumerate(critical, 1):
                lines.append(f"  {i}. [{issue.field}] {issue.description}")
                lines.append(f"     ç•¶å‰å€¼: {issue.current_value}")
                if issue.suggested_fix and detail_level == "comprehensive":
                    lines.append(f"     å»ºè­°ä¿®å¾©: {issue.suggested_fix}")
                if issue.auto_fixable:
                    lines.append(f"     âœ… å¯è‡ªå‹•ä¿®å¾©")
            lines.append("")

        warnings = self.get_warnings()
        if warnings and detail_level in ["standard", "comprehensive"]:
            lines.append(f"âš ï¸  è­¦å‘Š ({len(warnings)} å€‹):")
            for i, issue in enumerate(warnings, 1):
                lines.append(f"  {i}. [{issue.field}] {issue.description}")
            lines.append("")

        info = self.get_info()
        if info and detail_level == "comprehensive":
            lines.append(f"â„¹ï¸  æç¤º ({len(info)} å€‹):")
            for i, issue in enumerate(info, 1):
                lines.append(f"  {i}. [{issue.field}] {issue.description}")
            lines.append("")

        lines.append("=" * 70)
        return "\n".join(lines)

    def _get_quality_level_label(self, level: str) -> str:
        """ç²å–è³ªé‡ç­‰ç´šæ¨™ç±¤"""
        labels = {
            "excellent": "å„ªç§€ â­â­â­â­â­",
            "good": "è‰¯å¥½ â­â­â­â­",
            "acceptable": "å¯æ¥å— â­â­â­",
            "poor": "è¼ƒå·® â­â­",
            "critical": "åš´é‡å•é¡Œ â­",
            "unknown": "æœªçŸ¥"
        }
        return labels.get(level, level)


class QualityChecker:
    """è«–æ–‡è³ªé‡æª¢æŸ¥å™¨"""

    def __init__(self,
                 kb_manager: Optional[KnowledgeBaseManager] = None,
                 rules_file: Optional[str] = None,
                 enable_api: bool = True):
        """
        åˆå§‹åŒ–è³ªé‡æª¢æŸ¥å™¨

        Args:
            kb_manager: çŸ¥è­˜åº«ç®¡ç†å™¨å¯¦ä¾‹
            rules_file: è¦å‰‡é…ç½®æ–‡ä»¶è·¯å¾‘
            enable_api: æ˜¯å¦å•Ÿç”¨å¤–éƒ¨API
        """
        self.kb = kb_manager or KnowledgeBaseManager()

        # è¼‰å…¥è¦å‰‡é…ç½®
        if rules_file is None:
            rules_file = Path(__file__).parent / "quality_rules.yaml"

        with open(rules_file, 'r', encoding='utf-8') as f:
            self.rules = yaml.safe_load(f)

        self.enable_api = enable_api
        self.current_year = datetime.now().year

    def check_paper(self, paper_id: int, auto_fix: bool = False) -> QualityReport:
        """
        æª¢æŸ¥å–®ç¯‡è«–æ–‡çš„è³ªé‡

        Args:
            paper_id: è«–æ–‡ID
            auto_fix: æ˜¯å¦è‡ªå‹•ä¿®å¾©å•é¡Œ

        Returns:
            è³ªé‡æª¢æŸ¥å ±å‘Š
        """
        # ç²å–è«–æ–‡æ•¸æ“š
        paper = self.kb.get_paper_by_id(paper_id)
        if not paper:
            raise ValueError(f"è«–æ–‡ ID {paper_id} ä¸å­˜åœ¨")

        # å‰µå»ºå ±å‘Š
        report = QualityReport(
            paper_id=paper_id,
            title=paper.get('title', 'Unknown')
        )

        # æª¢æŸ¥å„é …å…ƒæ•¸æ“š
        title_issues, title_score = self._check_title(paper)
        authors_issues, authors_score = self._check_authors(paper)
        year_issues, year_score = self._check_year(paper)
        abstract_issues, abstract_score = self._check_abstract(paper)
        keywords_issues, keywords_score = self._check_keywords(paper)

        # åˆä½µæ‰€æœ‰å•é¡Œ
        report.issues.extend(title_issues)
        report.issues.extend(authors_issues)
        report.issues.extend(year_issues)
        report.issues.extend(abstract_issues)
        report.issues.extend(keywords_issues)

        # è¨ˆç®—å…ƒæ•¸æ“šè©•åˆ†
        report.metadata_scores = {
            "title": title_score,
            "authors": authors_score,
            "year": year_score,
            "abstract": abstract_score,
            "keywords": keywords_score
        }

        # è¨ˆç®—æ•´é«”è©•åˆ†
        weights = self.rules['quality_scoring']['weights']
        report.overall_score = (
            title_score * weights['title'] +
            authors_score * weights['authors'] +
            year_score * weights['year'] +
            abstract_score * weights['abstract'] +
            keywords_score * weights['keywords']
        )

        # ç¢ºå®šè³ªé‡ç­‰ç´š
        report.quality_level = self._determine_quality_level(report.overall_score)

        # è‡ªå‹•ä¿®å¾©ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if auto_fix and self.rules['auto_fix']['enabled']:
            self._auto_fix_issues(paper_id, report)

        return report

    def check_all_papers(self, auto_fix: bool = False) -> List[QualityReport]:
        """
        æª¢æŸ¥æ‰€æœ‰è«–æ–‡çš„è³ªé‡

        Args:
            auto_fix: æ˜¯å¦è‡ªå‹•ä¿®å¾©å•é¡Œ

        Returns:
            è³ªé‡æª¢æŸ¥å ±å‘Šåˆ—è¡¨
        """
        papers = self.kb.list_papers(limit=1000)
        reports = []

        for paper in papers:
            try:
                report = self.check_paper(paper['id'], auto_fix=auto_fix)
                reports.append(report)
            except Exception as e:
                print(f"æª¢æŸ¥è«–æ–‡ {paper['id']} æ™‚å‡ºéŒ¯: {e}")

        return reports

    def _check_title(self, paper: Dict[str, Any]) -> Tuple[List[QualityIssue], float]:
        """æª¢æŸ¥æ¨™é¡Œè³ªé‡"""
        issues = []
        score = 100.0

        title = paper.get('title', '')
        if not title:
            title = ''

        rules = self.rules['title_checks']

        # æª¢æŸ¥ç„¡æ•ˆæ¨¡å¼
        for pattern_rule in rules['invalid_patterns']:
            if re.search(pattern_rule['pattern'], title, re.IGNORECASE):
                issues.append(QualityIssue(
                    field="title",
                    severity=pattern_rule['severity'],
                    issue_type="invalid_pattern",
                    description=pattern_rule['description'],
                    current_value=title,
                    auto_fixable=(pattern_rule.get('auto_fix') == 'extract_from_content')
                ))
                score -= 25  # åš´é‡æ‰£åˆ†

        # æª¢æŸ¥å¯ç–‘æ¨¡å¼
        for pattern_rule in rules['suspicious_patterns']:
            if re.search(pattern_rule['pattern'], title, re.IGNORECASE):
                issues.append(QualityIssue(
                    field="title",
                    severity=pattern_rule['severity'],
                    issue_type="suspicious",
                    description=pattern_rule['description'],
                    current_value=title,
                    auto_fixable=False
                ))
                if pattern_rule['severity'] == 'warning':
                    score -= 5
                else:
                    score -= 2

        # æª¢æŸ¥é•·åº¦
        metrics = rules['quality_metrics']
        if len(title) < metrics['min_length']:
            issues.append(QualityIssue(
                field="title",
                severity="critical",
                issue_type="too_short",
                description=f"æ¨™é¡ŒéçŸ­ï¼ˆ{len(title)} å­—å…ƒï¼Œæœ€å°‘ {metrics['min_length']}ï¼‰",
                current_value=title,
                auto_fixable=True
            ))
            score -= 10
        elif len(title) < metrics['preferred_min_length']:
            issues.append(QualityIssue(
                field="title",
                severity="info",
                issue_type="suboptimal_length",
                description=f"æ¨™é¡ŒåçŸ­ï¼ˆå»ºè­°è‡³å°‘ {metrics['preferred_min_length']} å­—å…ƒï¼‰",
                current_value=title,
                auto_fixable=False
            ))
            score -= 3

        if len(title) > metrics['max_length']:
            issues.append(QualityIssue(
                field="title",
                severity="warning",
                issue_type="too_long",
                description=f"æ¨™é¡Œéé•·ï¼ˆ{len(title)} å­—å…ƒï¼Œæœ€å¤š {metrics['max_length']}ï¼‰",
                current_value=title,
                auto_fixable=False
            ))
            score -= 5

        return issues, max(0, score)

    def _check_authors(self, paper: Dict[str, Any]) -> Tuple[List[QualityIssue], float]:
        """æª¢æŸ¥ä½œè€…è³ªé‡"""
        issues = []
        score = 100.0

        authors = paper.get('authors', [])
        if not authors:
            authors = []

        rules = self.rules['authors_checks']

        # æª¢æŸ¥æ˜¯å¦æœ‰ä½œè€…
        if not authors or len(authors) == 0:
            issues.append(QualityIssue(
                field="authors",
                severity="critical",
                issue_type="missing",
                description="ç¼ºå°‘ä½œè€…ä¿¡æ¯",
                current_value=authors,
                auto_fixable=True
            ))
            return issues, 0.0

        # æª¢æŸ¥æ¯å€‹ä½œè€…
        for i, author in enumerate(authors):
            # æª¢æŸ¥ç„¡æ•ˆæ¨¡å¼
            for pattern_rule in rules['invalid_patterns']:
                if re.search(pattern_rule['pattern'], author, re.IGNORECASE):
                    issues.append(QualityIssue(
                        field="authors",
                        severity=pattern_rule['severity'],
                        issue_type="invalid_pattern",
                        description=f"ä½œè€… {i+1} {pattern_rule['description']}: {author}",
                        current_value=author,
                        auto_fixable=True
                    ))
                    score -= 15

        # æª¢æŸ¥ä½œè€…æ•¸é‡
        metrics = rules['quality_metrics']
        if len(authors) > metrics['max_authors']:
            issues.append(QualityIssue(
                field="authors",
                severity="warning",
                issue_type="suspicious_count",
                description=f"ä½œè€…æ•¸é‡éå¤šï¼ˆ{len(authors)} ä½ï¼Œé€šå¸¸ä¸è¶…é {metrics['max_authors']} ä½ï¼‰",
                current_value=len(authors),
                auto_fixable=False
            ))
            score -= 5

        return issues, max(0, score)

    def _check_year(self, paper: Dict[str, Any]) -> Tuple[List[QualityIssue], float]:
        """æª¢æŸ¥å¹´ä»½è³ªé‡"""
        issues = []
        score = 100.0

        year = paper.get('year')
        rules = self.rules['year_checks']

        # æª¢æŸ¥æ˜¯å¦æœ‰å¹´ä»½
        if year is None:
            issues.append(QualityIssue(
                field="year",
                severity="critical",
                issue_type="missing",
                description="ç¼ºå°‘ç™¼è¡¨å¹´ä»½",
                current_value=None,
                auto_fixable=True
            ))
            return issues, 0.0

        # æª¢æŸ¥å¹´ä»½ç¯„åœ
        valid_range = rules['valid_range']
        if year < valid_range['min_year'] or year > valid_range['max_year']:
            issues.append(QualityIssue(
                field="year",
                severity="critical",
                issue_type="out_of_range",
                description=f"å¹´ä»½è¶…å‡ºæœ‰æ•ˆç¯„åœï¼ˆ{valid_range['min_year']}-{valid_range['max_year']}ï¼‰",
                current_value=year,
                auto_fixable=True
            ))
            score -= 20

        # æª¢æŸ¥å¯ç–‘å¹´ä»½
        for condition in rules['suspicious_conditions']:
            if condition['condition'] == 'too_old' and year < condition['threshold']:
                issues.append(QualityIssue(
                    field="year",
                    severity=condition['severity'],
                    issue_type="suspicious",
                    description=condition['description'],
                    current_value=year,
                    auto_fixable=False
                ))
                score -= 2
            elif condition['condition'] == 'future' and year > self.current_year + 2:
                issues.append(QualityIssue(
                    field="year",
                    severity=condition['severity'],
                    issue_type="suspicious",
                    description=condition['description'],
                    current_value=year,
                    auto_fixable=True
                ))
                score -= 5

        return issues, max(0, score)

    def _check_abstract(self, paper: Dict[str, Any]) -> Tuple[List[QualityIssue], float]:
        """æª¢æŸ¥æ‘˜è¦è³ªé‡"""
        issues = []
        score = 100.0

        abstract = paper.get('abstract', '')
        if not abstract:
            abstract = ''

        rules = self.rules['abstract_checks']

        # æª¢æŸ¥æ˜¯å¦ç‚ºç©º
        if not abstract or len(abstract.strip()) == 0:
            issues.append(QualityIssue(
                field="abstract",
                severity="critical",
                issue_type="missing",
                description="ç¼ºå°‘æ‘˜è¦",
                current_value=abstract,
                auto_fixable=True
            ))
            return issues, 0.0

        # æª¢æŸ¥æ˜¯å¦ç‚ºä½”ä½ç¬¦
        for pattern in rules['content_checks'][1]['patterns']:
            if re.search(pattern, abstract, re.IGNORECASE):
                issues.append(QualityIssue(
                    field="abstract",
                    severity="critical",
                    issue_type="placeholder",
                    description="æ‘˜è¦ç‚ºä½”ä½ç¬¦",
                    current_value=abstract,
                    auto_fixable=True
                ))
                return issues, 0.0

        # æª¢æŸ¥é•·åº¦
        length = rules['length_checks']
        if len(abstract) < length['min_length']:
            issues.append(QualityIssue(
                field="abstract",
                severity="critical",
                issue_type="too_short",
                description=f"æ‘˜è¦éçŸ­ï¼ˆ{len(abstract)} å­—å…ƒï¼Œæœ€å°‘ {length['min_length']}ï¼‰",
                current_value=f"{abstract[:50]}...",
                auto_fixable=True
            ))
            score -= 25
        elif len(abstract) < length['preferred_min_length']:
            issues.append(QualityIssue(
                field="abstract",
                severity="info",
                issue_type="suboptimal_length",
                description=f"æ‘˜è¦åçŸ­ï¼ˆå»ºè­°è‡³å°‘ {length['preferred_min_length']} å­—å…ƒï¼‰",
                current_value=f"{abstract[:50]}...",
                auto_fixable=False
            ))
            score -= 5

        if len(abstract) > length['max_length']:
            issues.append(QualityIssue(
                field="abstract",
                severity="warning",
                issue_type="too_long",
                description=f"æ‘˜è¦éé•·ï¼ˆ{len(abstract)} å­—å…ƒï¼Œæœ€å¤š {length['max_length']}ï¼‰",
                current_value=f"{abstract[:50]}...",
                auto_fixable=False
            ))
            score -= 5

        # æª¢æŸ¥è©æ•¸
        word_count = len(abstract.split())
        min_words = rules['content_checks'][2]['min_words']
        if word_count < min_words:
            issues.append(QualityIssue(
                field="abstract",
                severity="warning",
                issue_type="insufficient_content",
                description=f"æ‘˜è¦å…§å®¹éå°‘ï¼ˆ{word_count} å­—ï¼Œå»ºè­°è‡³å°‘ {min_words} å­—ï¼‰",
                current_value=f"{abstract[:50]}...",
                auto_fixable=False
            ))
            score -= 10

        return issues, max(0, score)

    def _check_keywords(self, paper: Dict[str, Any]) -> Tuple[List[QualityIssue], float]:
        """æª¢æŸ¥é—œéµè©è³ªé‡"""
        issues = []
        score = 100.0

        keywords = paper.get('keywords', [])
        if not keywords:
            keywords = []

        rules = self.rules['keywords_checks']

        # æª¢æŸ¥æ•¸é‡
        count = rules['count_checks']
        if len(keywords) < count['min_keywords']:
            issues.append(QualityIssue(
                field="keywords",
                severity="warning",
                issue_type="insufficient_count",
                description=f"é—œéµè©éå°‘ï¼ˆ{len(keywords)} å€‹ï¼Œå»ºè­°è‡³å°‘ {count['preferred_min']} å€‹ï¼‰",
                current_value=keywords,
                auto_fixable=True
            ))
            score -= 15
        elif len(keywords) < count['preferred_min']:
            issues.append(QualityIssue(
                field="keywords",
                severity="info",
                issue_type="suboptimal_count",
                description=f"é—œéµè©åå°‘ï¼ˆå»ºè­° {count['preferred_min']}-{count['preferred_max']} å€‹ï¼‰",
                current_value=keywords,
                auto_fixable=False
            ))
            score -= 5

        if len(keywords) > count['max_keywords']:
            issues.append(QualityIssue(
                field="keywords",
                severity="warning",
                issue_type="excessive_count",
                description=f"é—œéµè©éå¤šï¼ˆ{len(keywords)} å€‹ï¼Œå»ºè­°ä¸è¶…é {count['max_keywords']} å€‹ï¼‰",
                current_value=keywords,
                auto_fixable=False
            ))
            score -= 10

        # æª¢æŸ¥é‡è¤‡
        if len(keywords) != len(set(keywords)):
            issues.append(QualityIssue(
                field="keywords",
                severity="warning",
                issue_type="duplicates",
                description="é—œéµè©åŒ…å«é‡è¤‡é …",
                current_value=keywords,
                auto_fixable=True
            ))
            score -= 10

        # æª¢æŸ¥ç©ºå­—ä¸²
        if any(not kw.strip() for kw in keywords):
            issues.append(QualityIssue(
                field="keywords",
                severity="warning",
                issue_type="empty_keyword",
                description="åŒ…å«ç©ºç™½é—œéµè©",
                current_value=keywords,
                auto_fixable=True
            ))
            score -= 5

        return issues, max(0, score)

    def _determine_quality_level(self, score: float) -> str:
        """æ ¹æ“šè©•åˆ†ç¢ºå®šè³ªé‡ç­‰ç´š"""
        levels = self.rules['quality_scoring']['score_levels']

        if score >= levels['excellent']:
            return "excellent"
        elif score >= levels['good']:
            return "good"
        elif score >= levels['acceptable']:
            return "acceptable"
        elif score >= levels['poor']:
            return "poor"
        else:
            return "critical"

    def _auto_fix_issues(self, paper_id: int, report: QualityReport):
        """è‡ªå‹•ä¿®å¾©å•é¡Œ"""
        # TODO: å¯¦ä½œè‡ªå‹•ä¿®å¾©é‚è¼¯
        # é€™éœ€è¦æ•´åˆ PDF æå–å™¨å’Œå¤–éƒ¨ API
        pass

    def detect_duplicates(self, threshold: float = 0.85) -> List[Tuple[int, int, float]]:
        """
        æª¢æ¸¬é‡è¤‡è«–æ–‡

        Args:
            threshold: ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆ0-1ï¼‰

        Returns:
            (paper_id_1, paper_id_2, similarity) åˆ—è¡¨
        """
        duplicates = []
        papers = self.kb.list_papers(limit=1000)

        # å…©å…©æ¯”è¼ƒ
        for i in range(len(papers)):
            for j in range(i+1, len(papers)):
                similarity = self._calculate_similarity(papers[i], papers[j])
                if similarity >= threshold:
                    duplicates.append((papers[i]['id'], papers[j]['id'], similarity))

        return duplicates

    def _calculate_similarity(self, paper1: Dict[str, Any], paper2: Dict[str, Any]) -> float:
        """è¨ˆç®—å…©ç¯‡è«–æ–‡çš„ç›¸ä¼¼åº¦"""
        # æ¨™é¡Œç›¸ä¼¼åº¦
        title1 = paper1.get('title', '').lower()
        title2 = paper2.get('title', '').lower()
        title_sim = SequenceMatcher(None, title1, title2).ratio()

        # ä½œè€…é‡ç–Šåº¦
        authors1 = set(paper1.get('authors', []))
        authors2 = set(paper2.get('authors', []))
        if authors1 and authors2:
            author_overlap = len(authors1 & authors2) / len(authors1 | authors2)
        else:
            author_overlap = 0

        # å¹´ä»½ç›¸ä¼¼åº¦
        year1 = paper1.get('year', 0)
        year2 = paper2.get('year', 0)
        year_diff = abs(year1 - year2) if year1 and year2 else 10
        year_sim = max(0, 1 - year_diff / 10)

        # ç¶œåˆç›¸ä¼¼åº¦
        similarity = (
            title_sim * 0.6 +
            author_overlap * 0.3 +
            year_sim * 0.1
        )

        return similarity

    def generate_summary_report(self, reports: List[QualityReport], detail_level: str = "standard") -> str:
        """
        ç”Ÿæˆç¸½çµå ±å‘Š

        Args:
            reports: è³ªé‡å ±å‘Šåˆ—è¡¨
            detail_level: è©³ç´°ç¨‹åº¦ï¼ˆminimal/standard/comprehensiveï¼‰

        Returns:
            ç¸½çµå ±å‘Šæ–‡æœ¬
        """
        lines = []
        lines.append("=" * 80)
        lines.append("çŸ¥è­˜åº«è³ªé‡æª¢æŸ¥ç¸½çµå ±å‘Š")
        lines.append("=" * 80)
        lines.append(f"æª¢æŸ¥æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"æª¢æŸ¥è«–æ–‡æ•¸: {len(reports)}")
        lines.append("")

        # çµ±è¨ˆ
        total_issues = sum(len(r.issues) for r in reports)
        critical_issues = sum(len(r.get_critical_issues()) for r in reports)
        warnings = sum(len(r.get_warnings()) for r in reports)

        avg_score = sum(r.overall_score for r in reports) / len(reports) if reports else 0

        lines.append("ğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        lines.append(f"  å¹³å‡è©•åˆ†: {avg_score:.1f}/100")
        lines.append(f"  ç¸½å•é¡Œæ•¸: {total_issues}")
        lines.append(f"    - åš´é‡å•é¡Œ: {critical_issues}")
        lines.append(f"    - è­¦å‘Š: {warnings}")
        lines.append("")

        # è³ªé‡ç­‰ç´šåˆ†å¸ƒ
        level_counts = {}
        for report in reports:
            level = report.quality_level
            level_counts[level] = level_counts.get(level, 0) + 1

        lines.append("ğŸ“ˆ è³ªé‡ç­‰ç´šåˆ†å¸ƒ:")
        for level in ["excellent", "good", "acceptable", "poor", "critical"]:
            count = level_counts.get(level, 0)
            percentage = (count / len(reports) * 100) if reports else 0
            label = QualityReport(0, "")._get_quality_level_label(level)
            lines.append(f"  {label}: {count} ç¯‡ ({percentage:.1f}%)")
        lines.append("")

        # å•é¡Œè«–æ–‡åˆ—è¡¨
        if detail_level in ["standard", "comprehensive"]:
            problematic = [r for r in reports if r.has_critical_issues()]
            if problematic:
                lines.append(f"ğŸš¨ æœ‰åš´é‡å•é¡Œçš„è«–æ–‡ ({len(problematic)} ç¯‡):")
                for r in problematic[:20]:  # æœ€å¤šé¡¯ç¤º20ç¯‡
                    lines.append(f"  - [{r.paper_id}] {r.title[:60]}... ({len(r.get_critical_issues())} å€‹åš´é‡å•é¡Œ)")
                if len(problematic) > 20:
                    lines.append(f"  ... é‚„æœ‰ {len(problematic) - 20} ç¯‡")
                lines.append("")

        # è©³ç´°å»ºè­°
        if detail_level == "comprehensive":
            lines.append("ğŸ’¡ æ”¹é€²å»ºè­°:")

            # çµ±è¨ˆå„é¡å•é¡Œ
            issue_types = {}
            for report in reports:
                for issue in report.issues:
                    key = f"{issue.field}_{issue.issue_type}"
                    issue_types[key] = issue_types.get(key, 0) + 1

            # é¡¯ç¤ºæœ€å¸¸è¦‹çš„å•é¡Œ
            sorted_issues = sorted(issue_types.items(), key=lambda x: x[1], reverse=True)
            for issue_key, count in sorted_issues[:10]:
                field, issue_type = issue_key.split('_', 1)
                lines.append(f"  - {field} {issue_type}: {count} æ¬¡")
            lines.append("")

        lines.append("=" * 80)
        return "\n".join(lines)


if __name__ == "__main__":
    # Windows ç·¨ç¢¼ä¿®å¾©
    import sys
    import io
    if sys.platform == "win32":
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    # æ¸¬è©¦ä»£ç¢¼
    checker = QualityChecker()

    # æª¢æŸ¥æ‰€æœ‰è«–æ–‡
    print("æª¢æŸ¥çŸ¥è­˜åº«ä¸­çš„æ‰€æœ‰è«–æ–‡...")
    reports = checker.check_all_papers(auto_fix=False)

    # ç”Ÿæˆç¸½çµå ±å‘Š
    summary = checker.generate_summary_report(reports, detail_level="comprehensive")
    print(summary)

    # é¡¯ç¤ºå‰5ç¯‡æœ‰åš´é‡å•é¡Œçš„è«–æ–‡è©³æƒ…
    problematic = [r for r in reports if r.has_critical_issues()]
    if problematic:
        print("\n\nè©³ç´°å ±å‘Šï¼ˆå‰5ç¯‡æœ‰åš´é‡å•é¡Œçš„è«–æ–‡ï¼‰:")
        for report in problematic[:5]:
            print("\n" + report.to_text(detail_level="comprehensive"))
