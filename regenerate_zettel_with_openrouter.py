#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Zettelkasten å¡ç‰‡é‡æ–°ç”Ÿæˆ - OpenRouter æ”¯æŒç‰ˆæœ¬
æ”¯æŒå¤šç§ LLM providerï¼Œç‰¹åˆ«ä¼˜åŒ– OpenRouter é›†æˆ
"""

import os
import shutil
from pathlib import Path
from datetime import datetime
from src.generators.zettel_maker import ZettelMaker
from src.generators.slide_maker import SlideMaker

def extract_paper_content(md_path):
    """ä» MD æ–‡ä»¶æå–è®ºæ–‡å†…å®¹"""
    content = md_path.read_text(encoding='utf-8')

    import re
    title_match = re.search(r"title:\s*['\"]?(.+?)['\"]?\s*$", content, re.MULTILINE)
    title = title_match.group(1) if title_match else "Unknown"

    authors_match = re.search(r"authors:\s*(.+?)$", content, re.MULTILINE)
    authors = authors_match.group(1) if authors_match else "Unknown"

    year_match = re.search(r"year:\s*(\d{4})", content)
    year = int(year_match.group(1)) if year_match else None

    content_start = content.find('---', content.find('---') + 3) + 3
    full_content = content[content_start:].strip()

    return {
        'title': title,
        'authors': authors,
        'year': year,
        'content': full_content[:15000],
        'cite_key': md_path.stem
    }

def generate_with_llm(paper_data, llm_provider='auto', model=None, temperature=0.3):
    """ä½¿ç”¨æŒ‡å®š LLM ç”Ÿæˆå¡ç‰‡"""
    print(f"\n{'='*70}")
    print(f"ç”Ÿæˆ Zettelkasten: {paper_data['cite_key']}")
    print(f"æ ‡é¢˜: {paper_data['title'][:60]}...")
    print(f"{'='*70}\n")

    # åˆå§‹åŒ– SlideMaker
    print(f"åˆå§‹åŒ– LLM...")
    print(f"  Provider: {llm_provider}")
    if model:
        print(f"  Model: {model}")
    print(f"  Temperature: {temperature}\n")

    slide_maker = SlideMaker(
        llm_provider=llm_provider,
        selection_strategy='balanced'
    )

    # å‡†å¤‡ prompt
    from jinja2 import Template
    template_path = Path("templates/prompts/zettelkasten_template.jinja2")
    template = Template(template_path.read_text(encoding='utf-8'))

    prompt = template.render(
        topic=paper_data['title'],
        card_count=20,
        detail_level="comprehensive",
        paper_content=paper_data['content'],
        cite_key=paper_data['cite_key'],
        language="chinese"  # ä¿®å¾© Gemini èªè¨€è¨­ç½®å¤±æ•ˆå•é¡Œ
    )

    print(f"Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"ç”Ÿæˆ 20 å¼ å¡ç‰‡ (comprehensive æ¨¡å¼)\n")

    # è°ƒç”¨ LLM
    print("æ­£åœ¨è°ƒç”¨ LLM...")
    # DeepSeek å’Œ Llama éœ€è¦æ›´å¤§çš„ max_tokens ä»¥ç”Ÿæˆå®Œæ•´å¡ç‰‡
    max_tokens = 16000 if ('deepseek' in model.lower() or 'llama' in model.lower()) else 4096
    print(f"ä½¿ç”¨ max_tokens: {max_tokens}")
    result = slide_maker.call_llm(prompt, model=model, max_tokens=max_tokens)

    if not result:
        print("âŒ LLM è¿”å›ç©ºå“åº”")
        return None

    response, provider = result
    print(f"âœ… LLM å“åº” ({provider}): {len(response)} å­—ç¬¦\n")

    # ç”Ÿæˆå¡ç‰‡æ–‡ä»¶ï¼ˆæ·»åŠ æ¨¡å‹æ¨™è¨˜ï¼‰
    model_name = model.split('/')[-1].replace('-', '_')  # deepseek-r1 -> deepseek_r1

    # ä¿å­˜åŸå§‹è¼¸å‡ºç”¨æ–¼èª¿è©¦
    debug_file = Path("output") / f"debug_llm_output_{paper_data['cite_key']}_{model_name}.txt"
    debug_file.write_text(response, encoding='utf-8')
    print(f"ğŸ› èª¿è©¦è¼¸å‡ºå·²ä¿å­˜: {debug_file}\n")

    output_dir = Path("output/zettelkasten_notes") / f"zettel_{paper_data['cite_key']}_{datetime.now().strftime('%Y%m%d')}_{model_name}"

    if output_dir.exists():
        shutil.rmtree(output_dir)

    print(f"ç”Ÿæˆå¡ç‰‡åˆ°: {output_dir}")

    zettel_maker = ZettelMaker()

    # æ§‹å»ºå¼•ç”¨æ ¼å¼
    authors_short = paper_data['authors'].split(',')[0].split()[0] if paper_data['authors'] else "Unknown"
    citation = f"[[{paper_data['cite_key']}.pdf|{authors_short} et al. ({paper_data['year']})]]"

    result = zettel_maker.generate_zettelkasten(
        llm_output=response,
        output_dir=output_dir,
        paper_info={
            'cite_key': paper_data['cite_key'],
            'title': paper_data['title'],
            'authors': paper_data['authors'],
            'year': paper_data['year'],
            'citation': citation
        }
    )

    print(f"âœ… ç”Ÿæˆ {result['card_count']} å¼ å¡ç‰‡\n")
    print(f"âœ… ç”Ÿæˆå®Œæˆ: {output_dir}\n")

    return output_dir

def test_multiple_llms(paper_data):
    """æµ‹è¯•å¤šä¸ª LLM çš„ç”Ÿæˆæ•ˆæœ"""
    print("\n" + "="*70)
    print("å¤š LLM å¯¹æ¯”æµ‹è¯•")
    print("="*70 + "\n")

    # æµ‹è¯•é…ç½®
    test_configs = []

    # æ£€æŸ¥å“ªäº› LLM å¯ç”¨
    import os
    from dotenv import load_dotenv
    load_dotenv()

    if os.getenv('OPENROUTER_API_KEY'):
        test_configs.extend([
            ('openrouter', 'anthropic/claude-3.5-sonnet', 0.3, 'æœ€ä½³æ ¼å¼éµå¾ª'),
            ('openrouter', 'anthropic/claude-3-haiku', 0.3, 'å¿«é€Ÿç»æµ'),
        ])

    if os.getenv('ANTHROPIC_API_KEY'):
        test_configs.append(('anthropic', 'claude-3-sonnet-20240229', 0.3, 'ç›´æ¥ Anthropic'))

    if os.getenv('GOOGLE_API_KEY'):
        test_configs.append(('google', 'gemini-2.0-flash-exp', 0.5, 'Google Gemini'))

    if not test_configs:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ LLM API key")
        print("è¯·é…ç½®è‡³å°‘ä¸€ä¸ª API key:")
        print("  - OPENROUTER_API_KEY (æ¨è)")
        print("  - GOOGLE_API_KEY")
        print("  - ANTHROPIC_API_KEY")
        return

    print(f"å°†æµ‹è¯• {len(test_configs)} ä¸ªé…ç½®:\n")
    for i, (provider, model, temp, desc) in enumerate(test_configs, 1):
        print(f"{i}. {desc}")
        print(f"   Provider: {provider}")
        print(f"   Model: {model}\n")

    results = {}
    for provider, model, temp, desc in test_configs:
        print("\n" + "="*70)
        print(f"æµ‹è¯•: {desc}")
        print("="*70)

        try:
            output_dir = generate_with_llm(paper_data, provider, model, temp)
            if output_dir:
                results[desc] = {
                    'provider': provider,
                    'model': model,
                    'output_dir': output_dir,
                    'status': 'æˆåŠŸ'
                }
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
            results[desc] = {
                'provider': provider,
                'model': model,
                'status': f'å¤±è´¥: {e}'
            }

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*70 + "\n")

    for desc, result in results.items():
        print(f"{desc}:")
        print(f"  çŠ¶æ€: {result['status']}")
        if result['status'] == 'æˆåŠŸ':
            print(f"  è¾“å‡º: {result['output_dir']}")
        print()

    print("ä¸‹ä¸€æ­¥: ä½¿ç”¨ analyze_card_links.py åˆ†ææ¯ä¸ªç‰ˆæœ¬çš„è¿ç»“æ•°é‡")

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Zettelkasten é‡æ–°ç”Ÿæˆ - å¤š LLM æ”¯æŒ')
    parser.add_argument('--provider', default='auto',
                        choices=['auto', 'openrouter', 'google', 'anthropic', 'openai', 'ollama'],
                        help='LLM provider')
    parser.add_argument('--model', help='Model name')
    parser.add_argument('--temperature', type=float, default=0.3, help='Temperature')
    parser.add_argument('--test-all', action='store_true', help='æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„ LLM')

    args = parser.parse_args()

    print("\n" + "="*70)
    print("Zettelkasten é‡æ–°ç”Ÿæˆ - OpenRouter & å¤š LLM æ”¯æŒ")
    print("="*70 + "\n")

    # è¯»å–è®ºæ–‡
    md_path = Path("knowledge_base/papers/Jones-2024.md")
    if not md_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {md_path}")
        return

    print(f"ğŸ“„ è¯»å–è®ºæ–‡: {md_path}")
    paper_data = extract_paper_content(md_path)
    print(f"âœ… æå–æˆåŠŸ\n")

    # å¤‡ä»½æ—§å¡ç‰‡
    old_dir = Path("output/zettelkasten_notes/zettel_Jones-2024_20251110_deepseek")
    if old_dir.exists() and not args.test_all:
        backup_dir = old_dir.parent / f"{old_dir.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"ğŸ’¾ å¤‡ä»½æ—§å¡ç‰‡: {backup_dir}")
        shutil.copytree(old_dir, backup_dir)
        print(f"âœ… å¤‡ä»½å®Œæˆ\n")

    # ç”Ÿæˆæ–°å¡ç‰‡
    if args.test_all:
        test_multiple_llms(paper_data)
    else:
        output_dir = generate_with_llm(
            paper_data,
            llm_provider=args.provider,
            model=args.model,
            temperature=args.temperature
        )

        if output_dir:
            print("="*70)
            print("ä¸‹ä¸€æ­¥:")
            print("="*70)
            print("1. åˆ†ææ–°å¡ç‰‡:")
            print("   python analyze_card_links.py")
            print("2. æŸ¥çœ‹æŠ¥å‘Š:")
            print("   cat output/card_link_analysis_after.txt")

if __name__ == '__main__':
    main()
