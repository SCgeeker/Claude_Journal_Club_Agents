#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èªç¾©æœç´¢æº–ç¢ºæ€§æ¸¬è©¦è…³æœ¬
è©•ä¼°æŒ‡æ¨™ï¼šRecall@K, Precision@K, MRR
"""

import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Set
import io

# è¨­ç½® UTF-8 ç·¨ç¢¼ï¼ˆWindows ç›¸å®¹æ€§ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.embeddings import create_manager


def calculate_recall_at_k(predictions: List, ground_truth: List, k: int) -> float:
    """è¨ˆç®— Recall@K

    Args:
        predictions: é æ¸¬çµæœåˆ—è¡¨ï¼ˆæœ‰åºï¼‰
        ground_truth: çœŸå¯¦ç›¸é—œçµæœåˆ—è¡¨
        k: å–å‰ K å€‹çµæœ

    Returns:
        Recall å€¼ï¼ˆ0-1ï¼‰
    """
    if not ground_truth:
        return 0.0

    predictions_at_k = set(predictions[:k])
    ground_truth_set = set(ground_truth)

    relevant_retrieved = len(predictions_at_k & ground_truth_set)
    return relevant_retrieved / len(ground_truth_set)


def calculate_precision_at_k(predictions: List, ground_truth: List, k: int) -> float:
    """è¨ˆç®— Precision@K

    Args:
        predictions: é æ¸¬çµæœåˆ—è¡¨ï¼ˆæœ‰åºï¼‰
        ground_truth: çœŸå¯¦ç›¸é—œçµæœåˆ—è¡¨
        k: å–å‰ K å€‹çµæœ

    Returns:
        Precision å€¼ï¼ˆ0-1ï¼‰
    """
    if k == 0:
        return 0.0

    predictions_at_k = set(predictions[:k])
    ground_truth_set = set(ground_truth)

    relevant_retrieved = len(predictions_at_k & ground_truth_set)
    return relevant_retrieved / k


def calculate_mrr(predictions: List, ground_truth: List) -> float:
    """è¨ˆç®— MRR (Mean Reciprocal Rank)

    æ‰¾åˆ°ç¬¬ä¸€å€‹ç›¸é—œçµæœçš„ä½ç½®ï¼Œè¨ˆç®—å…¶å€’æ•¸

    Args:
        predictions: é æ¸¬çµæœåˆ—è¡¨ï¼ˆæœ‰åºï¼‰
        ground_truth: çœŸå¯¦ç›¸é—œçµæœåˆ—è¡¨

    Returns:
        RR å€¼ï¼ˆ0-1ï¼‰
    """
    ground_truth_set = set(ground_truth)

    for rank, pred in enumerate(predictions, 1):
        if pred in ground_truth_set:
            return 1.0 / rank

    return 0.0


def run_semantic_search(
    manager,
    query: str,
    search_type: str,
    limit: int = 20
) -> List:
    """åŸ·è¡Œèªç¾©æœç´¢ä¸¦è¿”å›çµæœ ID åˆ—è¡¨

    Args:
        manager: EmbeddingManager å¯¦ä¾‹
        query: æœç´¢æŸ¥è©¢
        search_type: æœç´¢é¡å‹ï¼ˆpapers æˆ– zettelï¼‰
        limit: è¿”å›æ•¸é‡

    Returns:
        çµæœ ID åˆ—è¡¨
    """
    results = manager.search(query, type=search_type, limit=limit)

    if search_type == 'papers':
        return [item['paper_id'] for item in results['papers']]
    else:  # zettel
        return [item['zettel_id'] for item in results['zettel']]


def evaluate_semantic_search(
    test_dataset_path: str,
    provider: str = "gemini",
    verbose: bool = True
) -> Dict:
    """è©•ä¼°èªç¾©æœç´¢æº–ç¢ºæ€§

    Args:
        test_dataset_path: æ¸¬è©¦æ•¸æ“šé›†è·¯å¾‘
        provider: åµŒå…¥æä¾›è€…
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°éç¨‹

    Returns:
        è©•ä¼°çµæœå­—å…¸
    """
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“šé›†
    with open(test_dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    # åˆå§‹åŒ– EmbeddingManager
    if verbose:
        print(f"\nåˆå§‹åŒ– EmbeddingManager (provider: {provider})...")
    manager = create_manager(provider=provider)

    results = []
    total_queries = len(dataset['queries'])

    if verbose:
        print(f"\né–‹å§‹æ¸¬è©¦ {total_queries} å€‹æŸ¥è©¢...\n")

    for i, query_data in enumerate(dataset['queries'], 1):
        query_id = query_data['id']
        query_text = query_data['query']
        search_type = query_data['type']
        ground_truth = query_data['relevant_ids']

        if verbose:
            print(f"[{i}/{total_queries}] æ¸¬è©¦æŸ¥è©¢: '{query_text}'")
            print(f"  é¡å‹: {search_type} | ç›¸é—œé …ç›®æ•¸: {len(ground_truth)}")

        # åŸ·è¡Œæœç´¢
        try:
            predictions = run_semantic_search(
                manager=manager,
                query=query_text,
                search_type=search_type,
                limit=20
            )

            # è¨ˆç®—æŒ‡æ¨™
            recall_at_5 = calculate_recall_at_k(predictions, ground_truth, 5)
            recall_at_10 = calculate_recall_at_k(predictions, ground_truth, 10)
            precision_at_5 = calculate_precision_at_k(predictions, ground_truth, 5)
            precision_at_10 = calculate_precision_at_k(predictions, ground_truth, 10)
            mrr = calculate_mrr(predictions, ground_truth)

            result = {
                'query_id': query_id,
                'query': query_text,
                'type': search_type,
                'domain': query_data.get('domain', ''),
                'ground_truth_count': len(ground_truth),
                'predictions_count': len(predictions),
                'recall@5': recall_at_5,
                'recall@10': recall_at_10,
                'precision@5': precision_at_5,
                'precision@10': precision_at_10,
                'mrr': mrr,
                'predictions': predictions[:10],  # ä¿å­˜å‰ 10 å€‹é æ¸¬
                'ground_truth': ground_truth
            }

            results.append(result)

            if verbose:
                print(f"  âœ… Recall@5: {recall_at_5:.1%} | Recall@10: {recall_at_10:.1%} | MRR: {mrr:.3f}")

        except Exception as e:
            if verbose:
                print(f"  âŒ éŒ¯èª¤: {str(e)}")
            results.append({
                'query_id': query_id,
                'query': query_text,
                'error': str(e)
            })

    # è¨ˆç®—å¹³å‡æŒ‡æ¨™
    valid_results = [r for r in results if 'error' not in r]

    if not valid_results:
        return {
            'total_queries': total_queries,
            'successful': 0,
            'failed': len(results),
            'results': results
        }

    avg_recall_5 = sum(r['recall@5'] for r in valid_results) / len(valid_results)
    avg_recall_10 = sum(r['recall@10'] for r in valid_results) / len(valid_results)
    avg_precision_5 = sum(r['precision@5'] for r in valid_results) / len(valid_results)
    avg_precision_10 = sum(r['precision@10'] for r in valid_results) / len(valid_results)
    avg_mrr = sum(r['mrr'] for r in valid_results) / len(valid_results)

    # æŒ‰é¡å‹çµ±è¨ˆ
    papers_results = [r for r in valid_results if r['type'] == 'papers']
    zettel_results = [r for r in valid_results if r['type'] == 'zettel']

    summary = {
        'total_queries': total_queries,
        'successful': len(valid_results),
        'failed': total_queries - len(valid_results),
        'average_metrics': {
            'recall@5': avg_recall_5,
            'recall@10': avg_recall_10,
            'precision@5': avg_precision_5,
            'precision@10': avg_precision_10,
            'mrr': avg_mrr
        },
        'by_type': {
            'papers': {
                'count': len(papers_results),
                'recall@5': sum(r['recall@5'] for r in papers_results) / len(papers_results) if papers_results else 0,
                'recall@10': sum(r['recall@10'] for r in papers_results) / len(papers_results) if papers_results else 0
            },
            'zettel': {
                'count': len(zettel_results),
                'recall@5': sum(r['recall@5'] for r in zettel_results) / len(zettel_results) if zettel_results else 0,
                'recall@10': sum(r['recall@10'] for r in zettel_results) / len(zettel_results) if zettel_results else 0
            }
        },
        'results': results
    }

    return summary


def print_summary_report(summary: Dict):
    """æ‰“å°æ‘˜è¦å ±å‘Š"""
    print("\n" + "=" * 70)
    print("ğŸ“Š èªç¾©æœç´¢æº–ç¢ºæ€§æ¸¬è©¦å ±å‘Š")
    print("=" * 70)

    print(f"\nç¸½æŸ¥è©¢æ•¸: {summary['total_queries']}")
    print(f"æˆåŠŸ: {summary['successful']} | å¤±æ•—: {summary['failed']}")

    avg = summary['average_metrics']
    print("\nå¹³å‡æŒ‡æ¨™:")
    print(f"  Recall@5:     {avg['recall@5']:.1%}")
    print(f"  Recall@10:    {avg['recall@10']:.1%}")
    print(f"  Precision@5:  {avg['precision@5']:.1%}")
    print(f"  Precision@10: {avg['precision@10']:.1%}")
    print(f"  MRR:          {avg['mrr']:.3f}")

    print("\næŒ‰é¡å‹çµ±è¨ˆ:")
    for search_type, stats in summary['by_type'].items():
        if stats['count'] > 0:
            print(f"  {search_type.upper()} ({stats['count']} æŸ¥è©¢):")
            print(f"    Recall@5:  {stats['recall@5']:.1%}")
            print(f"    Recall@10: {stats['recall@10']:.1%}")

    # ç›®æ¨™é”æˆæƒ…æ³
    print("\nç›®æ¨™é”æˆæƒ…æ³:")
    target_recall_5 = 0.60
    target_recall_10 = 0.80
    target_mrr = 0.70

    def check_target(value, target, name):
        status = "âœ…" if value >= target else "âŒ"
        return f"  {status} {name}: {value:.1%} (ç›®æ¨™: {target:.1%})"

    print(check_target(avg['recall@5'], target_recall_5, "Recall@5"))
    print(check_target(avg['recall@10'], target_recall_10, "Recall@10"))
    print(check_target(avg['mrr'], target_mrr, "MRR"))

    print("\n" + "=" * 70)


def save_detailed_report(summary: Dict, output_path: str):
    """ä¿å­˜è©³ç´°å ±å‘Šåˆ° JSON æ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="èªç¾©æœç´¢æº–ç¢ºæ€§æ¸¬è©¦",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--dataset',
        type=str,
        default='tests/semantic_search_test_queries.json',
        help='æ¸¬è©¦æ•¸æ“šé›†è·¯å¾‘ï¼ˆé»˜èªï¼štests/semantic_search_test_queries.jsonï¼‰'
    )
    parser.add_argument(
        '--provider',
        choices=['gemini', 'ollama'],
        default='gemini',
        help='åµŒå…¥æä¾›è€…ï¼ˆé»˜èªï¼šgeminiï¼‰'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='tests/semantic_accuracy_report.json',
        help='è¼¸å‡ºå ±å‘Šè·¯å¾‘ï¼ˆé»˜èªï¼štests/semantic_accuracy_report.jsonï¼‰'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='å®‰éœæ¨¡å¼ï¼ˆä¸é¡¯ç¤ºè©³ç´°éç¨‹ï¼‰'
    )

    args = parser.parse_args()

    # åŸ·è¡Œè©•ä¼°
    summary = evaluate_semantic_search(
        test_dataset_path=args.dataset,
        provider=args.provider,
        verbose=not args.quiet
    )

    # æ‰“å°æ‘˜è¦å ±å‘Š
    print_summary_report(summary)

    # ä¿å­˜è©³ç´°å ±å‘Š
    save_detailed_report(summary, args.output)


if __name__ == "__main__":
    main()
