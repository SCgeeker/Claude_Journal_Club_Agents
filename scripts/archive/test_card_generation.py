#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ–° Template çš„å¡ç‰‡ç”Ÿæˆæ•ˆæœ
"""

import os
import re
import json
from pathlib import Path
from collections import defaultdict

def analyze_existing_cards():
    """åˆ†æç°æœ‰å¡ç‰‡çš„è¿ç»“æƒ…å†µ"""
    print("=" * 70)
    print("åˆ†æç°æœ‰ Zettelkasten å¡ç‰‡")
    print("=" * 70)

    zettel_dir = Path("output/zettelkasten_notes")
    if not zettel_dir.exists():
        print(f"Error: {zettel_dir} ä¸å­˜åœ¨")
        return None

    stats = {
        'total_papers': 0,
        'total_cards': 0,
        'cards_with_explicit_links': 0,
        'ai_notes_with_links': 0,
        'total_ai_notes': 0,
        'link_counts': [],
        'papers': []
    }

    # Wiki Link æ¨¡å¼
    wiki_link_pattern = re.compile(r'\[\[([^\]]+)\]\]')

    for paper_dir in sorted(zettel_dir.iterdir()):
        if not paper_dir.is_dir() or not paper_dir.name.startswith('zettel_'):
            continue

        stats['total_papers'] += 1
        paper_info = {
            'name': paper_dir.name,
            'cards': 0,
            'cards_with_links': 0,
            'ai_notes_links': []
        }

        cards_dir = paper_dir / "zettel_cards"
        if not cards_dir.exists():
            continue

        for card_file in sorted(cards_dir.glob("*.md")):
            stats['total_cards'] += 1
            paper_info['cards'] += 1

            content = card_file.read_text(encoding='utf-8')

            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®è¿ç»“
            links = wiki_link_pattern.findall(content)
            if links:
                stats['cards_with_explicit_links'] += 1
                paper_info['cards_with_links'] += 1

            # æ£€æŸ¥ AI notes åŒºå—
            ai_notes_match = re.search(r'## å€‹äººç­†è¨˜\s*\n\s*ğŸ¤–\s*\*\*AI\*\*:\s*(.*?)(?=\n\n|âœï¸|\Z)', content, re.DOTALL)
            if ai_notes_match:
                stats['total_ai_notes'] += 1
                ai_notes = ai_notes_match.group(1)
                ai_links = wiki_link_pattern.findall(ai_notes)

                if ai_links:
                    stats['ai_notes_with_links'] += 1
                    stats['link_counts'].append(len(ai_links))
                    paper_info['ai_notes_links'].append({
                        'card': card_file.name,
                        'count': len(ai_links),
                        'links': ai_links
                    })
                else:
                    stats['link_counts'].append(0)

        stats['papers'].append(paper_info)

    return stats

def print_stats(stats):
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"   - è®ºæ–‡æ€»æ•°: {stats['total_papers']}")
    print(f"   - å¡ç‰‡æ€»æ•°: {stats['total_cards']}")
    print(f"   - æœ‰æ˜ç¡®è¿ç»“çš„å¡ç‰‡: {stats['cards_with_explicit_links']} ({stats['cards_with_explicit_links']/stats['total_cards']*100:.1f}%)")
    print(f"   - AI notes æ€»æ•°: {stats['total_ai_notes']}")
    print(f"   - æœ‰è¿ç»“çš„ AI notes: {stats['ai_notes_with_links']} ({stats['ai_notes_with_links']/stats['total_ai_notes']*100:.1f}%)" if stats['total_ai_notes'] > 0 else "   - æœ‰è¿ç»“çš„ AI notes: 0")

    if stats['link_counts']:
        avg_links = sum(stats['link_counts']) / len(stats['link_counts'])
        print(f"   - AI notes å¹³å‡è¿ç»“æ•°: {avg_links:.2f}")
        print(f"   - è¿ç»“æ•°åˆ†å¸ƒ: min={min(stats['link_counts'])}, max={max(stats['link_counts'])}")
    else:
        print(f"   - AI notes å¹³å‡è¿ç»“æ•°: 0.00")

    # æ˜¾ç¤ºå‰ 5 ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“„ å‰ 5 ç¯‡è®ºæ–‡è¯¦æƒ…:")
    for i, paper in enumerate(stats['papers'][:5], 1):
        print(f"\n{i}. {paper['name']}")
        print(f"   - å¡ç‰‡æ•°: {paper['cards']}")
        print(f"   - æœ‰è¿ç»“çš„å¡ç‰‡: {paper['cards_with_links']}")
        if paper['ai_notes_links']:
            print(f"   - AI notes æœ‰è¿ç»“çš„å¡ç‰‡:")
            for link_info in paper['ai_notes_links'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"     â€¢ {link_info['card']}: {link_info['count']} ä¸ªè¿ç»“")
                print(f"       {link_info['links'][:2]}")  # æ˜¾ç¤ºå‰2ä¸ªè¿ç»“

def select_test_paper(stats):
    """é€‰æ‹©æµ‹è¯•è®ºæ–‡"""
    print("\n" + "=" * 70)
    print("é€‰æ‹©æµ‹è¯•è®ºæ–‡")
    print("=" * 70)

    # é€‰æ‹©ä¸€ç¯‡æœ‰ä¸€å®šå¡ç‰‡æ•°é‡çš„è®ºæ–‡
    candidates = [p for p in stats['papers'] if p['cards'] >= 5 and p['cards'] <= 15]

    if not candidates:
        candidates = stats['papers'][:5]

    print("\næ¨èçš„æµ‹è¯•è®ºæ–‡:")
    for i, paper in enumerate(candidates[:5], 1):
        print(f"{i}. {paper['name']} ({paper['cards']} å¼ å¡ç‰‡, {paper['cards_with_links']} å¼ æœ‰è¿ç»“)")

    return candidates[0] if candidates else None

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("Zettelkasten å¡ç‰‡ç”Ÿæˆæµ‹è¯• - Phase 2.3")
    print("=" * 70)

    # åˆ†æç°æœ‰å¡ç‰‡
    stats = analyze_existing_cards()
    if not stats:
        return

    print_stats(stats)

    # é€‰æ‹©æµ‹è¯•è®ºæ–‡
    test_paper = select_test_paper(stats)

    if test_paper:
        print(f"\nâœ… æ¨èæµ‹è¯•è®ºæ–‡: {test_paper['name']}")
        print(f"   è¿™ç¯‡è®ºæ–‡æœ‰ {test_paper['cards']} å¼ å¡ç‰‡")
        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"   1. ä»çŸ¥è¯†åº“æ‰¾åˆ°å¯¹åº”çš„ paper_id")
        print(f"   2. é‡æ–°ç”Ÿæˆ Zettelkasten å¡ç‰‡")
        print(f"   3. æ¯”è¾ƒå‰åå·®å¼‚")

    # ä¿å­˜ç»Ÿè®¡æ•°æ®
    output_file = Path("output/card_generation_test_baseline.json")
    output_file.parent.mkdir(exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

if __name__ == '__main__':
    main()
