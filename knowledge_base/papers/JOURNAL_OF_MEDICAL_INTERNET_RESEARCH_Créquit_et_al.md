---
title: JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
authors: Statistics Sorbonne, Assistance Publique, Philippe Ravaud, Paris Cit, Systematic Review, Paris Descartes, Columbia University, Mehdi Benchoufi, Public Health, United States
year: N/A
keywords: 
created: 2025-11-23 18:03:22
---

# JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al

## 基本信息

- **作者**: Statistics Sorbonne, Assistance Publique, Philippe Ravaud, Paris Cit, Systematic Review, Paris Descartes, Columbia University, Mehdi Benchoufi, Public Health, United States
- **年份**: N/A
- **關鍵詞**: 

## 摘要

Background: Crowdsourcing involves obtaining ideas, needed services, or content by soliciting Web-based contributions from
a crowd. The 4 types of crowdsourced tasks (problem solving, data processing, surveillance or monitoring, and surveying) can
be applied in the 3 categories of health (promotion, research, and care).
Objective: This study aimed to map the different applications of crowdsourcing in health to assess the fields of health that are
using crowdsourcing and the crowdsourced tasks used. We also describe the logistics of crowdsourcing and the characteristics
of crowd workers.
Methods: MEDLINE, EMBASE, and ClinicalTrials.gov were searched for available reports from inception to March 30, 2016,
with no restriction on language or publication status.
Results: We identified 202 relevant studies that used crowdsourcing, including 9 randomized controlled trials, of which only
one had posted results at ClinicalTrials.gov. Crowdsourcing was used in health promotion (91/202, 45.0%), research (73/202,
36.1%), and care (38/202, 18.8%). The 4 most frequent areas of application were public health (67/202, 33.2%), psychiatry
(32/202, 15.8%), surgery (22/202, 10.9%), and oncology (14/202, 6.9%). Half of the reports (99/202, 49.0%) referred to data
processing, 34.6% (70/202) referred to surveying, 10.4% (21/202) referred to surveillance or monitoring, and 5.9% (12/202)
referred to problem-solving. Labor market platforms (eg, Amazon Mechanical Turk) were used in most studies (190/202, 94%).
The crowd workers’ characteristics were poorly reported, and crowdsourcing logistics were missing from two-thirds of the reports.
When reported, the median size of the crowd was 424 (first and third quartiles: 167-802); crowd workers’ median age was 34
years (32-36). Crowd workers were mainly recruited nationally, particularly in the United States. For many studies (58.9%,
119/202), previous experience in crowdsourcing was required, and passing a qualification test or training was seldo

## 研究背景

## 研究方法

## 主要結果

## 討論與結論

## 個人評論

## 相關文獻

## 完整內容

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Review
Mapping of Crowdsourcing in Health: Systematic Review
Perrine Créquit1,2,3, MD, PhD; Ghizlène Mansouri1, MSc; Mehdi Benchoufi2, MD; Alexandre Vivot1,2, MD, PhD;
Philippe Ravaud1,2,3,4, MD, PhD
1INSERM UMR1153, Methods Team, Epidemiology and Statistics Sorbonne Paris Cité Research Center, Paris Descartes University, Paris, France
2Centre d’Epidémiologie Clinique, Hôpital Hôtel Dieu, Assistance Publique des Hôpitaux de Paris, Paris, France
3Cochrane France, Paris, France
4Department of Epidemiology, Columbia University, Mailman School of Public Health, New York, NY, United States
Corresponding Author:
Perrine Créquit, MD, PhD
INSERM UMR1153, Methods Team
Epidemiology and Statistics Sorbonne Paris Cité Research Center
Paris Descartes University
1 place du Parvis Notre Dame
Paris, 75004
France
Phone: 33 142348932
Email: perrine.crequit@aphp.fr
Abstract
Background: Crowdsourcing involves obtaining ideas, needed services, or content by soliciting Web-based contributions from
a crowd. The 4 types of crowdsourced tasks (problem solving, data processing, surveillance or monitoring, and surveying) can
be applied in the 3 categories of health (promotion, research, and care).
Objective: This study aimed to map the different applications of crowdsourcing in health to assess the fields of health that are
using crowdsourcing and the crowdsourced tasks used. We also describe the logistics of crowdsourcing and the characteristics
of crowd workers.
Methods: MEDLINE, EMBASE, and ClinicalTrials.gov were searched for available reports from inception to March 30, 2016,
with no restriction on language or publication status.
Results: We identified 202 relevant studies that used crowdsourcing, including 9 randomized controlled trials, of which only
one had posted results at ClinicalTrials.gov. Crowdsourcing was used in health promotion (91/202, 45.0%), research (73/202,
36.1%), and care (38/202, 18.8%). The 4 most frequent areas of application were public health (67/202, 33.2%), psychiatry
(32/202, 15.8%), surgery (22/202, 10.9%), and oncology (14/202, 6.9%). Half of the reports (99/202, 49.0%) referred to data
processing, 34.6% (70/202) referred to surveying, 10.4% (21/202) referred to surveillance or monitoring, and 5.9% (12/202)
referred to problem-solving. Labor market platforms (eg, Amazon Mechanical Turk) were used in most studies (190/202, 94%).
The crowd workers’ characteristics were poorly reported, and crowdsourcing logistics were missing from two-thirds of the reports.
When reported, the median size of the crowd was 424 (first and third quartiles: 167-802); crowd workers’ median age was 34
years (32-36). Crowd workers were mainly recruited nationally, particularly in the United States. For many studies (58.9%,
119/202), previous experience in crowdsourcing was required, and passing a qualification test or training was seldom needed
(11.9% of studies; 24/202). For half of the studies, monetary incentives were mentioned, with mainly less than US $1 to perform
the task. The time needed to perform the task was mostly less than 10 min (58.9% of studies; 119/202). Data quality validation
was used in 54/202 studies (26.7%), mainly by attention check questions or by replicating the task with several crowd workers.
Conclusions: The use of crowdsourcing, which allows access to a large pool of participants as well as saving time in data
collection, lowering costs, and speeding up innovations, is increasing in health promotion, research, and care. However, the
description of crowdsourcing logistics and crowd workers’ characteristics is frequently missing in study reports and needs to be
precisely reported to better interpret the study findings and replicate them.
(J Med Internet Res 2018;20(5):e187) doi:10.2196/jmir.9330
KEYWORDS
review [publication type]; crowdsourcing; health
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.1
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
scientific community and researchers needing to obtain data
Introduction
from any domain.
Scientific research performed with the involvement of the Crowdsourcing represents a great opportunity in health and
broader public, the crowd, is attracting increasing attention from medical research. As mentioned by Swan [10], crowdsourced
scientists and policy makers. Crowdsourcing uses the power of health research studies are the nexus of 3 contemporary trends:
many, using the collective wisdom and resources of the crowd, “citizen science,” crowdsourcing, and Medicine 2.0. Medicine
to complete human intelligence tasks (ie, tasks that cannot be 2.0 or Health 2.0 refers to the active participation of individuals
entirely automated and require human intelligence). in their health care, particularly using Web 2.0 technologies.
Crowdsourcing is not a new concept and has often been used
Crowdsourcing is not limited to health research but can also be
in the past as a competition to discover a solution. It originated
used in health promotion or health care. Crowdsourcing could
in 1714 in England, where the British Government proposed
be a great way to solve a specific scientific mission that cannot
£20,000 to anyone who could find a solution for calculating the
be entirely automated and requires human intelligence in these
longitudinal position of a ship [1], and then it was applied in a
3 health categories. However, mapping of crowdsourcing use
variety of fields such as astronomy, energy system research,
in health is needed to describe all its applications and to detail
genealogy and genetic research, journalism, linguistics,
specificities, so that health researchers can assess whether they
ornithology, public policy, seismology, and molecular biology
can use this approach in their research.
[2].
The aim of the study was to map the different applications of
Crowdsourcing currently involves a network of people, the
crowdsourcing used in health to outline the fields of health that
“crowd workers,” responding to an open call and completing
are using crowdsourcing and the type of crowdsourced tasks
Web-based tasks of requesters [3]. These crowd workers provide
involved. We also describe the logistics of crowdsourcing and
a large wide of activities, especially via the internet, using
the characteristics of crowd workers.
specific platforms, but have no formal training in the topic of
investigation [4]. They have access to the crowdsourcing
Methods
websites from anywhere at times convenient for them. They
carry out tasks posted by requesters, who accept or reject their
Design
work and may or not pay them for the work. Crowdsourcing
has grown rapidly with the evolution of technology, with 2.3 We conducted a systematic review to identify studies using
billion internet users and 6 billion mobile phone subscribers crowdsourcing in health. We uploaded a prespecified protocol
[5]. The main Web platform for crowdsourcing is Amazon to a publicly accessible institutional Website (Multimedia
Mechanical Turk (MTurk), which was exploited by scientists Appendix 1) and followed standard procedures for systematic
5 years ago. In 1 month—May 2016—23,000 people completed reviews and reported processes and results according to the
230,000 tasks on their computers in 3.3 million min, Preferred Reporting Items for Systematic Reviews and
corresponding to a total of more than 6 years of effort [6]. Meta-Analyses guidelines [11].
Crowdsourcing has several benefits. Crowdsourcing provides Criteria for Considering Studies for This Review
easy access to a potentially large pool of participants for a The inclusion criteria were as follows:
research problem, particularly for increasing the number of
1. Studies reporting on health, considering the definition
respondents for mining crowd data (eg, Web-based surveys)
proposed by Prpic [12], with the activities of the 3
and active crowdsourcing (eg, data processing). It offers
categories of health:
important time savings, in that a large number of contributors
• Health promotion: disease detection and surveillance,
working in parallel reduces the time required to perform a fixed
behavioral interventions, health literacy, and health
amount of work, mainly saving the elapsed time to collect data.
education
Project organizers can lower the cost of labor inputs. By
• Health research: pharmaceutical research, clinical trials
soliciting ideas from a large group of people through the internet,
and health experiment methodology, and improving
crowdsourcing can be used to speed up innovations, particularly
health care research knowledge
those with challenges.
• Health maintenance (here “health care”): patient- or
Crowdsourcing has been used primarily in nonmedical fields physician-related, diagnostics, medical practice, and
[7]. The Galaxy Zoo project successfully classified about treatment support.
900,000 galaxies with the help of hundreds of thousands of
2. Studies conducted with a crowdsourced population: workers
Web-based volunteers [8]. The eBird project collected more
are recruited by crowdsourcing (ie, recruited via a website
than 48 million bird observations from more than 35,000
[labor markets such as Amazon MTurk or Crowdflower]
contributors [9]. The fields of research using Amazon MTurk
or an open call to a large audience using internet-related
are psychology, marketing, management, business, political
technologies [eg, scientific games or community challenges
science, computer science (improvement of artificial intelligence
with dedicated platforms]) [13]. Studies can refer either to
software, for example, by naming objects to help the computer
a feasibility study (can crowdsourcing be used for a specific
identify the content of a photograph), and neuroscience [6].
task?) or to the use of crowdsourcing to supply data that
Crowdsourcing is becoming the center of attention of the
support a finding in some research activity.
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.2
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
We excluded studies considering structural and molecular specialty journals); impact factor (Clarivate Analytics); average
biology (eg, studies reporting Web-based games to manipulate journal impact factor percentile from Journal Citation Reports
the 3D structures of proteins or moving colored blocks (classified in four categories: >90th percentile, 70th-90th
representing different nucleotide sequences). percentile, <70th percentile, and not indexed); and year of
publication.
Search Method for Identification of Studies
Characteristics of Crowdsourcing Applications in Health
We performed an electronic search of MEDLINE via PubMed
and EMBASE to identify all reports published from inception The following characteristics of crowdsourcing applications
to March 30, 2016, with no restriction on date, language, study were extracted:
design, or publication status (published papers or conference
1. We determined the category of health the study referred to
abstracts). All databases were searched using both controlled
(health promotion, research, or care [12]) and health field
vocabulary (namely, MeSH terms in MEDLINE and Emtree
(eg, public health, surgery, oncology [details in Multimedia
terms in EMBASE) and a wide range of free-text terms. Indeed,
Appendix 4]).
crowdsourced health studies may be a blend of crowdsourcing
2. We classified the tasks into 1 of the 4 categories of
and citizen science (ie, nonprofessionally trained individuals
crowdsourcing tasks defined: problem-solving, data
conducting science-related activities); these terms can be used
processing, surveillance or monitoring, and surveying.
interchangeably and so were included in our search equation.
3. We determined whether the study was led by researchers
We used different terms referring to crowdsourcing, citizen
(ie, a traditional study led by institutionally trained
science, and Web platforms. The search strategy used to search
researchers) or by participants (ie, studies designed and
MEDLINE and EMBASE is in Multimedia Appendix 2. We
operated by patients or citizen scientists) [10].
also screened ClinicalTrials.gov (search strategy in Multimedia
Appendix 3) and the reference lists of previous systematic Logistics of Crowdsourcing and Characteristics of Crowd
reviews [5,10] and selected papers to identify additional studies. Workers
Selection of Studies Considering the logistics of crowdsourcing and characteristics
of crowd workers, the following points were extracted:
Two reviewers (PC and GM) independently examined each title
and abstract identified to exclude irrelevant reports. The 2 1. We defined how the crowdsourcing was applied: whether
reviewers then independently examined full-text articles to a large task was divided into microtasks and distributed to
determine eligibility. Disagreements were discussed to reach workers [13] or whether the same task—a high-difficulty
consensus. We documented the primary reason for exclusion task called a megatask, such as a challenge—was given to
of full-text articles. For ClinicalTrials.gov, only studies with several groups of workers [14].
posted results were included. 2. We extracted the type of platform used (labor markets,
scientific games, mobile phone apps, social media, or
Definition of the Crowdsourcing Tasks
community challenges with dedicated platforms) [13];
We used the classification described by Ranard [5] with 4 tasks
whether monetary incentives were offered and their amount;
of crowdsourcing: (1) problem-solving: to propose empirical
the time to perform the task; whether a data quality
solutions to scientific problems; (2) data processing: to perform
validation was performed; whether the task performed by
several human intelligence microtasks to provide in total an
the crowd workers was compared with that performed by
analysis of a large amount of data; (3) surveillance or
experts (which corresponds to a feasibility study).
monitoring: to find and collect information into a common 3. We extracted the number of crowd workers, the median
location and format such as the creation of collective resources;
age, the proportion of women, their status (eg, researchers,
and (4) surveying: to answer a Web-based survey. Surveillance
physicians, and students), their geographic location, their
or monitoring and surveying belong to mining crowd data
motivations, whether a skill set was required to perform the
described by Khare [4] and are defined as data collected and
task, and whether they had to undergo training and pass a
analyzed by crowd workers for the knowledge discovery
qualification test to be recruited.
process. Problem-solving and data processing belong to active 4. We also assessed the proportion of studies not reporting all
crowdsourcing, which refers to crowd workers recruited to solve
these data.
scientific problems.
Analysis
Data Extraction and Management
The analysis was descriptive. Data are summarized as number
The data were extracted from reports by the two reviewers (PC
(%) for qualitative variables and median (Q1-Q3) for continuous
and GM) who used a standardized data extraction form
variables. All analyses involved the use of R v3.0.2 (R
(provided with the protocol as Multimedia Appendix 1).
Foundation for Statistical Computing, Vienna, Austria) [15].
Disagreements were discussed to reach consensus. From each
study, we extracted the following characteristics. Results
Publication Characteristics of the Study
Systematic Literature Search
Publication characteristics of the study were as follows: Journal
The flow of study selection is in Multimedia Appendix 5.
Citation Reports categories (ie, general medicine and health
Briefly, the electronic search yielded 2354 references; 326 were
care science, biomedical informatics and technology, or medical
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.3
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
selected for further evaluation, and 202 studies were included Mapping of Crowdsourcing Applications in Health
(182 published papers and 20 conference abstracts [3,16-216]).
Crowdsourcing applications were more frequent in studies of
More than half of the included studies (108/202, 53.5%) were health promotion (91/202, 45.0%) and health research (72/202,
published during the last 2 years. The median impact factor of 35.7%) than health care (39/202, 19.3%). More than half of the
the journals of publication was 3.2 (Q1-Q3: 2.1-3.5); for 42/202 studies concerned active crowdsourcing (data processing
studies (20.8%), reports were published in a journal with very (99/202, 49.0%) and problem-solving (12/202, 5.9%)) and 45%
high relative impact factor (>90th percentile of journal impact of the studies were about mining crowd data (surveying (70/202,
factors averaged across journal categories). Reports for 34.6%) and surveillance or monitoring (21/202, 10.4%)).
two-thirds of studies (129/202) were published in medical Examples of crowdsourced tasks by health category are provided
specialty journals and for one-fourth (50/202) in biomedical in Figure 2.
informatics and technology journals. All these publication
Almost 50% of the studies related to health promotion used
characteristics are in Figure 1. A total of 9 studies corresponded
surveys to conduct their research compared with studies related
to randomized controlled trials, only 1 with results posted on
to health care, which used mainly data processing activity. All
ClinicalTrials.gov.
included studies were led by researchers.
Figure 1. Publication characteristics of included studies. Two-thirds of the studies have been published in one of the 18 medical specialty journals,
covering almost all medical fields showing the widespread use of crowdsourcing, and sometimes in a journal with very high relative impact factor.
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.4
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Figure 2. Examples of crowdsourced tasks according to health category. EEG: electroencephalography.
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.5
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Figure 3. Mapping of crowdsourcing applications in health. Sankey diagram representing the distribution of medical fields applying crowdsourcing
for each of the 4 types of task. Width of links is proportional to the number of studies. Medical specialties: anatomopathology (n=3), cardiology (n=5),
dermatology (n=5), endocrinology (n=1), gynecology (n=2), infectiology (n=6), nephrology (n=1), neurology (n=7), pediatrics (n=2), pneumology
(n=3), radiology (n=2),and rheumatology (n=2).
In Figure 3, we provide a mapping of crowdsourcing Reporting of the Logistics of Crowdsourcing and Crowd
applications in health, detailing the medical fields applied to Workers’ Characteristics
each type of task.
For data processing and surveillance or monitoring, a large task
Data Processing was divided into microtasks and distributed to crowd workers.
For problem-solving, a megatask was given to several groups
One-fourth of studies (27/99) involved public health, one-fifth
of crowd workers. We identified 7 challenges in our sample. A
(21/99) involved surgery, and one-fifth involved medical
Web platform was used in 190/202 studies (94.1%), of which
specialties (20/99). For example, in the Ghani et al study,
133/190 (70.0%) were labor markets (eg, Amazon MTurk; Table
published in 2016, crowd workers used the Global Evaluative
1).
Assessment of Robotic Skills tool to assess surgical skill in a
video recording of a nerve-sparing robot-assisted radical Crowd workers’ characteristics and crowdsourcing logistics
prostatectomy [77]. were poorly reported. Reports for almost one-fourth of studies
(47/202) did not mention monetary incentives, and for two-thirds
Surveying
of studies (130/202), the time to perform the task was not
A total of 43% of studies (30/70) involved public health, and mentioned. Crowd workers’ characteristics were frequently
37% (26/70) involved psychiatry. In the Stroh et al study, missing: age and gender were not reported for about 60% of the
published in 2015, crowd workers completed a questionnaire studies (128/202 and 105/202, respectively), and crowd workers’
related to public views on organ donation for people who need location was not reported for one-fourth of the studies (50/202).
transplantation because of alcohol abuse [192]. The survey
measured attitudes on liver transplantation in general and early For 109/202 studies (53.9%), reports mentioned monetary
transplantation for this patient population. incentives, mainly less than US $1 to perform a task. When
reported, the time needed to perform the task was mostly less
Surveillance or Monitoring
than 10 min (42/72, 58% of studies). For one-fourth of studies
A total of 43% of studies (9/21) concerned public health and (54/202), reports mentioned using data quality validation, mainly
24% (5/21) concerned dermatology. In the Merchant et al study, by attention check questions (19/54, 35%) or by replicating the
published in 2012, during 2 months, crowd workers had to task by several crowd workers (16/54, 30%). About one-fifth
locate, photograph, and submit the most eligible automated of studies (36/202) compared crowd workers’ performance with
external defibrillator in Philadelphia [168]. that of experts (corresponding in these cases to a feasibility
study), mainly for evaluating surgical skills (15/36, 42%).
Problem-Solving
The number of crowd workers was reported for 176 studies
One-third of studies (4/12) concerned oncology and one-fourth
(87.1%), and the size of the crowd varied from 5 to about 2
(3/12) concerned medical education. In the Margolin et al study,
million, with median 424 (first and third quartiles Q1-Q3:
published in 2013, crowd workers were challenged during 6
167-802; Table 2). When specified, crowd workers’ median age
months to develop computational models that predict overall
was 34 years (Q1-Q3: 32-36) and 55% were men. Crowd
survival of breast cancer patients based on clinical information
workers were recruited nationally in 93/152 studies (61.2%),
[131].
mainly the United States (83/93, 89%).
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.6
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Table 1. Logistics of crowdsourcing in systematic review studies.
Characteristics Statistics (N=202)
Type of platform used, n (%)
Web platform 190 (94.0)
Labor marketsa 133 (70.0)
Social mediab 10 (5.3)
Labor markets and social media 2 (1.0)
Community challenge 7 (3.7)
Scientific games 6 (3.2)
Other websites 32 (16.8)
Mobile apps 12 (6.0)
Monetary incentives to perform a task,n (%)
Not reported 47 (23.2)
Yes 109 (54.0)
≤US $0.1 15 (13.8)
US $0.2 to 0.5 30 (27.5)
US $0.6 to 1 23 (21.1)
>US $1 23 (21.1)
Amount not specified 18 (16.5)
No 46 (22.8)
Time to perform a task, n (%)
Not reported 130 (64.4)
Reported 72 (35.6)
≤1 min 17 (24)
2 to 10 min 25 (35)
11 to 30 min 21 (29)
30 min to 1 hr 9 (12)
Data quality validation, n (%)
Not reported 148 (73.3)
Yes 54 (26.7)
Task replication by several CWsc 16 (30)
Attention check questions 19 (35)
Discriminative questions 12 (22)
Limited timing for the task 4 (7)
Not specified 3 (6)
CW performance compared with experts, n (%)
Yes 36 (17.8)
aAmazon MTurk, Crowdflower.
bMainly Facebook, Twitter, LinkedIn, and Curetogether.
cCW: crowd worker.
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.7
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Table 2. Characteristics of crowd workers.
Characteristics Statistics (N=202)
Size of the crowd
Median (Q1-Q3) 424 (167-802)
Not reported, n (%) 26 (12.9)
Reported, n (%) 176 (87.1)
<100 20 (11.4)
100-499 78 (44.3)
500-999 41 (23.3)
1000-4999 25 (14.2)
5000-10,000 7 (4.0)
>10,000 5 (2.8)
Geographic location, n (%)
Not reported 50 (24.8)
Reported 152 (75.2)
International 59 (38.8)
National 93 (61.2)
United States 83 (89.2)
Canada 2 (2.2)
The Netherlands 3 (3.2)
Othera 5 (5.4)
Age
Median (Q1-Q3) 34 (32-36)
Not reported, n (%) 128 (63.4)
Reported, n (%) 74 (36.6)
Gender
Not reported, n (%) 105 (52)
Mean proportion of men (%) 55.0
Status, n (%)
Not reported 61 (30.2)
Reported 141 (69.8)
Anyone 51 (36.2)
People graduated from college 35 (24.8)
People with specificities 19 (13.5)
Patients 14 (9.9)
Medical or health care providers 9 (6.4)
Researchers 8 (5.7)
Students 5 (3.5)
Skill set required, n (%)
Not specified 128 (63.4)
Yes 74 (36.6)
Master qualificationb 44 (60)
Speak English 14 (19)
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.8
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Characteristics Statistics (N=202)
Scientific background 8 (11)
Medical background 8 (10)
Qualification test, n (%)
Not reported 34 (16.8)
Yes 26 (12.9)
No 142 (70.3)
Training of workers, n (%)
Not reported 31 (15.3)
Yes 22 (10.9)
No 149 (73.8)
aIndia, Australia, Israel, China, and South Korea.
bDefined as “consistently completing human intelligence tasksof a certain type with a high degree of accuracy across a variety of requesters.”
The motivations of crowd workers were recorded for 5/202 up to 2011 [10]. Our mapping is more exhaustive—focused on
studies (2.5%) and included fun, curiosity, altruism, health research but also health promotion and health care—and
compensation, contribution to an important cause, personal up-to-date. Many of our studies (80%) were published after the
reasons, research education, and advancing science last search date of the Ranard et al’s systematic review [5]. This
[82,139,168,183,193]. A skill set was required in 74/202 studies point highlights the increasing use of crowdsourcing in health
(36.7%); for 60%, this involved previous experience in during the last few years. Indeed, many health fields have since
crowdsourcing. For two-thirds of studies (128/202), a specific used crowdsourcing, with 20 medical fields identified in our
skill set required was not specified. For only 12.8% of studies, systematic review compared with 8 fields in the Ranard et al’s
the Web-based tasks (26/202) required passing a qualification study [5]. Moreover, crowdsourcing use is still growing, as
test, and for 10.9%, (22/202), they required training. shown by the 11 articles published in Journal of Medical
Internet Researchsince our last search date, mainly involving
Discussion a survey task (9/11, 82%) [217-227]. Our study has some
limitations. First, we did not search the gray literature to identify
Principal Findings some unpublished studies. However, the EMBASE search
In this systematic review of the use of crowdsourcing in studies allowed us to identify 20 studies (10%) corresponding to
of health promotion, research, and care, we included 202 studies, conference abstracts. Second, we did not search Google Scholar
mainly published in the last 2 years with for one-fifth of a because of the number of records found (about 30,000).
publication in a journal with very high relative IF. Data Screening all these references would be extremely
processing was the most frequent type of task used (mainly in time-consuming for only 2 reviewers without using a
public health and surgery), followed by surveying (public health crowdsourcing process. Third, we did not include studies related
and psychiatry), then surveillance or monitoring (public health to biology, such as studies using the “Fold it” platform to solve
and dermatology), and finally problem-solving (oncology). protein-folding problems [228]. We did not consider this topic
Labor market platforms (Amazon MTurk) were mainly used. in our definition of health. Finally, we included only
The description of crowdsourcing logistics and crowd workers’ crowdsourcing performed via the internet. For example, we did
characteristics were frequently missing from reports. When not include studies in which the crowdsourced tasks were
reported, the median size of the crowd was less than 500; crowd performed in a particular workshop without individual data
workers’ median age was around 34 years and 55% were men. collected online. Therefore, we may have underestimated the
Crowd workers were mainly recruited in the United States. A number of studies using crowdsourcing in health.
previous experience in crowdsourcing was required in about
Every health category (promotion, research, and care) has a
60% of the studies, whereas passing a qualification test or
potential need for human computing power that crowdsourcing
training was only needed in about 12%. The time needed to
could fulfill to accelerate the process. Our systematic review,
perform the task was mostly less than 10 min for monetary
focusing on peer-reviewed papers, may have not captured some
incentives less than US $1. Data quality validation was used in
kinds of crowdsourcing. Studies recruiting crowd workers with
less than one-third of studies.
social media platforms were few in our selection (12/202 studies
Our systematic review has advantages over previous ones on [5.9%]). This type of recruitment seems less attractive than
the same topic [5,10]. The systematic review conducted by labor markets, although it is free and easier to use, perhaps
Ranard et al in March 2013 described the scope of because it is considered less reliable or used for purposes other
crowdsourcing in health and medical research but included only than publication. Another way of exploiting social media data
21 articles [5]. The narrative review conducted by Swan is under development, whereby tweets referring to a specific
described the use of crowdsourcing in health research studies disease are analyzed as part of a health maintenance approach
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.9
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
(eg, HIV in the Adrover et al’s study to identify adverse effects processing tasks, 36/202 feasibility studies (17.8%) compared
of drug treatment in tweets using crowdsourcing [229]). crowd workers’ performance with that of an expert group
Considering health research, a fundamental aspect of this considered as reference. These studies mainly considered
crowdsourcing is that it allows research to be performed with surgical skills evaluation (15/36, 42%) and parasite identification
patients and not only tothem or onthem. However, studies with in infectious diseases (4/36, 11%). At each time, the performance
patients as crowd workers represented only 10% of our included of crowd workers was similar to that of the reference group.
studies, perhaps because the primary aim of collecting these However, because the participation is anonymous and
data was not to conduct research with the data. Nevertheless, compensated, participants may provide unsatisfactory quality
in 2013, the PatientsLikeMe platform [230] had more than data. In our review, 54/202 studies (26.7%) reported using data
220,000 members sharing health data on more than 2000 quality validation. Several types of validation techniques were
diseases and conditions [231]. Using these data and conducting found, from inserting random questions with known answers
research with the data represent a great future challenge of into the task, to screening for crowd workers who were
mining crowd data and a real opportunity to collect large incorrectly marking answers (31/54, 57%) and to comparing
amounts of data on symptoms of diseases, drug efficacy, or responses among multiple crowd workers to discard outliers
adverse events to solve a wide range of health issues with a (16/54, 30%). The second concern is its unethical aspect:
more real-life approach. Crowdsourcing also has potential in Amazon MTurk is a bargain for researchers but not for crowd
health promotion, especially preventive medicine, by taking it workers [234]. Indeed, many MTurk tasks are completed by a
one step further. For example, specific tips in the form of slides small set of workers who spend long hours on the website, many
or films could be added to the end of a Web-based survey about with low income.
addiction to conduct a behavioral intervention, in addition to a
A detailed description of the crowdsourcing logistics in the
simple survey. In some cases, data processing tasks may require
Methods section and all the characteristics of the crowd workers
thinking about a healthier lifestyle, for example, by suggesting
(population of the study) should be provided in high-quality
healthier alternatives in addition to gathering information on
research, even if its importance depends on the type of study.
the nutritional characteristics of packaged foods. Such
In cases of surveying and surveillance or monitoring studies
crowdsourced tasks could be expanded to change dietary
related to illness, crowd workers’ characteristics need to be
behaviors, exercise, or adherence to treatment. Finally, the
precisely described to better interpret the study findings and to
combination of crowdsourcing and mobile health technologies
judge the external validity. In cases of data processing and
could be the ultimate step in providing an ideal vehicle for
problem-solving, crowd workers’ characteristics also need to
behavioral interventions that can reach users in real time, in real
be reported to allow reproducibility of studies and to select more
life, without being resource-intensive.
quickly and more easily the best population of crowd workers
Crowdsourcing allows for a large number of crowd workers to for a future similar study. In our review, the lack of details of
be mobilized in record time and at low cost. For instance, in crowd workers’ characteristics in one-third of the included
Peabody et al’s study [158], experts completed 318 video ratings studies impedes the interpretation of results of these studies.
in 15 days, but crowd workers completed 2531 ratings in 21 Rather than being a virtually infinite subject pool, crowd workers
hours. These crowdsourced resources might be further harnessed are far less diverse than was previously thought. As we found,
in a world of high health costs. Crowdsourcing also allows for although crowd workers should be recruited from all over the
speeding up innovations, when used in the form of collaborative world, 61% were actually recruited nationally, mainly the United
scientific competitions—challenges—to solve diverse and States (89%). Previously, crowd workers were mainly young,
important biomedical problems. Problem-solving was the fourth urban, and single and more often had postsecondary education
task we identified in terms of frequency, and only 7 challenges [6]. In our review, the median age of crowd workers was 34
were individualized, perhaps because challenges are an emerging years, 55% were men, and half reported a high level of
form of crowdsourcing, which should be more prominent in the education. Therefore, logistics of crowdsourcing and crowd
next few years and lead to more publications [232]. In future, workers’ characteristics must be reported, and standardized
it will be necessary to facilitate and promote the use of this type guidelines on crowdsourcing metrics that needed to be collected
of crowdsourced tasks in health research, given the amount of and reported could be useful to improve the quality of such
data to be considered (big data) and the complexity of medical studies.
issues that will require increasingly skilled and qualified
Conclusions
individuals to resolve them.
Crowdsourcing appears to be a trendy, efficient, competitive,
As previously mentioned, crowdsourcing has many advantages:
and useful tool to improve health actions, whether in preventive
improved cost, speed, quality, flexibility, scalability, and
medicine, research, or care. Its use in health is increasing,
diversity. However, some points that remain controversial
particularly in public health, psychiatry, surgery, and oncology.
include the impact of crowdsourcing on product quality or its
Crowdsourcing allows for access to a large pool of participants,
unethical aspect. The first remaining potential concern of
saves time to collect data, lowers costs, and speeds up
crowdsourced studies in health is the validity of their results.
innovations. Each health field could benefit from some tasks
Some studies have assessed whether we should trust Web-based
that could be crowdsourced to facilitate advances in research.
studies, and it appears that the data provided by internet methods
To optimize the use of crowdsourcing in health, the logistics of
have at least as good quality as those provided by traditional
crowdsourcing and crowd workers’ characteristics must be
paper-and-pencil methods [233]. In our review, for data
reported.
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.10
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
Acknowledgments
The authors thank Ludovic Trinquart for helping in the initial protocol conception. The authors also thank Laura Smales
(BioMedEditing, Toronto, Canada) for language revision of the manuscript and Elise Diard for the layout of the figures. This
study was supported by a grant from the French National Cancer Institute (Institut National du Cancer, INCa; no.
2016-020/058/AB-KA). The funding source had no role in the design of this study, its execution, analyses, interpretation of the
data, and decision to submit results.
Authors' Contributions
PC was involved in the study conception, selection of trials, data extraction, data analysis, interpretation of results, and drafting
the manuscript and revision. GM was involved in the study conception, the selection of trials, and data extraction. MB was
involved in the interpretation of results and drafting the manuscript. AV was involved in the study conception, data analysis,
interpretation of results, and drafting the manuscript and revision. PR was involved in the study conception, interpretation of
results, and drafting the manuscript and revision. All authors read and approved the final manuscript.
Conflicts of Interest
None declared.
Multimedia Appendix 1
Protocol of the systematic review.
[PDF File (Adobe PDF File), 12KB- jmir_v20i5e187_app1.pdf]
Multimedia Appendix 2
Search terms for MEDLINE and EMBASE (March 30, 2016).
[PDF File (Adobe PDF File), 241KB- jmir_v20i5e187_app2.pdf]
Multimedia Appendix 3
Search strategy for ClinicalTrials.gov.
[PDF File (Adobe PDF File), 17KB- jmir_v20i5e187_app3.pdf]
Multimedia Appendix 4
Details of the health fields considered.
[PDF File (Adobe PDF File), 31KB- jmir_v20i5e187_app4.pdf]
Multimedia Appendix 5
Flow diagram of selection of studies applying crowdsourcing in health.
[PDF File (Adobe PDF File), 157KB- jmir_v20i5e187_app5.pdf]
References
1. Sobel D. Longitude: The True Story of a Lone Genius Who Solved the Greatest Scientific Problem of His Time. New York:
Walker Publishing Company; 1995.
2. Wikipedia. Crowdsourcing URL: https://en.wikipedia.org/wiki/Crowdsourcing[accessed 2018-02-06] [WebCite Cache ID
6x1xeHjIX]
3. Deal SB, Lendvay TS, Haque MI, Brand T, Comstock B, Warren J, et al. Crowd-sourced assessment of technical skills: an
opportunity for improvement in the assessment of laparoscopic surgical skills. Am J Surg 2016 Feb;211(2):398-404. [doi:
10.1016/j.amjsurg.2015.09.005] [Medline: 26709011]
4. Khare R, Good BM, Leaman R, Su AI, Lu Z. Crowdsourcing in biomedicine: challenges and opportunities. Brief Bioinform
2016 Jan;17(1):23-32. [doi: 10.1093/bib/bbv021] [Medline: 25888696]
5. Ranard BL, Ha YP, Meisel ZF, Asch DA, Hill SS, Becker LB, et al. Crowdsourcing--harnessing the masses to advance
health and medicine, a systematic review. J Gen Intern Med 2014 Jan;29(1):187-203 [FREE Full text] [doi:
10.1007/s11606-013-2536-8] [Medline: 23843021]
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.11
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
6. Bohannon J. Psychology. Mechanical Turk upends social sciences. Science 2016 Jun 10;352(6291):1263-1264. [doi:
10.1126/science.352.6291.1263] [Medline: 27284175]
7. Dawson R, Bynghall S. Getting Results From Crowds: The Definitive Guide to Using Crowdsourcing to Grow Your
Business. San Francisco: Advanced Human Technologies; 2012.
8. Lintott C, Schawinski K, Bamford S, Slosar A, Land K, Thomas D, et al. Galaxy Zoo 1: data release of morphological
classifications for nearly 900 000 galaxies. Mon Not R Astron Soc 2011 Jan 01;410(1):166-178. [doi:
10.1111/j.1365-2966.2010.17432.x]
9. Marris E. Supercomputing for the birds. Nature 2010 Aug 12;466(7308):807. [doi: 10.1038/466807a] [Medline: 20703280]
10. Swan M. Crowdsourced health research studies: an important emerging complement to clinical trials in the public health
research ecosystem. J Med Internet Res 2012;14(2):e46 [FREE Full text] [doi: 10.2196/jmir.1988] [Medline: 22397809]
11. Liberati A, Altman DG, Tetzlaff J, Mulrow C, Gøtzsche PC, Ioannidis JP, et al. The PRISMA statement for reporting
systematic reviews and meta-analyses of studies that evaluate health care interventions: explanation and elaboration. J Clin
Epidemiol 2009 Oct;62(10):e1-34 [FREE Full text] [doi: 10.1016/j.jclinepi.2009.06.006] [Medline: 19631507]
12. Prpić PJ. Health Care Crowds: Collective Intelligence in Public Health Internet. Rochester, NY: Social Science Research
Network; 2015.
13. Michelucci P, Dickinson JL. Human Computation. The power of crowds. Science 2016 Jan 01;351(6268):32-33. [doi:
10.1126/science.aad6499] [Medline: 26721991]
14. Silberzahn R, Uhlmann EL. Crowdsourced research: many hands make tight work. Nature 2015 Oct 08;526(7572):189-191.
[doi: 10.1038/526189a] [Medline: 26450041]
15. R-project. Vienna, Austria: R Foundation for Statistical Computing R: A language and environment for statistical computing
URL: https://www.r-project.org/[accessed 2018-04-05] [WebCite Cache ID 6yS5IavS3]
16. Adams SA. Using patient-reported experiences for pharmacovigilance? Stud Health Technol Inform 2013;194:63-68.
[Medline: 23941932]
17. Aghdasi N, Bly R, White LW, Hannaford B, Moe K, Lendvay TS. Crowd-sourced assessment of surgical skills in
cricothyrotomy procedure. J Surg Res 2015 Jun 15;196(2):302-306. [doi: 10.1016/j.jss.2015.03.018] [Medline: 25888499]
18. Albarqouni S, Baur C, Achilles F, Belagiannis V, Demirci S, Navab N. AggNet: deep learning from crowds for mitosis
detection in breast cancer histology images. IEEE Trans Med Imaging 2016 Dec;35(5):1313-1321. [doi:
10.1109/TMI.2016.2528120] [Medline: 26891484]
19. Kuerbis A, Muench F. Normative feedback for adult drinkers: intervention through developing discrepancy or by validating
pre-existing worries? 2015 Presented at: Personalized Feedback Interventions for Individuals with Substance-Related
Problems; January 14-18, 2015; New Orleans, LA.
20. Allam A, Schulz PJ, Nakamoto K. The impact of search engine selection and sorting criteria on vaccination beliefs and
attitudes: two experiments manipulating Google output. J Med Internet Res 2014 Apr;16(4):e100 [FREE Full text] [doi:
10.2196/jmir.2642] [Medline: 24694866]
21. Alvare G, Gordon R. CT brush and CancerZap!: two video games for computed tomography dose minimization. Theor
Biol Med Model 2015 May 12;12(1):7. [doi: 10.1186/s12976-015-0003-4]
22. Alvaro N, Conway M, Doan S, Lofi C, Overington J, Collier N. Crowdsourcing Twitter annotations to identify first-hand
experiences of prescription drug use. J Biomed Inform 2015 Dec;58:280-287 [FREE Full text] [doi: 10.1016/j.jbi.2015.11.004]
[Medline: 26556646]
23. Andover MS. Non-suicidal self-injury disorder in a community sample of adults. Psychiatry Res 2014 Oct;219(2):305-310.
[doi: 10.1016/j.psychres.2014.06.001]
24. Arditte KA, Morabito DM, Shaw AM, Timpano KR. Interpersonal risk for suicide in social anxiety: the roles of shame and
depression. Psychiatry Res 2016 May;239:139-144. [doi: 10.1016/j.psychres.2016.03.017]
25. Armstrong AW, Cheeney S, Wu J, Harskamp CT, Schupp CW. Harnessing the power of crowds. Am J Clin Dermatol
2012;13(6):405-416. [doi: 10.2165/11634040-000000000-00000]
26. Armstrong AW, Harskamp CT, Cheeney S, Schupp CW. Crowdsourcing for research data collection in rosacea. Dermatol
Online J 2012 Mar 15;18(3):15. [Medline: 22483526]
27. Armstrong AW, Harskamp CT, Cheeney S, Wu J, Schupp CW. Power of crowdsourcing: novel methods of data collection
in psoriasis and psoriatic arthritis. J Am Acad Dermatol 2012 Dec;67(6):1273-1281.e9. [doi: 10.1016/j.jaad.2012.05.013]
[Medline: 22818792]
28. Armstrong AW, Harskamp CT, Cheeney S, Schupp CW. Crowdsourcing in eczema research: a novel method of data
collection. J Drugs Dermatol 2012 Oct;11(10):1153-1155. [Medline: 23285714]
29. Armstrong AW, Wu J, Harskamp CT, Cheeney S, Schupp CW. Crowdsourcing for data collection: a pilot study comparing
patient-reported experiences and clinical trial data for the treatment of seborrheic dermatitis. Skin Res Technol 2013
Feb;19(1):55-57. [doi: 10.1111/j.1600-0846.2012.00667.x] [Medline: 22891700]
30. Bahk CY, Goshgarian M, Donahue K, Freifeld CC, Menone CM, Pierce CE, et al. Increasing patient engagement in
pharmacovigilance through online community outreach and mobile reporting applications: an analysis of adverse event
reporting for the Essure device in the US. Pharmaceut Med 2015;29(6):331-340 [FREE Full text] [doi:
10.1007/s40290-015-0106-6] [Medline: 26635479]
http://www.jmir.org/2018/5/e187/ J Med Internet Res 2018 | vol. 20 | iss. 5 | e187 | p.12
XSL FO (page number not for citation purposes)
•
RenderX

JOURNAL OF MEDICAL INTERNET RESEARCH Créquit et al
31. Baruch RL, Vishnevsky B, Kalman T. Split-care patients and their caregivers: how collaborative is collaborative care? J
Nerv Ment Dis 2015 Jun;203(6):412-417 [FREE Full text] [doi: 10.1097/NMD.0000000000000297] [Medline: 25938507]
32. Bell RA, McGlone MS, Dragojevic M. Bacteria as bullies: effects of linguistic agency assignment in health message. J
Health Commun 2014 Sep;19(3):340-358. [doi: 10.1080/10810730.2013.798383] [Medline: 24015807]
33. Bevelander KE, Kaipainen K, Swain R, Dohle S, Bongard JC, Hines PD, et al. Crowdsourcing novel childhood predictors
of adult obesity. PLoS One 2014 Feb;9(2):e87756 [FREE Full text] [doi: 10.1371/journal.pone.0087756] [Medline: 24505310]
34. Bickel WK, George WA, Franck CT, Terry ME, Jarmolowicz DP, Koffarnus MN, et al. Using crowdsourcing to compare
temporal, social temporal, and probability discounting among obese and non-obese individuals. Appetite 2014 Apr;75:82-89
[FREE Full text] [doi: 10.1016/j.appet.2013.12.018] [Medline: 24380883]
35. Bickel WK, Jarmolowicz DP, Mueller ET, Franck CT, Carrin C, Gatchalian KM. Altruism in time: social temporal discounting
differentiates smokers from problem drinkers. Psychopharmacology (Berl) 2012 Nov;224(1):109-120. [doi:
10.1007/s00213-012-2745-6] [Medline: 22644127]
36. Borgida E, Loken B, Williams AL, Vitriol J, Stepanov I, Hatsukami D. Assessing constituent levels in smokeless tobacco
products: a new approach to engaging and educating the public. Nicotine Tob Res 2015 Nov;17(11):1354-1361 [FREE
Full text] [doi: 10.1093/ntr/ntv007] [Medline: 25634934]
37. Bow HC, Dattilo JR, Jonas AM, Lehmann CU. A crowdsourcing model for creating preclinical medical education study
tools. Acad Med 2013 Jun;88(6):766-770. [doi: 10.1097/ACM.0b013e31828f86ef] [Medline: 23619061]
38. Boynton MH, Richman LS. An online daily diary study of alcohol use using Amazon's Mechanical Turk. Drug Alcohol
Rev 2014 Jul;33(4):456-461 [FREE Full text] [doi: 10.1111/dar.12163] [Medline: 24893885]
39. Brady CJ, Villanti AC, Pearson JL, Kirchner TR, Gupta OP, Shah CP. Rapid grading of fundus photographs for diabetic
retinopathy using crowdsourcing. J Med Internet Res 2014 Oct 30;16(10):e233 [FREE Full text] [doi: 10.2196/jmir.3807]
[Medline: 25356929]
40. Brooks SC, Simmons G, Worthington H, Bobrow BJ, Morrison LJ. The PulsePoint Respond mobile device application to
crowdsource basic life support for patients with out-of-hospital cardiac arrest: challenges for optimal implementation.
Resuscitation 2016 Jan;98:20-26. [doi: 10.1016/j.resuscitation.2015.09.392] [Medline: 26475397]
41. Brown AW, Allison DB. Using crowdsourcing to evaluate published scientific literature: methods and example. PLoS One
2014;9(7):e100647 [FREE Full text] [doi: 10.1371/journal.pone.0100647] [Medline: 24988466]
42. Burger JD, Doughty E, Khare R, Wei CH, Mishra R, Aberdeen J, et al. Hybrid curation of gene-mutation relations combining
automated extraction and crowdsourcing. Database (Oxford) 2014;2014:- [FREE Full text] [doi: 10.1093/database/bau094]
[Medline: 25246425]
43. Cabrera LY, Fitz NS, Reiner PB. Reasons for comfort and discomfort with pharmacological enhancement of cognitive,
affective, and social domains. Neuroethics 2014 Oct 10;8(2):93-106. [doi: 10.1007/s12152-014-9222-3]
44. Campisi J, Folan D, Diehl G, Kable T, Rademeyer C. Social media users have different experiences, motivations, and
quality of life. Psychiatry Res 2015 Aug 30;228(3):774-780. [doi: 10.1016/j.psychres.2015.04.042] [Medline: 26054935]
45. Candido Dos Reis FJ, Lynn S, Ali HR, Eccles D, Hanby A, Provenzano E, et al. Crowdsourcing the general public for large
scale molecular patholog

## 引用

```

```
