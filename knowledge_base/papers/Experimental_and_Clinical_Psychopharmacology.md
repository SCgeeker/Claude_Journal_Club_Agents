---
title: Experimental and Clinical Psychopharmacology
authors: Amazon Mechanical, W. Stoops, Behavioral Science, The Use, Addiction Science, Alcohol Research, American Psychological, C. Strickland, Clinical Psychopharmacology, National Science
year: N/A
keywords: Crowdsourcing, Internet, mTurk, Sampling, Substance Use
created: 2025-11-23 18:15:57
---

# Experimental and Clinical Psychopharmacology

## 基本信息

- **作者**: Amazon Mechanical, W. Stoops, Behavioral Science, The Use, Addiction Science, Alcohol Research, American Psychological, C. Strickland, Clinical Psychopharmacology, National Science
- **年份**: N/A
- **關鍵詞**: Crowdsourcing, Internet, mTurk, Sampling, Substance Use

## 摘要

Crowdsourcing, the use of the Internet to outsource work to a large number of people, has
witnessed a dramatic growth over the past decade. One popular crowdsourcing option,
Amazon Mechanical Turk (mTurk), is now commonly used to sample participants for
psychological research. Addiction science is positioned to benefit greatly from crowdsourced
sampling due to the ability to efficiently and effectively tap into populations with specific
behavioral and health histories. The primary objective of this review is to describe the utility of
crowdsourcing, broadly, and mTurk, specifically, for conducting research relevant to substance
use and misuse. Studies in psychological and other health science have supported the reliability
and validity of data gathered using crowdsourced samples. Promising research relevant to
addiction science has also been conducted, including studies using cross-sectional designs and
those for measure development purposes. Preliminary work using longitudinal methods and for
interventions development has also revealed the potential of mTurk for studying alcohol and
other drug use through these designs. Additional studies are needed to better understand the
benefits, as well as the limits and constraints, of research conducted through crowdsourced
online platforms. Crowdsourcing, such as on mTurk, can ultimately provide an important
complement to existing methods used in human laboratory, clinical trial, community intervention,
and epidemiological research. The combinations of these methodological approaches could
help improve the rigor, reproducibility, and overall scope of research conducted in addiction
science.
Keywords: Crowdsourcing; Internet; mTurk; Sampling; Substance Use

## 研究背景

## 研究方法

## 主要結果

## 討論與結論

## 個人評論

## 相關文獻

## 完整內容

Experimental and Clinical Psychopharmacology
Manuscript version of
The Use of Crowdsourcing in Addiction Science Research: Amazon Mechanical Turk
Justin C. Strickland, William W. Stoops
Funded by:
• National Science Foundation
© 2018, American Psychological Association. This manuscript is not the copy of record and may not exactly
replicate the final, authoritative version of the article. Please do not copy or cite without authors’ permission.
The final version of record is available via its DOI: https://dx.doi.org/10.1037/pha0000235
This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.

Running head: ADDICTION SCIENCE MECHANICAL TURK 1
The Use of Crowdsourcing in Addiction Science Research: Amazon Mechanical Turk
Justin C. Strickland, M.S. and William W. Stoops, Ph.D.
Author Note
Justin C. Strickland and William W. Stoops, Department of Psychology, University of
Kentucky; William W. Stoops, Department of Behavioral Science, University of Kentucky;
Department of Psychiatry, University of Kentucky; Center on Drug and Alcohol Research,
University of Kentucky.
This review was supported by the National Science Foundation (Grant 1247392 to JCS).
This funding source had no role in the preparation and submission of the manuscript. Data
presented in this review have not been presented elsewhere. The authors have no financial
conflicts of interest in regard to this research.
Correspondence concerning this article should be addressed to Justin C. Strickland, 171
Funkhouser Drive, Lexington, KY 40506-0044. Email: justrickland@uky.edu Phone: 859-257-
5388. Facsimile: 859-257-7684.
Tables: 1
Figures: 3
Abstract Word Count: 230
Body of Text Word Count: 9724

ADDICTION SCIENCE MECHANICAL TURK 2
Abstract
Crowdsourcing, the use of the Internet to outsource work to a large number of people, has
witnessed a dramatic growth over the past decade. One popular crowdsourcing option,
Amazon Mechanical Turk (mTurk), is now commonly used to sample participants for
psychological research. Addiction science is positioned to benefit greatly from crowdsourced
sampling due to the ability to efficiently and effectively tap into populations with specific
behavioral and health histories. The primary objective of this review is to describe the utility of
crowdsourcing, broadly, and mTurk, specifically, for conducting research relevant to substance
use and misuse. Studies in psychological and other health science have supported the reliability
and validity of data gathered using crowdsourced samples. Promising research relevant to
addiction science has also been conducted, including studies using cross-sectional designs and
those for measure development purposes. Preliminary work using longitudinal methods and for
interventions development has also revealed the potential of mTurk for studying alcohol and
other drug use through these designs. Additional studies are needed to better understand the
benefits, as well as the limits and constraints, of research conducted through crowdsourced
online platforms. Crowdsourcing, such as on mTurk, can ultimately provide an important
complement to existing methods used in human laboratory, clinical trial, community intervention,
and epidemiological research. The combinations of these methodological approaches could
help improve the rigor, reproducibility, and overall scope of research conducted in addiction
science.
Keywords: Crowdsourcing; Internet; mTurk; Sampling; Substance Use

ADDICTION SCIENCE MECHANICAL TURK 3
Public Significance Statement
Crowdsourcing provides an efficient and effective means of recruiting participants for
psychological research. This review summarizes existing evidence supporting the use of
crowdsourcing for topics relevant to addiction science and explains how crowdsourcing may be
used to complement existing research by improving the rigor and reproducibility of study
outcomes.

ADDICTION SCIENCE MECHANICAL TURK 4
Disclosures and Acknowledgements
This review was supported by the National Science Foundation (Grant 1247392 to JCS).
This funding source had no role in the preparation and submission of the manuscript.
All authors have contributed in a significant way to the preparation of this manuscript and
have read and approved the final manuscript.
The authors have no financial conflicts of interest in regard to this research. Data presented
in this review have not been presented elsewhere.

ADDICTION SCIENCE MECHANICAL TURK 5
Using Amazon Mechanical Turk to Conduct Addiction Science Research
Human laboratory, clinical trial, community intervention, and epidemiological approaches
have traditionally guided the conduct of addiction science research with human participants.
Studies from these perspectives have provided insights into basic science (e.g., mechanism or
theory development) and applied science (e.g., interventions development) questions relevant
for substance use and misuse. Nevertheless, the sampling procedures typically used for these
forms of research present well-documented and persistent challenges. Participant recruitment
and retention are notorious problems in human laboratory studies and clinical trials (e.g.,
Backinger et al., 2008; Del Boca & Darkes, 2007; Gul & Ali, 2010; Hansen, Collins, Malotte,
Johnson, & Fielding, 1990; Howard & Beckwith, 1996). Slow participant accrual also means that
it is often challenging for human work to keep pace with that conducted in the animal laboratory,
thereby making translational and collaborative research difficult. Even after devoting extensive
financial resources and time to recruiting participants, small samples can lead to underpowered
studies that lack the number of participants needed for appropriate statistical comparisons
(Button et al., 2013; Ioannidis, 2005). Problems with geographic and/or demographic
homogeneity and subsequently reduced generalizability may also result from sampling that
occurs at a single or limited number of sites (e.g., one addiction clinic or university) (Del Boca &
Darkes, 2007). Although these concerns are not unique to addiction science, such problems are
often compounded when working with hard-to-reach populations, such as illicit-substance users
or individuals with specific behavioral histories (e.g., injection drug use).
An emerging sampling methodology positioned to supplement existing research practice, as
well as to advance current methods, is crowdsourcing. Crowdsourcing refers to the completion
of tasks through a flexible, open call to a large number of people (Estellés-Arolas & González-
Ladrón-de-Guevara, 2012; Howe, 2006). The last decade has witnessed the development and
refinement of open Internet crowdsourcing markets, one popular source being Amazon
Mechanical Turk (mTurk; also commonly abbreviated as AMT, MTurk, or MTURK) (Bohannon,

ADDICTION SCIENCE MECHANICAL TURK 6
2016). This period has also observed a dramatic growth in the use of mTurk to conduct
research in psychological and other health sciences. In fact, the number of manuscripts indexed
in PsycINFO under the terms “Mechanical Turk” or “mTurk” increased nearly 4-fold in the 4-year
span from 2014 to 2017 (Figure 1).
mTurk’s “Internet laboratory” presents numerous strengths, such as the rapid and cost-
effective sampling of diverse and potentially hard-to-reach participants, that may help to offset
limitations related to traditional sampling methods. The primary objective of this review is to
describe the utility of using crowdsourcing and mTurk for research relevant to addiction science.
This objective will be accomplished by first reviewing the historical context of crowdsourcing that
led to its current use in academic research. Next, evidence supporting the validity of mTurk for
clinical and behavioral science, broadly, and addiction science, specifically, will be examined. A
review of existing mTurk studies evaluating substance use and misuse will then be provided to
highlight the realized and potential applications of mTurk for addiction science researchers.
Finally, we conclude with some best practice recommendations for the conduct of crowdsourced
research and remaining questions that future research will be well positioned to address.
The Historical Context of Crowdsourcing
The phrase crowdsourcing may be traced to 2006 when Wired Editors Jeff Howe and Mark
Robinson coined the term referring to the use of the Internet to “outsource work to the crowd”
(Howe, 2006). Multiple definitions have been presented since, but all share a common idea of
creating an open call to the public in order to solve a specific problem (Estellés-Arolas &
González-Ladrón-de-Guevara, 2012). This application of crowdsourcing is present in varied
aspects of personal and professional life. For example, Wikipedia can be considered one of the
most successful crowdsourced projects, wherein the efforts of many individuals were (and still
are) relied upon for the curation of online encyclopedia articles. A particularly compelling
example from the biomedical community was the solution in less than three weeks of the protein
structure of a retroviral protease that had remained unsolved by scientists for over a decade

ADDICTION SCIENCE MECHANICAL TURK 7
(Khatib et al., 2011). Lofty goals, such as the creation of a free online encyclopedia or answers
to otherwise boggling scientific problems, may be accomplished with the division and
aggregation of responsibilities through crowdsourcing.
One popular crowdsourcing option to emerge in the past decade is mTurk (Amazon,
2018). Amazon initially developed mTurk as an online labor market that allowed businesses to
outsource problems to a human workforce. This idea was inspired by a need to complete simple
tasks and other problems that computers are unable to accomplish, are inefficient and error-
prone at solving, or are able to do only after extensive and/or complex coding (e.g., transcribing
receipts, categorizing items). This “human machine” was designed to effectively and efficiently
complete these problems, akin to the origin of the name Mechanical Turk, an 18th century
chess-playing machine that was covertly operated by a human chess master inside the
automaton (Amazon, 2018; Morrison & Morrison, 1997).
Tasks on mTurk are created by requesters and presented as Human Intelligence Tasks
(HITs) that workers can complete (see Table 1 for commonly used terms in
crowdsourcing/mTurk work and their academic research analogs). This work is incentivized by
compensation for each HIT completed. Amazon also collects fees from the requester as a
percentage of this wage to help maintain the service as well as generate a profit from it
(currently 40% that is paid for by the requester, not taken out of the worker’s earnings).
Launched in November 2005, mTurk has rapidly grown over the past decade and Amazon now
boasts over 500,000 users from 190 countries (Amazon, 2018).
The completion of these simple tasks remained mTurk’s primary use in the first years
following its launch. However, psychological scientists soon realized the practical benefits for
generating convenience samples afforded by mTurk (Mason & Suri, 2012). Early adopters in the
research community drew clear parallels between the sampling pool available on mTurk and
undergraduate psychology participant pools that are often used for convenience sampling.
These individuals argued that, unlike psychology participant pools, mTurk provided a sample

ADDICTION SCIENCE MECHANICAL TURK 8
with greater demographic and geographic diversity and potential improvement upon the often
generated W.E.I.R.D. samples (i.e., Western, Educated, Industrialized, Rich, and Democratic
samples) (Henrich, Heine, & Norenzayan, 2010; Landers & Behrend, 2015; Mason & Suri,
2012). This rationale combined with a rapid rate of data collection at relatively low cost has
helped motivate the spread of mTurk through scientific disciplines.
Some of the first studies using mTurk for research purposes belonged to the cognitive and
industrial/organizational psychology literatures (e.g., Crump, McDonnell, & Gureckis, 2013;
Keith, Tay, & Harms, 2017). Personality, clinical, and social psychologists soon also adopted
mTurk as a sampling tool (e.g., Chandler & Shapiro, 2016; Miller, Crowe, Weiss, Maples-Keller,
& Lynam, 2017). More recent years have seen a spread of mTurk to widely varying fields, such
as education research (Follmer, Sperlin, & Suen, 2017), cancer biology (Lee, Arida, & Donovan,
2017), and theoretical biology (Rand, 2012). Although the type and purpose of research may
differ by discipline, common benefits such as enhanced participant diversity, reduced cost, and
improved rates of data collection are often cited as motivating factors behind using mTurk.
Evaluating the Validity of mTurk Samples
The following section provides an overview of seminal work evaluating the validity of mTurk
for psychological and addiction research. Several excellent reviews have recently addressed
aspects of these and related issues (e.g., Chandler & Shapiro, 2016; Keith et al., 2017; Woods,
Velasco, Levitan, Wan, & Spence, 2015). Therefore, rather than provide a comprehensive
review of this large (and rapidly expanding) literature, we instead highlight representative
publications and those directly relevant for addiction science research. All original data
presented in this review (i.e., Figure 2) were collected under protocols approved by the
University of Kentucky Institutional Review Board (IRB #15-1110 “Using Online Sampling
to Examine Population Data for Cognitive-Behavioral Tasks”).

ADDICTION SCIENCE MECHANICAL TURK 9
Demographics and Survey-Taking Behavior of mTurk Participants
mTurk is a form of non-probability convenience sampling that results in samples with a
demographic composition that differs in some ways from nationally representative probability
samples (Chandler & Shapiro, 2016; Landers & Behrend, 2015). Several studies have
attempted to capture how mTurk samples may systematically deviate from the demographic
characteristics of the United States population. The primary findings of this research are that
mTurk samples tend to be younger, more educated, less religious, and more liberal as well as
less likely to be married, a racial minority, or fully employed (e.g., Berinsky, Huber, & Lenz,
2012; Huff & Tingley, 2015; Paolacci & Chandler, 2014). Other research has demonstrated
that samples may depart from national representative sources with respect to health
behaviors, for example reporting lower rates of influenza vaccination, asthma, and
exercise and higher rates of depression (Walters, Christakis, & Wright, 2018).
The ideal sample for generalizable research are probability samples that are representative
of the United State population. However, this is seldom, if ever, achieved outside of large-scale
(and expensive) national survey data. The appropriate point of comparison for demographic
representativeness is then likely comparisons with other conventionally used and viable
convenience sampling methods (for more discussion of this issue see Landers & Behrend,
2015). Several studies have compared mTurk samples with traditional convenience samples to
show equivalence and, in some cases, superiority. For example, one study found that mTurk
samples were more representative of the United States population than college student samples
or those drawn from college towns, at least for the purposes of political science research
(Berinsky et al., 2012). Another study in political science demonstrated similarities in
occupational and geographic characteristics between an mTurk sample and the
Cooperative Congressional Election Survey (a nationally stratified survey of United
States adults) and found that demographic correspondence improved in younger cohorts
(Huff & Tingley, 2015).

ADDICTION SCIENCE MECHANICAL TURK 10
Other studies have demonstrated similarities in participant responding across sampling
methods. For example, one study found statistical equivalence in stress and sleep measures
collected on mTurk and in a college sample (Briones & Benham, 2017). Another study found
some statistically significant differences on an emotion classification task between samples
drawn from mTurk and those drawn from college campuses or online forums (Bartneck,
Duenser, Moltchanova, & Zawieska, 2015). However, it was argued that similarities in the
distribution of responding and the relatively small magnitude effect of these differences
observed meant that any deviations were unlikely to be practically meaningful. Another study
found that self-admission of previous problematic responding (e.g., responding in socially
acceptable ways, to “help” the researcher, or without paying attention) did not systematically
differ between mTurk, community, and college samples (Necka, Cacioppo, Norman, &
Cacioppo, 2016). In fact, some research suggests that mTurk samples pay more attention to the
tasks at hand perhaps because of the extensive experience with and expectation of attention
checks in the mTurk community (Hauser & Schwarz, 2016; see further discussion of attention
and validity checks below). mTurk has also shown some superiority in direct comparisons with
other online recruitment methods, such as Facebook or email Listservs, with one study in family
science showing improved demographic diversity at lower cost and higher speed of collection
for the mTurk sampling method (Dworkin, Hessel, Gliske, & Rudi, 2016).
mTurk participants have in some studies reported slightly higher rates of substance use than
those recorded in nationally representative studies (e.g., the National Survey on Drug Use and
Health [NSDUH]). For example, data collected in research studies conducted on mTurk in
2016 by our group indicated higher rates of lifetime illicit drug use among mTurk participants (N
= 5269) than those observed for data collected in a nationally representative sample during the
same time period (e.g., 61.5% reporting lifetime cannabis use on mTurk versus 47% nationally;
Figure 2, Center for Behavioral Health Statistics, 2017; see Strickland, Bolin, Lile, Rush, &
Stoops, 2016; Strickland, Lile, & Stoops, 2017; Strickland & Stoops, 2017 for published

ADDICTION SCIENCE MECHANICAL TURK 11
studies from which portions of these data were collected). These data are consistent with
other studies indicating higher rates of recent illicit drug use reported by mTurk participants. For
example, one study conducted in 2015 found rates of past month cocaine use (4.3%) that
exceeded estimates from nationally representative sources collected in the same year (~0.8%)
(Strickland & Stoops, 2015; but see Caulkins, Kilmer, Reuter, & Midgette, 2015; Caulkins,
Sussell, Kilmer, & Kasunic, 2015 for concerns about the conservative nature of national
estimates). Results from another study reported similar high rates of recent cannabis use
among mTurk participants relative to the general population in 2013 (10.6% versus 7.6%;
Shapiro, Chandler, & Mueller, 2013). These estimates should be taken with caution given the
problems associated with generalizing point estimates from non-probability sampling methods
such as mTurk. However, the reported rates of illicit drug use on mTurk do provide evidence
that individuals with varying substance use histories may be sampled through the platform. This
argument is bolstered by research studies (described in greater detail below) evaluating
participants reporting substance use across many drugs and drug classes, including alcohol,
cannabis, cigarettes, e-cigarettes, cocaine, hallucinogens, heroin, methamphetamine, and
prescription opioids (e.g., Dunn et al., 2016a, 2016b; Johnson, Herrmann, & Johnson, 2015;
Koffarnus, Franck, Stein, & Bickel, 2015; Mellis, Woodford, Stein, & Bickel, 2017; Peters,
Rosenberry, Schauer, O’Grady, & Johnson, 2017; Rass, Pacek, Johnson, & Johnson, 2015;
Strickland & Stoops, 2015).
Scale Psychometrics
Other studies have supported the validity of mTurk data collection by demonstrating scale
reliabilities and factor structures of common psychological scales that are consistent with
traditional sampling methods (e.g., Behrend, Sharek, Meade, & Wiebe, 2011; Buhrmester,
Kwang, & Gosling, 2011; Feitosa, Joseph, & Newman, 2015; Kim & Hodgins, 2017; Shapiro et
al., 2013). For example, personality researchers have observed a reliable five-factor solution
and strong internal consistency (e.g., Cronbach’s α > .80) for the Big Five Inventory on mTurk

ADDICTION SCIENCE MECHANICAL TURK 12
as typically recorded in laboratory and clinical samples (Behrend et al., 2011; Feitosa et al.,
2015). Strong test-retest reliabilities (r > .80) have also been described for the Big Five
xx
Inventory and other widely used personality and clinical measures, such as the Beck
Depression Inventory and Brief Experiential Avoidance Questionnaire (e.g., Buhrmester et al.,
2011; Shapiro et al., 2013). Similarly, high rates of consistency (> 95%) have been observed for
demographic measures taken at multiple survey locations or over multiple measurement periods
(e.g., Mason & Suri, 2011; Rand, 2012).
Recent data also indicate the reliability and validity of common substance use scales when
used on mTurk. Some of the most convincing and comprehensive evidence comes from a
recent study in participants with a history of alcohol use, cannabis use, or problematic gambling
(Kim & Hodgins, 2017). Participants in that study completed a battery of standardized measures
(e.g., the Alcohol Use Disorder Identification Test [AUDIT]) at two time periods separated by
one week. High internal consistency, test-retest reliability, and stability of diagnostic categories
were observed for most scales over this one-week period. These rates were also comparable to
those observed in other laboratory-based research with the exception that internal consistency
(Cronbach’s α = .75) and test-retest values (ICC = .72) were slightly lower for the WHO-ASSIST
in cannabis users. Participants also reported that they found it easier to answer honestly about
sensitive questions on mTurk than in an interview setting (mean rating of 6 on a 7-point scale
[Strongly Disagree-Strongly Agree]). Additional research is needed to test and confirm the
reliability and validity of other common measures in these and other substance-using
populations. However, the results of this study provide promising support for the use of common
substance use measures on mTurk.
Demonstration of Common Psychological Phenomena
Other studies have evaluated the validity of mTurk data collection by examining widely
documented psychological phenomena in the online setting. The premise for these studies is
that similar effects and effect size estimates should be observed online as in the laboratory

ADDICTION SCIENCE MECHANICAL TURK 13
setting thereby supporting the fidelity of mTurk for psychological research. For example, Crump
and colleagues (2013) successfully replicated a variety of common experimental psychology
outcomes, such as the Stroop effect (i.e., reaction time interference with incongruent stimuli
pairs) and the Simon effect (i.e. faster reaction times when stimuli are spatially congruent).
Failures to replicate other effects (e.g., masked priming using a short prime durations) were
attributed to concerns related to technology, like lack of control over browser-based display
properties, rather than problems specific to the mTurk participant pool. Similar results were
observed in another study wherein open-source software was used to replicate classic
psycholinguistic effects (e.g., filler-gap dependency processing) that were dependent on small
differences in response time and precise response time estimates (Enochson & Culbertson,
2015). A particularly compelling study conducted by Mullinix, Leeper, Druckman, and Freese
(2015) compared 20 different political science experiments when evaluated on mTurk with effect
sizes observed in a nationally representative population-based sample. Support for mTurk was
found with 80.6% of effect sizes estimates (29 of 36) replicating on mTurk.
Particularly relevant to addiction science is a growing literature replicating and extending
findings related to delay and probability discounting using the mTurk platform. When presented
with choices that differ in delay, probability, and amount, individuals must weigh the relative
benefits of such outcomes. The discounting of delayed monetary gains refers to the acceptance
of a smaller, sooner reward (e.g., $500 now) over a later, larger reward (e.g., $1000 in three
months) (Odum, 2011). Alternatively, in probability discounting, the value of a reward is reduced
as a function of the odds against receiving that reward. Discounting, and delay discounting in
particular, has received extensive attention in theoretical accounts of substance use and it has
been argued that excessive delay discounting represents a trans-disease process contributing
to disease-related vulnerability (e.g., Bickel, Jarmolowicz, Mueller, Koffarnus, & Gatchalian,
2012).

ADDICTION SCIENCE MECHANICAL TURK 14
Several mTurk studies have emphasized discounting processes, in part due to an interest in
episodic future thinking (EFT) for interventions development (see Interventions Development
section below). One large sample study replicated the well-described effect of higher delay
discounting rates in smokers compared to non-smokers (Jarmolowicz, Bickel, Carter, Franck, &
Mueller, 2012). This study also found no differences in probability discounting between these
groups, which was also consistent with prior literature and indicated that the observed effects
were behaviorally specific when tested in the online platform. Another study sought to replicate
six well-described effects in the discounting literature within the context of a novel question
related to opportunity cost (Johnson et al., 2015). This attempt was largely successful and
replicated at least five of six effects, such as the magnitude effect (i.e., steeper discounting for
smaller delayed rewards than larger delayed rewards) and steeper discounting of consumable
goods relative to money. An effect of smoking status was not observed in that study. However,
this apparent discrepancy with prior literature was attributed to procedural differences between
studies and the use of a delay task with a maximal 24-hour delay in which differences between
smokers and non-smokers had not been previously observed.
Taken together, the extant literature provides ample evidence for the reliability and validity of
data obtained on mTurk. Although demographic characteristics may differ in some ways from
nationally representative samples, these discrepancies are well documented and show some
improvements over alternative convenience sampling methods. Other research has described
similar psychometric properties and observed “known” psychological effects on mTurk as in
laboratory literature. Fewer validation studies have systematically and specifically evaluated
measures and behaviors related to substance use and misuse. However, those studies that do
exist provide initial support for the validity of mTurk for questions relevant to addiction science.
Applications of mTurk in Addiction Science
The application of mTurk in addiction science is a relatively recent development when
compared to its uses in other areas of psychological science (Figure 1). The following section

ADDICTION SCIENCE MECHANICAL TURK 15
reviews this emerging literature that utilizes mTurk to answer questions relevant to substance
use and misuse. This discussion is a narrative review of existing research rather than a
systematic review using PRISMA guidelines and, therefore, is not an exhaustive review
of all extant literature. The section is organized so as to focus on four of the broad approaches
utilized, to date: 1) cross-sectional research and replication studies, 2) measure development,
3) longitudinal designs, and 4) interventions development.
Cross-Sectional Research and Replication Studies
One of the most popular uses of mTurk for psychological and addiction science is cross-
sectional survey and basic cognitive-behavioral research. These studies may be conducted as
independent experiments or combined with ongoing laboratory projects as a replication sample.
This latter use is a particularly notable strength of online sampling as it allows for the relatively
rapid testing of effects observed in laboratory studies in a new and independent sample. Such
replication attempts have become increasingly relevant in psychological sciences given recent
challenges regarding reproducibility and related failures to replicate published findings (Nosek et
al., 2015; Open Source Data Collection, 2015).
The ability of online crowdsourcing to accompany typical studies conducted with human
participants can help enhance the overall rigor and generalizability of observed results. For
example, Athamneh, Stein, & Bickel (2017) found that individuals with higher intentions to quit
smoking showed lower delay discounting rates in two cross-sectional cohorts from the human
laboratory and mTurk. The application of mTurk in this study was particularly noteworthy as two
related, but independent and distinct tasks were used to evaluate discounting in each setting,
demonstrating that the observed effects were not methodologically bound. A similar multi-
sample approach was used in another study evaluating the relationship between drinking to
cope and hazardous drinking in a college psychology pool and a non-college mTurk sample
(Veilleux, Skinner, Reese, & Shaver, 2014). That study observed a significant relationship
between these variables in both samples thereby replicating previous research in college

ADDICTION SCIENCE MECHANICAL TURK 16
students and extending those findings to adults. Additionally, a novel mediation model
explaining the relationship between negative affect intensity and drinking to cope through
emotional clarity/strategies was supported in both samples (Veilleux et al., 2014).
mTurk has been widely utilized for the study of behavioral economic demand in substance
using populations. A review of this emerging literature serves several case examples of the
ways in which mTurk may be used to complement in-person studies. Behavioral economic
demand represents the orderly relationship between consumption of a good and its price (see
reviews by Bickel, Marsch, & Carroll, 2000; Hursh & Roma, 2013; MacKillop, 2016; Reed,
Niileksela, & Kaplan, 2013). Recent years have witnessed a growth in the human laboratory and
clinical study of demand due in part to an increasing utilization of the hypothetical commodity
purchase task. This procedure asks participants to report hypothetical consumption of a good
(e.g., alcohol) across a range of prices (e.g., $0.01, $1.00/drink) and is particularly appealing
because of its cost and time efficiency as well as adaptability for populations with whom drug
self-administration is not ethically or practically feasible (e.g., patients in residential treatment,
those with medical contraindications to drug administration) (Jacobs & Bickel, 1999; Reed et al.,
2013). These attributes have also made the purchase task portable to mTurk thereby affording
researchers the opportunity to index drug valuation in a remote, online setting.
One large sample study evaluated the validity of administering behavioral economic
measures, including the commodity purchase task, on mTurk (Morris et al., 2017). A large
sample of alcohol-using participants (N = 865) was recruited on mTurk and completed an
alcohol purchase task to measure alcohol demand and a reinforcement survey schedule to
measure proportionate alcohol reinforcement. Purchase task data were systematic and provided
unique prediction of alcohol use severity supporting the convergent validity of this measure on
mTurk, a finding that was consistent with extant laboratory and clinic research (see review of
alcohol purchase task studies in MacKillop, 2016). Another study evaluated the unique
prediction of cannabis use by behavioral economic demand and delay discounting (Strickland et

ADDICTION SCIENCE MECHANICAL TURK 17
al., 2017). Purchase task data were systematic for cannabis and alcohol commodities with
cannabis demand uniquely predicting cannabis quantity-frequency and cannabis delay
discounting uniquely predicting cannabis use severity (i.e., cannabis use disorder symptom
counts). These findings replicated those observed in a prior laboratory study (Aston, Metrik,
Amlung, Kahler, & MacKillop, 2016) supporting the validity of online data collection and
highlighting its utility as a source for replication studies. Other studies have successfully used
the purchase task to evaluate demand for alcohol (e.g., Kaplan et al., 2017: Noyes & Schlauch,
2018) and cannabis (e.g., Peters et al., 2017) as well cocaine (Strickland, Reynolds, & Stoops,
2016), cigarettes (Koffarnus et al., 2015; Snider, Cummings, & Bickel, 2017; Stein, Tegge,
Turner, & Bickel, 2018; Strickland & Stoops, 2017), and e-cigarettes (Johnson, Johnson, Rass,
& Pacek, 2017; Snider et al., 2017) emphasizing the versatility of the online platform for
studying varying drug classes. This ease and speed of data collection also allows for the study
of parametric manipulations or other aspects of task design that may be overlooked when
conducting in-person research in the interest of focusing on clinical applications. For example,
research on mTurk has assessed a novel demand equation for purchase task data (Koffarnus et
al., 2015), the stimulus-selectivity of the purchase task procedure (Strickland & Stoops, 2017),
and the influence of variations in task instructions on demand outcomes (Kaplan et al., 2017).
Other studies have leveraged mTurk for cross-commodity purchase task research. Cross-
commodity tasks present participants with a situation in which the price of one commodity (e.g.,
cigarettes) is manipulated while the price of a concurrently available commodity (e.g., e-
cigarettes, nicotine gum) is held constant. These procedures provide a measure of the extent to
which commodities function as complements (i.e., as the price of one increases, consumption
for the other decreases; hot dogs and hot dog buns, for example) or substitutes (i.e., as the
price of one increases, consumption for the other increases; Coca Cola® and Pepsi®, for
example). Two studies have evaluated the cross-commodity relationship between e-cigarettes
and other nicotine-containing products on mTurk (Johnson et al., 2017; Snider et al., 2017). The

ADDICTION SCIENCE MECHANICAL TURK 18
first study showed that e-cigarettes might serve as a superior substitute for tobacco cigarettes
when compared to nicotine gum (Johnson et al., 2017). The other study observed similar results
wherein e-cigarettes functioned as substitutes for tobacco cigarettes and that the magnitude of
this substitution was related to a participant’s e-cigarette use history (i.e., individuals with a
more extensive history of e-cigarette use reported greater substitution) (Snider et al., 2017).
Another study evaluated cannabis and tobacco cigarettes and found no evidence that these
commodities substituted for or complemented one another (Peters et al., 2017). Taken together,
this behavioral economic demand literature demonstrates the varied basic science and applied
applications of mTurk for cross-section research as well as the ability to study drug valuation
within an online context.
Corresponding laboratory and online studies may also be used to test the specificity of
laboratory effects by recruiting relevant control groups. This approach was used in a series of
experiments evaluating the relative rate of learning from positive and negative outcomes in
cocaine users and controls (Strickland et al., 2016). Cocaine users were first recruited for a
laboratory study in which a reduced sensitivity to learning from positive relative to negative
outcomes was observed on a probabilistic learning task. These effects were then replicated in
an independent mTurk sample and specificity to a cocaine-use history demonstrated by also
recruiting an online control sample.
Another apparent benefit of research conducted on mTurk is the ability to screen for and
select samples with specific behavioral or health histories. This advantage can be especially
useful for emerging trends in substance use whose profile has not yet been established, thereby
making targeted community recruitment difficult. In line with this idea, a number of researchers
have leveraged mTurk to sample electronic cigarette (e-cigarette) users. Large sample
characterization studies have been conducted, such as one evaluating use patterns and
perceptions of relative harm in dual e-cigarette and tobacco cigarettes (Rass et al., 2015). Other
studies have evaluated more specific aspects pertaining to this emerging and growing

ADDICTION SCIENCE MECHANICAL TURK 19
substance use trend, including relationships between tobacco cigarette smoking history and e-
cigarette perceptions (Bauhoff, Montero, & Scharf, 2017), factors related to the use of e-
cigarettes in women of reproductive age (Chivers, Hand, Priest, & Higgins, 2016), the
effectiveness of advertisements for e-cigarettes as smoking cessation aids (Jo, Golden, Noar,
Rini, & Ribisl, 2018), the development and validation of a vaping craving questionnaire (Dowd,
Motschman, & Tiffany, 2018), and predictors of using “vape” pens for cannabis administration
(Morean, Lipshie, Josephson, & Foster, 2017).
Similar targeted recruitment strategies have been used to identify individuals with specific
behavioral or health histories for which community sampling may yield low participant accrual
and difficulties in generating adequately powered samples. For example, HIV+ smokers have
been recruited to identify nicotine-related knowledge, perceived health risks of cigarette
smoking, and predictors of cessation interests within this particularly vulnerable health group
(Pacek, McClernon, Rass, Sweizter, & Johnson, 2018; Pacek, Rass, & Johnson, 2017a,
2017b). Crowdsourcing has also been used to rapidly recruit larger-than-typical samples of
special populations, such as individuals with lifetime psychedelic use (Forstmann & Sagioglou,
2017), chronic pain (Tompkins et al., 2016, 2017), men who have sex with men (Herrmann,
Johnson, & Johnson, 2015), and individuals with illicit drug use histories (e.g., Dunn et al.,
2016a, 2016b; Strickland & Stoops, 2015). These latter two examples are particularly
noteworthy given that snowball sampling is traditionally employed in human laboratory and
clinical studies with these populations, which can potentially result in biased observations and
greater homogeneity within the resulting samples (Biernacki & Waldorf, 1981; Faugier &
Sargeant, 1997). Although attention to the limitations presented by self-report data is necessary
(see further discussion of this issue below), mTurk may be used to access specialized
populations for recruitment of larger size and more diverse samples than are typically afforded
in community-based research.

ADDICTION SCIENCE MECHANICAL TURK 20
Measure Development
Measure development efforts have benefited from using mTurk to rapidly generate large
samples with relatively diverse substance use histories. Large samples may be utilized to
develop a measure and its initial factor structure (exploratory factor analysis; EFA) or serve as a
replication sample to determine the generalizability and factor invariance of a novel measure
(confirmatory factor analysis; CFA). This approach has been used to develop measures for a
variety of topics relevant to addiction science, such as therapeutic alliance during cigarette-
cessation counseling (Warlick et al., 2018), attitudes toward contraband cigarettes (Adkison,
O’Connor, Chaiton, & Schwartz, 2015), alcohol-related myopia (Lac & Berger, 2013), and
diagnostic testing for DSM-V caffeine use disorder (McGregor & Batis, 2016).
A blend of in-person and online samples may also be used for measures development. An
elegant pair of studies by Dunn and colleagues (2016a, 2016b) exemplifies this approach. Each
study first recruited smaller clinical samples for EFA and then recruited larger and more
geographically and clinically diverse samples from mTurk to determine factor invariance and
conduct CFA. For example, one study developed a 3-factor Brief Opioid Overdose Knowledge
(BOOK) questionnaire by first recruiting illicit opioid users (n = 147) from a single clinic in
Baltimore for initial scale development (Dunn et al., 2016b). The internal validity of the scale
was then confirmed by recruiting individuals from two independent clinic sites (n = 199) as well
as a larger mTurk cohort of chronic pain patients receiving an opioid analgesic (n = 502). The
ability to rapidly confirm factor structures and generalizability for novel measures is a clear
strength of crowdsourcing for measures development research.
mTurk-assisted measure development has also received considerable attention in the study
of behavioral addictions (i.e., non-drug related addictions). Novel food addiction measures,
including the Yale Food Addiction Scale Version 2.0 (Gearhardt, Corbin, & Brownwell, 2016)
and a brief version of this scale (Schulte & Gearhardt, 2017), were recently developed on mTurk
and subsequently used in cross-sectional research on the platform (e.g., Rainey, Furman, &

ADDICTION SCIENCE MECHANICAL TURK 21
Gearhardt, 2018). Similar methodological studies have used mTurk for studying technology-
related behavioral addictions, such as mobile phone addiction (Bock et al., 2016; Contractor,
Frankfort, Weiss, & Elhai, 2017), social media addiction (Muench, Hayes, Kuerbis, & Shao,
2015), and Internet gaming addiction (Beard & Wickham, 2016; Beard, Haas, Wickham, &
Stavropoulous, 2017). Although the debate over “behavioral addiction” versus the pathologizing
of common behavior is beyond the scope of this review (see Kardefelt-Winther et al., 2017 for a
relevant discussion of this issue), mTurk is becoming an increasingly utilized resource for those
interested in characterizing non-substance-related addictive disorders.
Longitudinal Research
The unique identifiers assigned to mTurk participants and easy-to-use interface also allow
for the conduct of follow up assessments. Such a test-retest design has been used by
researchers with diverse interests in psychological science. For example, Daly and Nataraajan
(2015) observed response rates of 75% at two months, 56% at four months, and 47% at three
months following a baseline assessment across three independent personality psychology
experiments. Others have observed similar rates when recording at weeks or months times
(Chandler, Muller, & Paolacci, 2014; Reese & Veilleux, 2015; Shapiro et al., 2013). One
particularly noteworthy study used an intensive daily diary approach to evaluate the relationship
between electronics use and sleep quality (Lanaj, Johnson, & Barnes, 2014). Some evidence
for the feasibility of this approach was observed with response rates of 61% for surveys
completed over 10 consecutive workdays in that study (Lanaj et al., 2014).
As noted above (see Scale Psychometrics section), Kim and Hodgins (2017) observed test-
retest reliabilities for substance use measures similar to those observed for in-person research.
One week follow up rates of 87% or greater were observed in that study for alcohol-using,
cannabis-using, or problematic gambling participants. Two other studies have used intensive
longitudinal methods (i.e., frequent or dense measurement such as daily diary or ecological
momentary assessment) to evaluate alcohol consumption (Boynton & Richman, 2014;

ADDICTION SCIENCE MECHANICAL TURK 22
Strickland & Stoops, 2018). In the first study, alcohol consumption was measured over a 14-day
period using a daily diary design and findings commonly reported in the literature were observed
supporting the validity of the approach (e.g., heavier drinking on the weekend) (Boynton &
Richman, 2014). Participants (N = 369) also completed 8.5 of the daily measurements (60.7%)
on average providing some support for study feasibility. The second study extended these
preliminary findings by collecting weekly recordings of alcohol and soda use over an 18-week
period (Strickland & Stoops, 2018). Participants (N = 278) reported that this design was
acceptable (i.e., 94% indicated they would participate again). Feasibility was also indicated by
an average completion rate of 73% of participants per week over the 18-week period (range:
64.1%-86.8%/week). Construct and external validity was further demonstrated through the
replication of expected relationships that were specific to alcohol use and not observed for soda,
such as heavier alcohol consumption by individuals with higher AUDIT scores and on
weekends. These studies collectively provide preliminary support for the feasibility,
acceptability, and validity of conducting longitudinal work with substance-using populations on
mTurk.
Interventions Development
mTurk has also been utilized for recent interventions development work. The application of
mTurk for interventions purposes may prove particularly useful because of the inherent
similarities to Internet-based interventions (see reviews by Andersson & Titov, 2014; Carroll &
Rounsaville, 2010; Dallery, Kurti, & Erb, 2015; Kurti et al., 2016 on the use of Internet-based
interventions in psychology and addiction science). Internet-based interventions provide many
benefits for substance use prevention and treatment, such as access to otherwise remote or
hard-to-reach populations (e.g., rural and/or adolescent populations; Harris & Reynolds, 2015;
Reynolds et al., 2015; Stoops et al., 2009). Pilot testing and refinement of such interventions on
mTurk is a particularly appealing application of crowdsourcing given the inherent portability for
future large scale, Internet-based trials and delivery.

ADDICTION SCIENCE MECHANICAL TURK 23
An emerging body of literature has used mTurk to evaluate the effects and mechanisms
underlying anti-smoking health warnings and other mass media messages targeting cigarette
use. Some studies, for example, have evaluated the effects of manipulating cigarette packaging
(e.g., packet label or the use of iconic images) on perceptions of harm (e.g., Lazard et al., 2017;
Leas, Pierce, Dimofte, Villasenor, & Strong, 2017; Pearson et al., 2016). One particularly
innovative design exposed participants to FDA-proposed textual and pictorial warnings about
smoking-related hazards, textual warnings with irrelevant images, or text-only warnings and
evaluated cigarette use and feelings about smoking at baseline and a seven-day follow up (Shi,
Wang, Emery, Sheerin, & Romer, 2017). Participants exposed to the FDA-proposed warnings
showed greater motivation to quit, fewer reported cigarettes smoked per day at seven-day
follow up, and better memory for the warnings than those in the other two conditions. These
findings suggested that images communicating smoking-related risk enhanced the
persuasiveness of the proposed warnings. Another study employed a pre-post design to
evaluate the impact of exposure to smokeless tobacco constituent information on risk and
knowledge measures (Borgida et al., 2015). Exposure to information about the carcinogenic
constituents of smokeless tobacco improved knowledge about the contribution of these
components of tobacco to disease risk and acknowledgement that products may present varied
levels of risk (e.g., medicinal nicotine replacement therapies present less risk than smokeless
tobacco and cigarette products).
mTurk is also well suited for exploring the efficacy and mechanisms underlying brief
interventions. Screening and brief interventions are commonly used in the clinical setting as a
“first-line of defense” for prevention and treatment (Pilowsky & Wu, 2013). This strategy is
consistent with the broader idea of “Screening, Brief Interventions, and Referral to Treatment” or
SBIRT (Madras et al., 2009). SBIRT proposes a comprehensive and integrated identification
and treatment linkage for individuals at risk for or suffering from a substance use disorder.

ADDICTION SCIENCE MECHANICAL TURK 24
Three studies have evaluated brief interventions for alcohol use on mTurk. One study
evaluated the feasibility and acceptability of providing online feedback of alcohol use in older
adults (50+) via personalized or normative feedback approaches, two brief interventions with
moderate effects on alcohol consumption (Kuerbis, Hail, Moore, & Muench, 2017). Online
feedback was deemed feasible and normative feedback outperformed personalized feedback
for motivating changes in drinking patterns. Another study evaluated normative feedback to
evaluate underlying mechanisms of change and found tentative support for changes in drinking
behavior through a belief in the accuracy of feedback mechanism (Kuerbis, Muench, Lee, Pena,
& Hail, 2016). A third study evaluated the effects of personalized feedback intervention (“Check
Your Drinking”; Cunningham, Wild, Cordingley, van Mierlo, & Humphreys, 2009) on alcohol use
at a 3-month follow up (Cunningham, Godinho, & Kushnir, 2017). High follow-up rates were
observed at 3-months (85%). However, reductions in alcohol use with the personalized
feedback intervention were only observed for one of four outcome variables (i.e., AUDIT
consumption subscale).
An additional study evaluated delivery of a brief opioid overdose education intervention on
mTurk (Huhn, Garcia-Romeu, & Dunn, 2018). Participants reporting prescription opioid use for
pain completed two variants of opioid overdose education related to opioid effects, opioid
overdose symptoms, and opioid overdose response. Overdose education increased scores on a
Brief Opioid Overdose Knowledge meas

## 引用

```

```
