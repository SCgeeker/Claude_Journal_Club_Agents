---
title: Comprehension effort as the cost of inference
authors: 
year: N/A
keywords: 
created: 2025-11-23 18:19:38
---

# Comprehension effort as the cost of inference

## 基本信息

- **作者**: 
- **年份**: N/A
- **關鍵詞**: 

## 摘要

As you read this text, word by word, you build an understanding of its meaning. What cognitive
mechanismsunderliethisability? Aninfluentialapproachtoansweringthisquestioncomesfromviewing
comprehensionasprobabilisticinferenceoverpotentialinterpretationsgivenlinguisticinput. Motivated
withinthisperspective,awealthofpreviousliteratureinpsycholinguisticshasfocusedonanimportant
empiricalrelationshipmadeprecisebysurprisaltheory(Hale,2001;Levy,2008a),thehypothesisthat
the effort required to process a word scales in its negative log probability, in context. However, the
standardderivationofsurprisalwithintheinferenceframeworkreliesonacrucialassumption: thatthere
isadeterministicrelationshipbetweenthelatentinterpretationstargetedbyinferenceandtheobservable
input. In this work we propose relaxing this assumption and formalize inference cost directly as the
amountofchangeinprobabilisticbeliefs. Thisproposalformsanontrivialgeneralizationofstandard
surprisaltheory,whichprovidesamoredirectconnectiontoalgorithmictheories,andnaturallyexplains
phenomenawhereunpredictableinputrequireslittleprocessingeffort. Totestthisframeworkagainst
surprisaltheory,weconductaself-pacedreadingtimestudytargetingwordswithorthographicerrors,a
specificsettingwhereourapproachpredictssubstantiallydifferentpatterns. Wefindthatprocessingeffort
followsthepredictionsofbelief-updateratherthansurprisal,inanoisy-channelmodelofcomprehension
asinferenceaboutintendedwords. Theseresultsdemonstrateaclearcasewheresurfacesurprisalcannot
explainhumanprocessingcost,andprovidefurthersupportformodelsoflanguagecomprehensionas
rationalinference.
1 Introduction
Howdowedecodemeaningfromasequenceofwords,asitunfoldsintime? Whiletheprecisecognitive
mechanismsunderlyinglanguagecomprehensionremainunknown,thepatternsofhumanbehaviorduring
processingofferanimportantsourceofclues. Considerthefollowingsentence: Aftertrippingontherug
and falling in front of everyone, my face flushed, and I felt deeply innovative as I returned to my seat.


## 研究背景

## 研究方法

## 主要結果

## 討論與結論

## 個人評論

## 相關文獻

## 完整內容

Comprehension effort as the cost of inference
JacobHooverVigly*,1,PengQian1,2,MorganSonderegger3,4,andTimothyJ.O’Donnell3,5,6
1MIT 2Harvard 3McGill 4CRBLM 5Mila 6CanadaCIFARAIChair
*jahoo@mit.edu
v1,18Jun2025
Abstract
As you read this text, word by word, you build an understanding of its meaning. What cognitive
mechanismsunderliethisability? Aninfluentialapproachtoansweringthisquestioncomesfromviewing
comprehensionasprobabilisticinferenceoverpotentialinterpretationsgivenlinguisticinput. Motivated
withinthisperspective,awealthofpreviousliteratureinpsycholinguisticshasfocusedonanimportant
empiricalrelationshipmadeprecisebysurprisaltheory(Hale,2001;Levy,2008a),thehypothesisthat
the effort required to process a word scales in its negative log probability, in context. However, the
standardderivationofsurprisalwithintheinferenceframeworkreliesonacrucialassumption: thatthere
isadeterministicrelationshipbetweenthelatentinterpretationstargetedbyinferenceandtheobservable
input. In this work we propose relaxing this assumption and formalize inference cost directly as the
amountofchangeinprobabilisticbeliefs. Thisproposalformsanontrivialgeneralizationofstandard
surprisaltheory,whichprovidesamoredirectconnectiontoalgorithmictheories,andnaturallyexplains
phenomenawhereunpredictableinputrequireslittleprocessingeffort. Totestthisframeworkagainst
surprisaltheory,weconductaself-pacedreadingtimestudytargetingwordswithorthographicerrors,a
specificsettingwhereourapproachpredictssubstantiallydifferentpatterns. Wefindthatprocessingeffort
followsthepredictionsofbelief-updateratherthansurprisal,inanoisy-channelmodelofcomprehension
asinferenceaboutintendedwords. Theseresultsdemonstrateaclearcasewheresurfacesurprisalcannot
explainhumanprocessingcost,andprovidefurthersupportformodelsoflanguagecomprehensionas
rationalinference.
1 Introduction
Howdowedecodemeaningfromasequenceofwords,asitunfoldsintime? Whiletheprecisecognitive
mechanismsunderlyinglanguagecomprehensionremainunknown,thepatternsofhumanbehaviorduring
processingofferanimportantsourceofclues. Considerthefollowingsentence: Aftertrippingontherug
and falling in front of everyone, my face flushed, and I felt deeply innovative as I returned to my seat.
While reading this sentence, you likely formed some expectations about what meaning of the sentence
wasgoingtobeevenbeforecompletingit,and,uponencounteringthewordinnovativeyoumayhavehad
to revise that idea somewhat. If so, you likely read that word a bit slower than you would have read a
more expected word (potentially something like embarrassed or humiliated) in its place. This example
illustrates a general phenomenon that has been of considerable interest in the study of human language
comprehensionandcognitionmorebroadly: Perceptualinputtendstobemorecostlytoprocesswhenitis
lessexpected. Expectation-basedmodelsoflanguagecomprehension(Kutas&Hillyard,1984;Narayanan&
Jurafsky,1998;Hale,2001,2003b;Levy,2008a,2013)provideawaytoexplainthisphenomenon,framing
thecomprehenderasperformingprobabilisticinferenceoversomelatentspaceofpossibleanalyseswhen
perceivinganutterance,representinguncertaintywithdistributionoversomespaceofpotentialinterpretations,
each held to different degrees of belief. When a word is observed, it causes a change in this distribution,
favoring interpretations according to how well they explain the observation. This perspective suggests
1

anexplanationforthephenomenonofinterest,ifalargerchangeindistributionsrequiresmorecognitive
resources.
Indeed,thisargumentintermsofbelief-updatesizeformsacoremotivationfortheinfluentialsurprisal
theory (Hale, 2001; Levy, 2008a)—the hypothesis that processing cost be measured with surprisal, an
information-theoreticquantitydefinedasthenegativelogprobabilityofaword,incontext. Empirically,this
relationshiphasbeenbroadlysupportedbyawealthofstudiesusingestimatesofsurprisaltopredicthuman
readingbehavior(Staub,2011;Frank&Bod,2011;Smith&Levy,2013;Shain,2019;Shainetal.,2024)or
brainactivity(Kutas&Hillyard,1984;Bachrachetal.,2009;Franketal.,2015;Shainetal.,2020;Heilbron
etal.,2022;Michaelov&Bergen,2024)duringlanguagecomprehension. Theargumentthatprocessingeffort
reflectsthecostofdoinginferencesituatessurprisaltheorywithinthelargerframeworkofrationalmodelsof
cognition(Shepard,1987;Anderson,1990;Griffithsetal.,2024),andadditionallyinspirestheexploration
ofprobabilisticinferencealgorithmsaspotentialmechanismstoexplainhumanlanguageprocessing(see
Hooveretal.,2023).
However,thecoreinference-costideadoesnotleadtostandardsurprisaltheorywithoutsomeadditional
assumptions. Namely,toderivesurprisalasmathematicallyequivalenttobelief-updatesize(quantifiedasa
divergencebetweenprobabilitydistributions),Levy(2008a)explicitlyreliedonanassumptionthatlatent
meaningrepresentationshaveadeterministicrelationshiptoobservablestringsofwords. Essentiallythis
istheassumptionthatanobservedformalwaysprovidesdirectevidenceaboutwhichintendedmeaningsit
iscompatiblewith,butreallanguagecomprehensioninvolvestypos,speechorperceptionerrorsandother
ambiguitieswheretherelationshipbetweensurfaceformsandintendedmeaningsislessstrict. Relaxingthis
assumption,theequivalencebetweensurprisalandbeliefupdatedoesnothold. Thisraisesasimplequestion
thathasbeenlargelyoverlookedinthebodyofpriorliteraturebuildingonsurprisaltheory: Issurprisala
goodmeasureofprocessingcost,evenwhenthisassumptiondoesnothold? Wecontendthattheanswerto
thisquestionisno. Anyobservationthatleadsarationalcomprehendertomakedrasticchangestobeliefs
isnecessarilyunpredictable,thereverseneednothold: Insomesituationsinputmaybedifficulttopredict
(highsurprisal)withoutcausingalargechangeinbeliefsaboutwhatisintended. Forinstance,thismaybe
thecasewhenapredictablelatentmeaninghasmanyalternativerealizations,duetonondeterminisminthe
systemofproductionorperception.
In this paper we propose that processing cost be quantified with the magnitude of belief-update in a
probabilistic inference setting. This hypothesis forms a generalization of surprisal theory, applicable in
anygenerativemodelwherelatentmeaningsgiverisetoobservableinput. Toexplorehowthepredictions
ofthis more general theory compare to surprisal, weexplore the application to a noisy-channel model of
comprehensionasinferenceaboutintendedwords,anintuitivelysimplesettingwherelatentstates(intended
words)donotdeterministicallyrelatetotheobservedinput. Wefocusontypographicalerrorsbecausethey
createaclearcasewheresurfaceformsdivergefromintendedmeanings. Inthissetting,thebelief-update
hypothesispredictsprocessingeffortcanbesmalleveniftheobservationishighlyunpredictable,contrary
tosurprisaltheory. Totestthisprediction,wedesignasetofstimuliwithcontextsdesignedtomaketarget
words’meaninghighlyexpected(orhighlyunexpected),andcarryoutaself-pacedreadingtimestudyon
these words, both with and without typographical errors. We compare human reading times to surprisal
estimatesfrompre-trainedlanguagemodels,aswellasestimatesofbelief-updateandsurprisalwithinan
explicit generative model of noisy strings, finding that reading times on these items pattern as predicted
bybelief-update,ratherthansurprisal. Ourresultsdemonstrateasettingwheresurprisalisinsufficientto
predictthepatternsofhumanprocessingeffort,andmotivateourgeneralizationofsurprisaltheory,viewing
comprehensionasinferencewitheffortdrivenbythecostofchangingexpectations.
2

belief update
prior posterior
p(Z) p(Z|u)
u
observation
Figure1: Comprehensionasinferenceaboutintendedmeaning. Inlightofsomeobservedinputu,arationalcompre-
henderrevisespriorbeliefsp(Z)toarriveatposteriorbeliefsp(Z |u),shiftingprobabilitymassonlatentinterpretations
accordingtothelikelihoodtheyassignedtotheobservation,asdescribedbyBayes’rule. Thecostofinferencecanbe
quantifiedbythemagnitudeofthisbeliefupdate.
2 Background
2.1 Languageprocessingasprobabilisticinference
Languagecomprehensioncanbeviewedasanincrementalinferenceproblem. Whenalinguisticutteranceis
observed,asasequenceofsoundsorsigns,themeaningitisintendedtoconveyisnotdirectlyobservable,
and must be inferred. Moreover, the setting is naturally incremental: One need not wait until the entire
utteranceiscompletetobeginforminganideaoftheintendedmessage. Framedthisway,wecanspeakof
a comprehender ‘processing’ language, defining that term to refer to whatever mechanism is involved in
carryingoutthisinferenceoverlatentrepresentations,givenobservedinput. Formalizedfromaprobabilistic
(Bayesian)perspective,atanyparticularpointduringcomprehension,uncertaintyabouttheintendedmeaning
is represented as a probability distribution over a space of possible candidates, which is updated when
observingsubsequentperceptualinput. Suchanapproachhasbeenwidelyandproductivelyemployedto
characterizemanyaspectsoflinguisticcomprehension,withinthelargerframeworkofrationalanalysisin
cognitivescience(seee.g.,Jurafsky,1996,2003;Chateretal.,1998;Chater&Manning,2006;Levy,2008a;
Kuperberg&Jaeger,2016;Degen,2023). Recentliteraturehasprovidedincreasinglyrobustevidencethat
comprehendersmakerationalinferencesabouttheintendedmessagewithinanoisyenvironment,assessing
relativelikelihoodsatthelevelofphonemes,morphemes,words,andlargerstructuralunits(Clayardsetal.,
2008;Levy,2008a,2008b,2011;Feldmanetal.,2009;Piantadosietal.,2011;Gibsonetal.,2013;Ryskin
etal.,2018,2025;Pickering&Gambi,2018;Keshev&Meltzer-Asscher,2021;Hahnetal.,2022;S.Chen
etal.,2023;Qian&Levy,2023;Y.Zhangetal.,2023).
Inferencesetting Treatingalinguisticutteranceasadiscretesequence,letusconsiderasingleinference
step,observingoneinput(aword,incontext). LetZ bearandomvariablerangingoverasetZ ofcandidate
interpretations/meaningsoftheutterance. Toremaingeneral,forthecurrentdiscussionwewillleavethe
natureoftheseinterpretationsintentionallyunspecified,1 andrequireonlythattheygiverisetotheinputU (a
discreterandomvariablerangingoverpossibleobservablestrings)viasomegenerativeprocess.
1Forexample,thesetZ mightbemodeledasconsistingofdiscretelinguisticstructuressuchasparsetrees(asinthework
introducingsurprisaltheory;Hale,2001;Levy,2008a). Buttheyalsocouldbesimplyintendedwords(inanoisy-channelmodelof
comprehensionasinferenceaboutintendedstrings,anapplicationwhichwewillexplorelaterinthispaper),orpoints,regions,or
trajectoriesinahigh-dimensional,continuousspace,discourserepresentationstructuresorotherlogicalrepresentations,oranyother
setrepresentingpossiblemeaningsorlinguisticstructures.
3

Beforetheinputisobserved,beliefsabouttheintendedmeaningcanberepresentedwithapriorprobability
distributionp(Z). UponobservinganoutcomeU = u,thesebeliefsmaychange,favoringtheinterpretations
that more plausibly explain the observation, and disfavoring those that do not, according to a likelihood
modelp(u | Z),whichdescribestheprobabilityofobservingugivenanyparticularoutcomeofZ.2 This
inferencesettingisillustratedschematicallyinfig.1,withpriorandposteriordistributionsoverZ represented
ashorizontalbargraphs. Bayes’ruleappliesthedefinitionofconditionalprobabilitytodescribetherational
waytoformposteriorbeliefsaboutlatentZ uponobservingu:
p(Z)p(u | Z)
p(Z | u) = (cid:82) (1)
p(z)p(u | z)dz
ThetwofactorsinnumeratoraretheingredientstotheBayesianupdate: Posteriorbeliefsareconcentrated
onhypothesesthatareplausibleincontext(describedbytheprior),andalsowell-matchedtotheobserved
data(describedbythelikelihood). Thedenominatorformsthenormalizingconstantrequiredtomakethe
posterioraproperprobabilitydistribution. Informulatingtheoriesofprocessingasinference,andlinking
toprocessingeffort,weareinterestedinhowthisupdatep(Z) (cid:59) p(Z | u)mightbeachievedandatwhat
computationalcost.
2.2 Surprisaltheory
Viewing comprehension as probabilistic inference, a substantial body of work has grown pursuing such
expectation-basedtheoriesoflanguageprocessing(Jurafsky,1996;Narayanan&Jurafsky,1998,2001,2004;
Hale,2001,2003b,2016;McDonald&Shillcock,2003a,2003b;Norris&McQueen,2008;Levy,2008a,
2013;Smith&Levy,2013; Gibsonetal.,2013; Rasmussen&Schuler,2018; Futrelletal.,2020; Futrell
&Levy,2017;Brothers&Kuperberg,2021;Hahnetal.,2022;Hooveretal.,2023;Shainetal.,2024). A
particularlyinfluentialproposalhasbeenthehypothesisthatprocessingcostofanobservation,u,scalesinits
negativelogprobability,orsurprisal(Hale,2001;Levy,2008a).
AlsoknownasShannoninformationcontent,surprisalassignstheoutcomeofadiscreterandomvariable
anonnegativerealnumberwhoseexpectedvalueformsShannon’s(1948)definitionofuncertaintyorentropy.
Surprisalissonamedforitsintendedcorrespondencetothe‘naturalmentalconcept’ofsurprise(Samson,
1953): itiszeroiftheoutcomeiscertain,andincreasesunboundedlyasprobabilityoftheoutcomeapproaches
zero. Inthecontextoflanguageprocessing,wecandefinesurprisaltheoryasconsistingofthefollowing
hypothesis.
Hypothesis1(surprisaltheory). Theprocessingcostofobservedinputuincontextincreasesmonotonically
withsurprisal. Thatis,
cost(u) = f(i(u)) (2)
wherei(u) =def −logPr(u | context)denotesthesurprisalofuand,f isamonotonicallyincreasingfunction
(oftenassumedtobelinear,oraffine).
Thespiritofthisinformation-theoretichypothesisistheideathattheeffortrequiredtoprocessanobservation
scalesintheamountofinformationitcontributes,encapsulatingthestrongclaimthatallfactorswhichmight
affectdifficultyduringincrementalprocessingdosothrough surprisal. Undersurprisaltheory, whatever
latentstructuresmayexistupstreamoftheobservablewords,theyonlycaneffectprocessingcostthrough
surprisal(the‘causalbottleneck’property;asframedbyLevy,2008a,§2.3).
2Weusenotationlikep(Z),p(u|Z)toindicatethatZistheargument,withufixed.Foraspecificvaluez∈Zwewillwrite
p(z),orp(u|z),respectively.Alsonote,ingeneralallthesequantitiesmaydependonadditionalvariable(s)representingprevious
context(formallyconstruedtoincludeatleastthesequenceofpreviouslyencounteredlinguisticinput,butpossiblyalsonon-linguistic
information).Sinceitisacommonconditionerinallterms,wesuppressmentionofcontextinourpresentationforbrevity.
4

Intheinferencesetting,surprisalisthenegativelogofthemarginallikelihood,p(u)—thequantitythat
formsthenormalizingconstantinBayes’rule,derivedfromthejointmodelbymarginalizing.
i(u) = −logp(u)
(cid:90)
= −log p(z)p(u | z)dz
(3)
= −log E [p(u | z)]
p(Z)
So, surprisal quantifies the information content of the observation as the (negative log of) the expected
likelihood.
A particularly relevant special case to consider is that of a generative model in which the likelihood
p(u | Z)formsabinaryindicatorfunction,specifyingforanyhypothesisz whetheritis‘consistent’withthe
observationu. Inthissetting,themarginallikelihoodp(u)issimplythetotalprobabilitymassofhypotheses
consistentwiththeobservationunderp(Z). Thissuggeststheconnectionbetweensurprisalandbelief-update:
Whenanobservationcausesbeliefstochangedrasticallyduetoalargeproportionofthelatentspacebeing
ruledout,thesurprisalishigh. Thistypeofargumenthasformedaprimaryjustificationforsurprisaltheory,
firstsuggestedexplicitlybyHale(2001,inspiredbyideasfromAttneave,1959),whoframedsurprisalasa
quantificationofthecognitiveeffortassociatedwithreallocatingresourceswhen‘disconfirming’hypotheses
thatwereinconsistentwiththeobservedword,whenparsingintoaprobabilisticgrammar.
Levy(2005,2008a,2013)formalizedanddevelopedthisargument,proposingthateffortbequantified
preciselyasthedivergencebetweendistributionsoverlatentstructuresbeforeandafterobservingaword,
assuming(crucially)abinary-likelihoodsettingwherethisquantificationofbelief-updatesizeismathemati-
callyequivalenttosurprisal.3 Inthefollowingsectionwewillreviewthisderivation,andgeneralizeitto
settingswherethebinary-likelihoodassumptiondoesnotnecessarilyhold.
Followingtheintroductionofsurprisaltheorywiththisjustification,subsequentresearchinpsycholin-
guisticshaselevatedthesurprisalofwordsasakeyquantityoftheoreticalinterest. Inparticular,therecent
developmentofincreasinglysophisticatedlanguagemodels(LMs)asestimatorsofcontextualprobabilityof
stringshasledtoaveinofresearchfocusedonidentifyingthesurprisalestimatorthatprovidesthebestfitto
humanbehaviorin(typically)linearregressionanalyses(Goodkind&Bicknell,2018;Aurnhammer&Frank,
2019;Wilcoxetal.,2020;Wilcoxetal.,2023;Hofmannetal.,2022;Ohetal.,2022;Oh&Schuler,2023a,
2023b;deVarda&Marelli,2023). Yet,connectingsuchanapproachdirectlytotheresource-reallocation
justificationintermsofbelief-updatesizeinrationalinferencereliesonsomeassumptions,whichareseldom
acknowledgedoutright: First,thatthelinkingfunctionbetweensurprisalandcostislinear,andsecond,that
thelikelihoodfunctionisbinary. Thefirstquestionhasreceivedsomeattentioninrecentliterature.4 Yet
toourknowledge,thissecondassumption—whichisnecessarytojustifysurprisalfromthebelief-update
perspective—remainsunexplored. Wetakethisupintheremainderofthispaper.
3 Inference cost as the magnitude of belief update
Withthismotivation,letusre-focusontheideathataword’sprocessingcostisintrinsicallyrelatedtothe
magnitudeofthechangeitinducesinrationalbeliefs. Withbeliefsrepresentedasprobabilitydistributions
3IntheoriginalworkofHaleandLevy,thelatentstructuresbeinginferredareexplicitlyspecifiedasconsistingofsyntactictrees
inamodelofcomprehensionasparsingintoaprobabilisticcontext-freegrammar.Yettheinference-costjustificationandtheidea
thatsurprisalmeasuresthecognitiveeffortinvolvedinreallocationofresourcesasthecomprehenderchangestheirexpectations
abouttheinterpretationoftheutterancewasintendedtobeverygeneral.
4Thereisageneralconsensusthattheempiricalrelationshipis(near-)linear(Smith&Levy,2008,2013;Wilcoxetal.,2023;
W.Xuetal.,2023;Shainetal.,2024),thoughtheformofthelinkingfunctiontohasbeenthetopicofsomedebate,withsome
theoreticalargumentsandempiricalresultsfavoringasuper-linear(seeLevy,2005,§2.8.3;Meisteretal.,2021;Hooveretal.,2023),
orsub-linear(Brothers&Kuperberg,2021)relationship.
5

overpossibleinterpretations,themagnitudeofthischangecanbequantifiedwiththeKullback-Leibler(KL)
divergence(Kullback&Leibler,1951). Alsoknownastherelativeentropyordiscriminationinformation,the
(cid:104) (cid:105)
KLdivergenceD (P (cid:107)Q) =def E log P quantifiesthecostofusingagivendistributionQasasurrogate
KL P Q
fortargetdistributionP,intermsofoptimalcodelength(Cover&Thomas,2006;M.Li&Vitányi,2019),or
statisticalmodelcomparison(e.g.,Burnham&Anderson,2004).5 UsingKLdivergencetocomparingthe
priortotheposterior,intheBayesianinferencesetting,thedivergence
(cid:20) (cid:21)
p(z | u)
D (p(Z | u)(cid:107)p(Z)) = E log (4)
KL
p(z)
p(Z|u)
givesaprecisequantificationoftheamountofinformationaboutlatentZ thatisgaineduponobservingu.
UsingKLdivergencetoquantifyofthemagnitudeofbeliefupdate,wecanstatethefollowinghypothesis
aboutincrementalprocessingeffortasthecostofinference.
Hypothesis2(belief-updatetheory). Theprocessingcostofobservedinputu,incontext,isamonotonic
increasingfunctionofmagnitudeofbelief-update,asquantifiedbytheKLdivergencebetweentheposterior
p(Z | u)andthepriorp(Z)distributions.
cost(u) = f(D (p(Z | u)(cid:107)p(Z))) (5)
KL
wheref isamonotonicallyincreasingfunction.a
aAnalogoustoourdefinitionofsurprisaltheoryabove,wehavenotproposedaspecificlinkingfunctionf,beyondrequiring
thatitbemonotonicallyincreasing.Strongerversionsofthehypothesiscouldbemadewhichrestrictthislinkingfunctiontoa
particularform.
Asameasureofcost,KLdivergencehasthedesirablepropertiesof(i)beingnonnegative,and(ii)vanishing
whenthedistributionsareidentical. WhileitmaybeconvenienttothinkofKLdivergenceasquantifying
the “distance” between two distributions, it does not in fact satisfy the requirements of a distance metric:
Importantly,itisnotasymmetricfunction. Giventhisfact,itisnaturaltoaskwhetherthechoiceoforderof
theargumentsinD (p(Z | u)(cid:107)p(Z))istherightone. Aversionofthisdivergencebutwiththearguments
KL
transposed(sotheprioristhefirstargument—thatis,D (p(Z)(cid:107)p(Z | u)))hassometimesbeenreferred
KL
to as ‘(Bayesian) surprise’ (proposed in Baldi, 2002; developed in Baldi & Itti, 2010, with applications
to vision), based on similar intuitions as those presented here. However, their motivation to choose the
transposed version appears to be mostly one of convenience, without a particular theoretical motivation
(andindeedinotherwork,thesameauthorsusetheversionwithposteriorasthefirstargument: e.g.,Itti&
Baldi,2009). Here,forourproposedmeasureofprocessingcost(hypothesis2)wespecificallyproposeto
usetheKLwiththeposteriorasthefirstargument,forthreereasons. Inthisdirection,itprovidesaproper
information-theoreticquantificationoftheamountbywhichbeliefsaboutZ areupdated(theposterioris
thetarget),andisdefinedevenwhentheposteriorhasasmallersupport(asisoftenplausible,whensome
partofthehypothesisspaceisruledoutbytheobservation;thetransposedversionisnotdefinedinsucha
situation). Also,thisKLdivergencecanprovidedirectconnectiontothecomplexityofsamplingalgorithms
forinference,providinganadditionalindependentargumentformeasuringcostwithdivergencebetween
beliefdistributions,aswewilldiscussbelowin§4.
5Technically,fortwodistributionsP andQoversetZ,thedivergenceD (P(cid:107)Q)isdefinedonlyifP isabsolutelycontinuous
KL
withrespecttoQ(meaninganyeventassignedzeroprobabilityunderQalsomusthavezeroprobabilityunderP).NotethatBayes’
ruleguaranteesthattheposteriorisabsolutelycontinuouswithrespecttotheprior,sothedivergenceD (p(Z |u)(cid:107)p(Z))isalways
KL
well-defined.
6

3.1 Therelationshipbetweenbeliefupdateandsurprisal
Inanylatent-variablemodel,wheresomelatentZ isinferredgivenobservedu,theKLdivergencebetween
posteriorandpriorcanbedecomposedinthefollowingway,pullingoutasurprisalterm.
D (p(Z | u)(cid:107)p(Z))
KL
(cid:122) (cid:125)(cid:124) (cid:123)
(cid:20) (cid:21) (cid:20) (cid:21)
p(z | u) p(z,u)
E log = E log
p(z) p(u)p(z)
p(Z|u) p(Z|u)
(cid:20) (cid:21)
p(u | z)
= E log (6)
p(u)
p(Z|u)
(cid:20) (cid:21)
1 1
= log − E log
p(u) p(u | z)
p(Z|u)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i(u) =def R(u)
So,withoutmakinganyassumptionsaboutthegenerativemodel,thedivergencebetweenpriorandposterior
consistsofthesurprisal,i(u) ≥ 0,minusasecondnonnegativetermwhichwedenoteasR(u)andreferto
asthereconstructioninformation6 ofu—definedastheexpectationundertheposteriorofthenegativelog
probabilityoftheword. Thisquantifiestheinformationthatwouldbeneededundertheposteriordistribution
to describe the precise value of u that was observed (that is, the residual information to ‘reconstruct’ the
observationitself,beyondthatdescribingtheamountbywhichitcausedachangeinbeliefs).
CharacterizingtheassumptionnecessaryforsurprisaltoequalKLdivergence
RearrangingtheaboveequationmakesitclearthatitdescribesawayofpartitioningtheShannoninformation
inuintotwononnegativecomponents:
i(u) = D (p(Z | u)(cid:107)p(Z))+R(u) (7)
KL
The non-negativity of these terms gives that 0 ≤ D (p(Z | u)(cid:107)p(Z)) ≤ i(u) and 0 ≤ R(u) ≤ i(u).
KL
The first term quantifies the size of the Bayesian belief update, and the second term R(u) = i(u) −
D (p(Z | u)(cid:107)p(Z))quantifiestheresidualinformationthatdoesnotcontributetobeliefupdate.
KL
Tounderstandthisdecompositionofsurprisal,itisusefultoconsidersomeexamplesettings. Figure2
givestwoexamplesillustratingthenon-equivalenceofsurprisalandKLdivergence. Eachexamplestartsfrom
thesamepriordistributionoverthefinitesetofalternativesZ = {z ,z ,z },andinbothcasestheresulting
1 2 3
posteriorisalsothesame. Theydifferonlyinthelikelihoodfunction: Inthefirst(fig.2a),thelikelihood
isbinary,andthesurprisalisequaltoKL.Bycontrast,inthesecondexample(fig.2b),thelikelihoodless
than1forsome(infactall)valuesofZ supportedintheposterior,andthisresultsinasituationwherethe
surprisaloftheobservationissubstantiallylargerthanthefirstexample,despitetheKLdivergencebeing
identical. Ineachcase,surprisalisindicatedbyahorizontalbarpartitionedintocomponentsrepresenting
D (p(Z | u)(cid:107)p(Z))andR(u).
KL
Asillustratedintheseexamples,eq.7expressesthefactthat,ingeneral,surprisalformsanupperbound
on the KLdivergence between prior and posterior. In terms of belief update, we can interpret R(u) as a
quantificationofhowmanybitsofsurprisalare,inasense,wasted. WhenR(u)isnegligible,nearlyallbits
of information measured by surprisal contribute to belief update, and surprisal is thus a good measure of
6Thisnameischosentoreflectthemathematicalsimilarityofthistermtowhatissometimescalledthe‘(negative)reconstruction
error’invariationalinference(inwhichcontexttheexpectationistakenunderavariationalapproximationoftheposterior—see,e.g.,
Bleietal.,2017;Liangetal.,2018).
7

surprisal = 2 bits
prior p(· ) likelihood p(u |·) posterior p(·|u)
Z z z z 1 2 3 0 1/4 3/4 0 1 1 0 0 1
0 0.5 1 0 0.5 1 0 0.5 1
⎤ ⎜ ⎬ ⎜ ⎦
KL = 2 bits (R = surprisal – KL = 0 bits)
size of Bayesian belief-update
⎦ ⎜ ⎬ ⎜ ⎤
(a)Ifthelikelihoodisdeterministic,surprisalisequaltoKL.
surprisal = 5 bits
prior p(· ) likelihood p(u |·) posterior p(·|u)
Z z z z 1 2 3 0 1/4 3/4 0 1/64 1/8 0 0 1
0 0.5 1 0 0.5 1 0 0.5 1
⎤ ⎜ ⎬ ⎜ ⎦
KL = 2 bits R = surprisal – KL = 3 bits
size of Bayesian belief-update
⎦ ⎜ ⎬ ⎜ ⎤
(b)Ifthelikelihoodisnondeterministic,surprisalcanbegreaterthanKL.
Figure2: DiagramsillustratingtwoexamplesoftheBayesianbeliefupdategivenanobservationuundertwodifferent
likelihoodfunctionsbutthesamepriordistribution. Ineachcase,thesurprisali(u)isshownasahorizontalbarpartitioned
intothesumoftwononnegativecomponents: D (p(Z |u)(cid:107)p(Z))andR(u). Inexample(a)thelikelihoodisbinary
KL
acrossthesupportoftheprior,andsurprisalisequaltoKLdivergenceat−log(1)=2bits. Bycontrast,in(b),while
4
thechangeinbeliefsfrompriortoposteriorisidentical,thelikelihoodisnotbinary,andsurprisalis−log(1·1)=5bits,
4 8
exceedingKLdivergencebyR(u)=3bits.
thesizeofbeliefupdate. ButwhenR(u)islarge,7 despiteanobservationcontainingalargeamountofraw
Shannoninformation,itdoesnotresultinacorrespondinglylargeshiftinrationalbeliefs.
Thereareanumberofequivalentwaystoviewthenecessaryandsufficientconditionsforsurprisaltobe
preciselyequaltoKL,asstatedinthefollowingproposition.
Proposition1. Thefollowingareequivalent:
(i) ThesurprisalisequaltotheKLdivergence: i(u) = D (p(Z | u)(cid:107)p(Z)).
KL
(ii) ThereconstructioninformationR(u) = 0.
(iii) Thelikelihoodp(u | z) = 1everywhereinthesupportoftheposteriorp(Z | u).
(iv) Thelikelihoodp(u | z) ∈ {0,1}everywhereinthesupportofthepriorp(Z).
Proof. Theequivalence(ii)⇐⇒(i)followstriviallyfromeq.7. Toseethat(ii) =⇒ (iii),notethatR(u)
canvanishonlyifthenegativeloglikelihood(asanonnegativequantity)istriviallyequaltozeroacross
the expectation: That is, if R(u) = 0 then ∀z ∈ Z[p(z | u) > 0 =⇒ p(u | z) = 1]. To see that (iii)
=⇒ (iv),assumeforcontradictionthatthereissomevalueofz inthesupportofthepriorsuchthatits
likelihoodp(u | z) ∈/ {0,1}. Thensincep(z | u) ∝ p(z)p(u | z)wehavethatp(z | u) > 0,sothisisa
valueinthesupportoftheposterior,whichcontradicts(iii). Thefactthat(iv) =⇒ (ii)followstrivially
fromthedefinitionofR(u)(eq.6).
7AndnoteR(u)maybemadeunboundedlylargewhilekeepingKLdivergencefixed,simplybyscalingdownthelikelihoodby
someconstantamountacrossthesupportoftheprior.
8

Inparticular,theaboveequivalentassumptionsholduniversallyinanygenerativemodelwherethelatent
representationsconsistatleastinpartofobservableoutcomes. Thisispreciselytheassumptionthatwasmade
byLevy(2008a),inordertoprovethatsurprisalwasequivalenttoKLdivergence: Inthatwork,asinthe
originalworkofHale(2001)introducingsurprisaltheory,theworkingexamplewasamodelofcomprehension
asparsingintoaprobabilisticgrammar,wherethelatentvariabletargetedbyinferenceconsistedofparse
trees,eachofwhichspecifiesasingleobservablestring(theyieldoftheparse). Inthatsetting,thelikelihood
p(u | z)issimplyabinaryindicatorfunctionforwhethertheobservedstringumatchestheyieldofz. Thus,
surprisalisequivalenttoKLdivergence(byproposition1). Yet,aswehavedemonstrated,inamoregeneral
inferencesetting,thisequivalenceneednothold,andsurprisalmaybearbitrarilylargerthanKLdivergence.
4 Connecting computational-level theory to algorithmic motivations
Surprisal theory, and its generalization proposed here, are situated within the broader scientific program
of rational analysis, aimed at understanding how cognition is adapted to its environment. This approach
tomodelingaspectsofcognitionstartsfromtheassumptionofthattheyareinsomewayoptimal(Newell,
1981;Pylyshyn,1984;Shepard,1987;Anderson,1990),andgivesaconstrainedframeworkwithinwhich
tobuildtheoriesatacomputational level(inthesenseofMarr,1982),generatingetiologicalexplanations
forbehavioralphenomena,bydescribingtheproblemsthatthemindmustsolve,andthepropertiesofideal
solutions. Once equipped with a robust and empirically well-supported explanation of optimal behavior
fromthisperspective,onemaysubsequentlyformulatetheoriesatthealgorithmicleveltypicaloftraditional
psychologicaltheories,providingcognitiveprocessesandmechanismstoexplainhowthebehaviorarises.
Whenitcomestoproblemsofinductivereasoning,probabilisticinferencegivesapreciseframeworkin
whichtodefinewhatitmeansforasystemtobeoptimalwithrespecttotheenvironment: Anoptimalsolution
to the inference problem is one which, on average, makes accurate predictions. The laws of probability
describetheoptimalwaytoredistributeweightonhypothesesinthelightofanobservation,underasubjectivist
interpretationofprobability(Keynes,1921;deFinetti,1972;Savage,1972),shiftingapriordistributiontoa
posteriordistributioninwhatisknownastheBayesianapproachtoinferenceindecisiontheory(Edwards,
1961;Earman,1992;Howson&Urbach,2006)andidealobservermodelsofperceptionandcognition(Green
&Swets,1966;Geisler&Diehl,2003). TotheextentthatarationalBayesianmodelofacognitiveprocess
providesgoodpredictionsofobservedbehavior,itnotonlyprovidesacomputational-levelexplanation,but
alsoprovidesasourceofconstraintstocircumscribealgorithmic-leveltheories,allowingforrationalprocess
models,whichaimtobridgethesetwolevelsofanalysis(Griffithsetal.,2012).
Thehypothesisthatcostscalesinbeliefupdateoperatesatthepurelycomputationallevel. Likesurprisal
theory, it makes predictions about optimal behavior, but does not by itself provide a way to formulate
algorithmic-levelexplanations. However,itsmotivationwithinaprobabilisticinferenceframingprovidesa
usefulintuition. Thegeneralquestionofinferringlatentcausesofobserveddataisacentralwell-studied
problemofinterestinstatisticsandartificialintelligence,andassuch,methodsofapproximateinference
studied in these fields provide a source of candidate mechanisms to explore. This allows us to take a
steptowardformalizingthisalgorithmicmotivationbyconsideringthecomplexityofalgorithmsbasedon
sampling.
AbroadimportantclassofalgorithmsforrationalinferencearethosebasedonMonte-Carlomethods
(Robert&Casella,2004). Thesealgorithmsmakeuseofthesimpleideathatdifficultcomputationscanbe
approximatedusingadiscretesetofrandomsamples,drawnfromappropriateprobabilitydistributions.
Ofparticularrelevanceisthemethodknownasimportancesampling(Kahn,1954;Doucetetal.,2001,
§1.3.2; Robert & Casella, 2004, Ch. 3), whereby an unbiased estimation of some target distribution P is
formedusingappropriatelyweightedsamplesdrawnfromsomeotherdistributionQ. Importancesampling
forms a foundational building block for algorithmic theories of inference. In a very general setting its
9

complexity(numberofsamplesrequiredforanaccurateestimation)canbeshowntoscaleexponentiallyin
thedivergencebetweenthesetwodistributions.8
#samples ≈ eD KL (Q(cid:107)P) (8)
IS(P←Q)
Therelevanceofthisresulttothequestionoffindinganalgorithmicexplanationforprocessingdifficulty
in terms of belief-update becomes clear if we consider the case where the proposal distribution Q is the
prior, and P isthe posterior. Inthis special case (a schemeknown as likelihood weighting), theresult in
eq.8relatescomputationalcomplexitydirectlytothesizeoftheBayesianbeliefupdate,asquantifiedby
divergence D (p(Z | u)(cid:107)p(Z)).9 This KL divergence has been used to quantify the degree of change
KL
to probabilistic representations, interpreted as a measure of the informativeness of an observation during
inferenceinlanguageprocessing(Levy,2008a;Rabovskyetal.,2018;Haleetal.,2022;Kuperberg&Jaeger,
2016;Kumaretal.,2023),andmorebroadlyincognitivescience(Baldi,2002;Itti&Baldi,2009;Friston,
2009;Parretal.,2022)drawingonitsinterpretationastheinformation-theoreticcostofusingonedistribution
to approximate another, in terms of statistical model comparison (e.g., Burnham & Anderson, 2004) or
optimalcodelength(Cover&Thomas,2006;M.Li&Vitányi,2019). Thisframingreinforcestheintuition
thatthecostofinferencebemeasuredwithadivergencebetweenprobabilitydistributions.
Within the rational analysis framework, if a computational level model provides good predictions of
observedbehavior,itprovidesusefulconstraintstocircumscribepotentialalgorithmicleveltheories,which
mustachievethetaskathandwhileatthesametimedemonstratingthekindofbehaviordescribedbythe
computationaltheory. Inourcase,analgorithmictheorymayprovideamodelofcognitiveeffortdirectlyvia
runtimecomplexitysoonewhichmightprovidealgorithmic-levelexplanationforthephenomenadescribed
bythecomputational-levelsurprisaltheory. Assuch,asampling-basedmodelwouldprovidewhathasbeen
calleda‘rationalprocessmodel’aimingtobridgethesetwolevelsofanalysis(Sanbornetal.,2010;Shietal.,
2010;Griffithsetal.,2012). Theconnectionwithimportancesamplingcomplexitymotivatessampling-based
methods’potentialasasourceofprocessleveltheoriesofinference,inlinewithwhathasbeensuggestedfor
sentenceprocessingrecentlyinHooveretal.(2023),andmorebroadlyinmodelsofhumancognition(Shi&
Griffiths,2009;Vul,2010;Griffithsetal.,2024).
Generalizingtoarbitraryproposaldistributions
For our purposes, we have chosen to quantify belief-update precisely as divergence from the prior
distribution,andwedonotexplorethemoregeneralspaceofalternativeproposaldistributionshere.
But,totheextentthatthereisachoiceofproposalthatisclosertotheposteriorthantheprioris,itwould
bebeneficialtodrawsamplesfromthatproposal,ratherthanfromtheprior. Forthatreasonitmaybe
usefultogeneralizeourdescriptiontoarbitraryproposaldistributions. Forinstance,whensampling
hypothesestoexplainanobservation,itishardlyoptimaltodisregardtheobservationwhendrawing
samples. Yetsamplingfromthepriordoespreciselythis. Usingasmarterproposal,whichfocuseson
hypothesesthatareplausiblegiventheobservationwouldbebeneficial(fewersamplesrequiredfora
goodapproximationofthetarget). FurtherdiscussionofthisgeneralizationisinappendixB.
8ThiscomplexityresultisduetoChatterjeeandDiaconis(2018),whoproveinarathergeneralsettingthatasamplesizethat
isexponentialintheKLdivergenceissufficient(andalsonecessary,undersomefurtherassumptions)forimportancesampling’s
approximationerrortobeclosetozerowithhighprobability.
9Note,thereareotherdivergencefunctionsthatcanbedefinedbetweenprobabilitydistributions,whichprovidealternative
waysofmeasuringtheamountbywhichtwodistributionsdiffer,andalsocandescribethecomputationalcomplexityofsampling
algorithms.Choosingsomeotherdivergencefunctionbetweendistributions,suchastotalvariationorχ2divergence(andsubstituting
thisinplaceofKLdivergenceinthecosthypothesisequation)wouldgiveanalternativerealizationofthishypothesis.Wedonot
exploresuchalternativeshere,butseeappendixAforsomediscussionofalternativeprobabilitydivergencefunctions.
10

5 Case study: Comprehension of words with orthographic errors
Anidealsituationfordistinguishingbetweenabelief-updatetheoryofprocessingcostandstandardsurprisal
theoryisonewherethemeaningawordconveysisverypredictableincontext,butthepreciseformofthe
wordislowprobability. Insuchacase,theposteriordistributionshouldbeessentiallyunchangedfromthe
prior,andthussmalldivergence,despitesurprisalbeinghigh.
Forthispurpose,orthographic/typographicalerrorsprovideausefulcasestudy. Naturally,atypo,likeany
noncanonicalproduction,willbelowprobability(highsurprisal),yetintherightcontextsuchanobservation
maynecessitateverylittleifanychangeinexpectationsaboutthemeaningoftheutterance(andthuslowKL
divergence),whentheerrorisrelativelyminor,andoccursonahighlypredictableword. Forinstance,consider
thecontextinfig.3,similartotheexamplediscussedintheintroduction,wherethewordembarrassed should
behighlypredictable(andaccordinglyrelativelyeasytoprocess). Nowconsiderasimpleletter-transposition
error,swappingansandana,resultinginanon-word,embarrsased. Thisobservationcontributesroughly
thesameinformationintermsofhowitwillcauseapersonwhoisreadingforcomprehensiontoadjusttheir
understandingoftheintendedmeaningofthesentence. Thus,intuitively,abelief-updatetheoryofprocessing
costshouldpredictsimilardifficultyforthesetwocases.
By contrast, traditional descriptions of surprisal theory, conceived of explicitly within a probabilistic
grammar(asinHale,2001;Levy,2008a),wouldpredictveryhighcostforamalformedword. Inatraditional
probabilisticgrammar,iftheobservedstringisnotgeneratedbythegrammar,surprisalwouldbeinfinite.
Evenamodelsmoothedtoassignnonzeroprobabilitytoout-of-vocabularystrings,suchobservationswill
haveveryhighsurprisal. Importantly,thisremainstrueforeveninanaccuratenoisy-channelmodelthat
accountsfortrueerrorstatistics: Anyparticulartypo, likeembarrsased, willnecessarilybeassignedlow
probability,evenifitismuchmorelikelythanothernon-wordstrings,forthesimplereasonthatthetotal
probabilitymassassignedtomalformedversionsoftheintendedwordmustbespreadacrossmultiplepossible
waysthismalformationmightberealized.10
Anidealsituationinwhichtodistinguishwhethereffortisdrivenbybelief-updateorsurprisalisone
wheretheKLdivergenceisidenticalacrossexperimentalconditions,butsurprisalcanbemanipulated. For
thispurpose,comparingprocessingeffortonahighlypredictableword,withandwithoutanorthographic
error,providespreciselythiskindofsituation,underanoisy-channelmodelofcomprehensionasinference
abouttheintendedwordZ givenpotentiallymalformedinputu. Inthissectionwedescribeanexperiment
gatheringhumanreadingtimes,whichwecompareagainstrawLMsurprisal,aswellasestimatesofKLand
surprisalinthenoisychannelmodel.
5.1 Experimentaldesign
Toinvestigatehumanprocessingoftyposinacontrolledsetting,wecreatedadatasetofexamplesentences
containingwithtargetwordsinidenticalcontexts,foreachoffourconditions,eitheranexpectedorunexpected
meaning, and with or without a typo: {expected,unexpected} × {nontypo,typo}, as illustrated with an
exampleinfig.3.
5.1.1 Contrastingpredictionsofsurprisalversusbelief-update
Viewingcomprehensionasinferenceabouttheintendedword,surprisalandbelief-updatesizemakedifferent
predictionsaboutacrossourfourconditions. Surprisalshouldbelowonlyfortheexpectedcondition,and
high for the other three, while belief-update should be low for the two expected conditions, and high for
the two unexpected conditions. The distribution over meanings changes little when encountering a word
10Thistypeofaccuracyofproduction-errorprobabilitiesmightplausiblybeexpectedfromthebestmodernLMs,basedonthe
occurrencefrequencytyposandmalformedwordsencounteredintrainingdata.
11

context = Aftertrippingovertheruginfrontofeveryone,shequicklygotup,buthercheeksturnedredand
shefeltdeeply targetu asshewalkedcarefullybacktoherchair.

Condition1. (expected) embarrassed



Condition2. (unexpected) innovative
targetu =
Condition3. (expected_typo) embarrsased



Condition4. (unexpected_typo) innovaitve
Figure3: Exampleexperimentalitem: Acontextwithasetoftargetsineachofthefourconditions. Condition1:
expected—the target word is very predictable given the pre-target context; condition 2: unexpected—the target
wordhasameaningthatisunexpectedinthecontext;condition3: expected_typo—thetargetisthesameasinthe
expectedcondition,butwithaletter-transpositionerrorintroduced;andcondition4: unexpected_typolikewise. Inall
experimentalitems,thesentencecontext(bothpre-andpost-target)waschoseninordertobegrammaticallycompatible
withbothexpectedandunexpectedmeaning.
interpretedasexpressingapredictablemeaningandchangesmoreifthemeaningisunexpected,andthis
shouldbethecaseregardlessofwhethertheobservedworditselfisunpredictableduetoitscontainingaminor
typo. Specifically,wecandescribethefollowingquestionsrelatingtocontrastsbetweenourconditions,in
termsoftheirpredictedeffectonhumanreadingtimeresponseaspredictedbysurprisalversusbelief-update
size.
(A) Whatistheeffectonprocessingcostwhenencounteringtypoversusnon-typo,marginalizingoverthe
expectednessoftheword? [Callthisthetypoeffect.] Surprisaltheorypredictsthistobelarge,asany
particulartypowillbelowprobability. Bycontrast,thebelief-updateaboutintendedwordshouldbe
smallornonexistent,ifthetyposareminorenoughtonotcauseadifferentchangeinbelief.
(B) Whatistheeffectofencounteringawordwithunexpectedversusexpectedmeaning,marginalizing
overwhetherornotthereisatypo? [Callthistheunexpectednesseffect.] Thisshouldbelargeunder
eithertheory.
(C) What is the difference between an unexpected word without a typo, versus an expected word with
atypo? [Thisisunexpectednesseffectversustypoeffect.] Thiscontraststwoconditionswherethe
typo effect and unexpectedness effect work against each other, measuring the extent to which the
unexpectednesseffectoverpowersthetypoeffect. Ifcostscalesinbelief-updatesizethisdifference
shouldbelarge,whereasundersurprisaltheorythepredictionislessclear,butitwouldbeplausiblefor
ittobesmallorevennegative.
5.2 Methods
Wegeneratedacorpusofshorttextscontainingtargetwordswithmeaningsintendedtobeeitherveryexpected
or very unexpected, in controlled contexts, and introduced typos by transposing pairs of adjacent letters.
Usingthiscorpusweconductedaself-pacedreadingtimeexperimenttoassesshumanprocessingcost. We
alsogatheredsurprisalestimatesfromLMsonthesesamematerials,andthenfitseparatestatisticalmodels
forthesetwodifferentdependentvariables,tocomparethepatternsofhumanprocessingcosttorawLM
surprisal.
5.2.1 Materials
Participantsread51shorttextsacross4experimentalconditions(fromadatasetof204uniquestimuli). Each
text contained a single target whose processing cost was of interest, preceded by a prefix of 10 or more
12

words,andfollowedbyasuffixof3ormorewordstofinishthesentenceandavoidwrap-upeffectsontarget
readingtimes. Thetargetdifferedacrossfourconditions: (1)expected,(2)unexpected,(3)expected_typo,
and(4)unexpected_typo. Contextswere eachdesignedtomakethe expectedwordbeverypredictable
giventheprefix. Theunexpectedwordwaschosentohaveameaningthatwouldbehighlyunlikelyinthe
contextbutnotungrammatical. Theexpected_typoandunexpected_typotargetswerecreatedbyadding
typographical errors to the respective non-typo words, as described below. For all items, the post-target
contextofeachtextwasdesignedtobeasnaturalaspossiblewhileworkinggrammaticallywithboththe
expectedandunexpectedwords,soeachtextdiffersonlyatthetargetlocationacrossconditions.
Targetspecification Toisolatetheeffectsweareinterestedin,targetwordsacrossconditionswerematched
forlengthandfrequency.11 Inchoosingthekindoftypographicalerrorstointroduceinourmaterials,our
goalisnottointroducenonwordsthatcannotbedecipheredorleadtoconfusionwithcompetingword(s).
Unambiguoustyposshouldstillhavehighsurprisal,butplausiblynotcausemuchincreasedeffortundera
belief-updatetheory. Forthisreason,weintroducedmisspellingsbythetranspositionoftwoadjacentletters,
inthemiddleoftheword,sincethesetypeoferrorsareamongthemosteasilyunderstood/corrected(Andrews,
1996; Perea & Lupker, 2003; Johnson et al., 2007; Johnson, 2009; Lupker et al., 2008; Huang & Staub,
2021). Alltargetwords(expectedandunexpected)werechosenfromamongthetop5050mostfrequent
lemmasin the Corpus of ContemporaryAmericanEnglish (COCA; Davies, 2008), witha median length
of10letters. Withineachitem,theexpectedwordandtheunexpectedwordwerechosentobematched
as closely as possible for frequency (in COCA) and length (number of characters). The expected_typo
andunexpected_typotargetsweregeneratedbytransposingtwoadjacentcharactersinthecorresponding
non-typoword. Allsuchtranspositionswereofmedialcharacters;notranspositionsinvolvedtheinitialtwo
characters,northefinalcharacter.
Figure3aboveshowsanexampleitem(acontextwithtargetsforeachofthefourconditions).
5.2.2 Procedure
Theself-pacedreadingexperimentwasimplementedusingthePennControllerforIBEX(Zehr&Schwarz,
2018)andwashostedonPCIbexFarm. Eachtrialstartedwithasingleasteriskdisplayedaloneatthecenter
ofthescreen,whichtheparticipantcouldnavigatepasttostarttheself-pacedreadingwhenready,bypressing
thespacebar. Afterthisprimer,thestimulustextwouldbeinitiallypresentedasasequenceofunderscores
thelengthofeachwordinthetext,andtheparticipantcouldpressthespacebartorevealeachwordoneata
timeinsequence,withthepreviouswordrevertingtoanunderscoreassoonastheparticipantadvancedtothe
next. Thetimeintervalbetweenpresseswasrecordedasreadingtime(RT)inmilliseconds. Aftercompleting
aself-pacedreadingsentence,thescreenwouldclear,andacomprehensionquestionwouldbedisplayed,with
fouranswerchoices,onecorrectandthreeincorrect,presentedinrandomorder. Oncetheparticipantselected
ananswer,thenextexperimentaltrialwouldbepresented,startingwiththeprimerasterisk. Comprehension
questionswereidenticalacrossconditions,queryinginformationfromthepre-targetpartofthetext,sothat
theanswerdidnotdependonthetargetword. Accuracyonthesequestionswasusedasanattentioncheck,
andaccuracybelow80%wasacriterionforparticipantdataexclusion. Inouranalysisweusedthereading
timesonnon-targetwordstocontrolforparticipantbaselinereadingspeed,allowingustofocusoncollecting
RTsontargetitemsfromourfourconditionsofinterest,withoutaglobalcontrolcondition.
Eachparticipantwasassignedtooneoffourgroups,inaLatinsquaredesign,withtheconditionsineach
itemrandomizedonceacrossgroups. Allparticipantssaweachofthe51itemsexactlyonetime,witharoughly
11Toreducethepotentialofaflooreffectonreadingtime,wechosetouseonlyrelativelylongwordsastargets(ranginginlength
from7to15characters,withthemajoritybeingbetween10and14characters).Wealsopartiallycontrolledforfrequencybyusing
onlyrelativelycommonwords,duetoempiricalevidencethatfrequency(unigrampredictability)mayhaveaneffectonprocessing
costindependenttocontextualpredictability(Goodkind&Bicknell,2021;Shain,2024).
13

evenbalanceofconditionsacrossitems. Theorderofitemswasrandomizedperparticipant. Participants
wereinformedthatthesentencestheyweretoreadmightcontaintypographicalerrors,andpresentedfour
illustrativepracticetrials,eachfollowedbyacomprehensionquestion,beforestartingexperimentaltrials.
Allself-pacedreadingstimuliandcorrespondingcomprehensionquestions,forexperimentalandpractice
trials,areprovidedinappendixH(tables2and3).
Participants 118participantswererecruitedontheProlificplatform. Participantstookamedianof18½
minutestocompletethestudy,witharewardperapprovedparticipantof£2.85(averagerewardrate: £9.24/hr).
AllparticipantswerenativespeakersofEnglish,mostlocatedintheUnitedStates.
Dataexclusion Mediancomprehensionscoreaccuracywas92.2%(withmean90.4%). Alldatafromany
participantwithlessthanthan80%accuracywasexcluded(followinge.g.,Witzeletal.,2012;Boyceetal.,
2020),resultingin14excludedparticipants,with104participantsremaining. Readingtimedatafromall
wordsintheself-pacedreadingsentences(targetwordsandcontextwords)wereusedinanalysis;reading
timeoncomprehensionquestionswasnotrecorded. Wefollowedcommonpractice(Jegerski,2013;Marsden
etal.,2018;Nicklin&Plonsky,2020;Futrelletal.,2021;HarringtonStacketal.,2018;Burchill&Jaeger,
2024)inexcludingasoutliersanyRTsfasterthan100ms(includingonenegativevalue,duetoanapparent
softwareerror). WealsoexcludedanyRTsslowerthan5000ms.12 ThisRToutlierexclusionstepremoved
onlyabout0.2%ofremainingRTs. Afterexclusions,thedataconsistedofatotalof128,179RTs,5,290of
whichwereontargetstrings,withamedianof24.5RTspertarget.
5.2.3 Languagemodelsurfacesurprisalestimates
Wecomputedsurprisalestimatesofthetargetstringsincontext,usingacollectionofpre-trainedcausalLMs.
Givencontext,intheformofasequenceofinputtokensw ,acausalLMM providesanestimateofthe
1:n−1
next-tokenprefixprobability−logp (w | w )foranytokenw initsvocabulary,computedfromthe
M n 1:n−1 n
logitsofitsfinalhiddenstate. Weobtainedsurprisalestimatesfromavarietyofpre-trainedTransformer-based
LMs,mostlycomputedusingthemodelimplementationsprovidedintheHuggingfaceTransformerslibrary
(Wolfetal.,2020). MoreinformationonlanguagemodelsandtokenizationdetailsareprovidedinappendixD.
5.3 Results
5.3.1 EmpiricalcomparisonofhumanreadingtimeversusandLMsurprisal
Figure 4 shows the empirical means of human reading time response (left subplot) and surprisal (right
subplot), with bootstrapped 99% confidence intervals. In the left subplot, the horizontal axis represents
readingslowdownassociatedwiththetargetword. Slowdowniscalculatedaslogreadingtimerelativeto
participantmean—sothatapositivevalueindicatesreadingtimethatisslowerthanaverageforaparticular
participant,andanegativevalueindicatesfaster. Foreachtargetword,readingtimeswereportareaggregated
overa3-wordregionofinterestconsistingofthetargetwordandthetwosubsequentwords,tocontrolfor
spillover(acommonstrategyinreadingtimestudies;seee.g.,Burchill&Jaeger,2024;Huangetal.,2024).
Inpost-hocanalyses,weexploredotherwindow-sizes,andfoundthata3-wordwindowindeedleadstothe
clearestdistinctionbetweenexpectedvsunexpectedconditions—furtheranalysisisgiveninappendixE.1.
12Wechoseaprioriamoreinclusiveupperoutlierboundthanthe2000msthresholdusedinmuchpreviousliterature(e.g.,the
samereferencescitedabove),sinceweareinterestedparticularlyinhighsurprisalitems,andduetothestrongskewandkurtosis
ofRTdatameaningclassifyinghighRTvaluesasoutliersismorelikelytobeunwarranted. However,preliminaryexperiments
re-runningouranalyseswiththelessinclusiveupperbounddidnotresultinanyappreciablechangeinresults.
14

Empirical means
model
Human RT response LM surprisal GPT2 0.124B
OPT 0.35B
GPT2−XL 1.5B
GPTNeo 2.7B
expected
OPT 2.7B
GPTNeoX 20B
OLMo 1B
OPT 6.7B
unexpected
OPT 13B
GPT3−babbage
OPT 30B
OPT 66B
expected
typo OLMo 7B
Llama2 7B
Mistral 7B
Llama3 8B
unexpected
typo Llama2 13B
Mixtral 8x7B
Llama3 70B
−0.05 0.00 0.05 0.10 0.15 0 10 20 30 GPT3−davinci
slowdown: log( RT/participant_meanRT ) surprisal
Llama2 70B
Figure 4: Empirical means of human reading time response and mean LM surprisal, across the four experimental
conditions. Horizontal bars indicate 99% CIs around the mean. Left: Reading time response represented on the
horizontalaxisas“slowdown”,thelogRTonthetargetregiontimerelativetotheparticipant’soverallmeanlogRT.
Right: Horizontalaxisissurprisal;eachLMisplottedinaseparatecolor. LMsareorderedbytheirmeansurprisalon
theexpected_typocondition,andLMnamesincludenumberofparamete

## 引用

```

```
