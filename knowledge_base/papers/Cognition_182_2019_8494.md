---
title: Cognition 182 (2019) 84–94
authors: Dennis Joosena, Falk Huettiga, Original Articles, Adil Ishagc, Markus Ostareka
year: N/A
keywords: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction
created: 2025-10-29 16:42:41
---

# Cognition 182 (2019) 84–94

## 基本信息

- **作者**: Dennis Joosena, Falk Huettiga, Original Articles, Adil Ishagc, Markus Ostareka
- **年份**: N/A
- **關鍵詞**: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction

## 摘要

Keywords: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction
Languagecomprehension timeadvantageforshape-matchingpicturesinthesentence-pictureverificationtask.Typically,thisfindinghas
Conceptualprocessing beeninterpretedasevidenceforperceptualsimulation,i.e.,thataccesstoimplicitshapeinformationinvolves
Perceptualsimulation theactivationofmodality-specificvisualprocesses.Itfollowsfromthisproposalthatdisruptingvisualproces-
Embodiedcognition singduringsentencecomprehensionshouldinterferewithperceptualsimulationandobliteratethematcheffect.
Sentence-pictureverification
Herewedirectlytestthishypothesis.Participantslistenedtosentenceswhileseeingeithervisualnoisethatwas
previously shown to strongly interfere with basic visual processing or a blank screen. Experiments 1 and 2
replicatedthematcheffectbutcruciallyvisualnoisedidnotmodulateit.Whenaninterferencetechniquewas
usedthattargetedhigh-levelsemanticprocessing(Experiment3)howeverthematcheffectvanished.Visual
noisespecificallytargetinghigh-levelvisualprocesses(Experiment4)onlyhadaminimaleffectonthematch
effect.Weconcludethattheshapematcheffectinthesentence-pictureverificationparadigmisunlikelytorely
onperceptualsimulation.

## 研究背景

## 研究方法

## 主要結果

## 討論與結論

## 個人評論

## 相關文獻

## 完整內容

Cognition 182 (2019) 84–94
ContentslistsavailableatScienceDirect
Cognition
journal homepage: www.elsevier.com/locate/cognit
Original Articles
“ ” ff
Are visual processes causally involved in perceptual simulation e ects in
fi T
the sentence-picture veri cation task?
Markus Ostareka,b,⁎ , Dennis Joosena, Adil Ishagc, Monique de Nijsa, Falk Huettiga
aMaxPlanckInstituteforPsycholinguistics,Nijmegen,TheNetherlands
bInternationalMaxPlanckResearchSchoolforLanguageSciences,TheNetherlands
cInternationalUniversityofAfrica,Khartoum,Sudan
ARTICLE INFO ABSTRACT
Keywords: Manystudieshaveshownthatsentencesimplyinganobjecttohaveacertainshapeproducearobustreaction
Languagecomprehension timeadvantageforshape-matchingpicturesinthesentence-pictureverificationtask.Typically,thisfindinghas
Conceptualprocessing beeninterpretedasevidenceforperceptualsimulation,i.e.,thataccesstoimplicitshapeinformationinvolves
Perceptualsimulation theactivationofmodality-specificvisualprocesses.Itfollowsfromthisproposalthatdisruptingvisualproces-
Embodiedcognition singduringsentencecomprehensionshouldinterferewithperceptualsimulationandobliteratethematcheffect.
Sentence-pictureverification
Herewedirectlytestthishypothesis.Participantslistenedtosentenceswhileseeingeithervisualnoisethatwas
previously shown to strongly interfere with basic visual processing or a blank screen. Experiments 1 and 2
replicatedthematcheffectbutcruciallyvisualnoisedidnotmodulateit.Whenaninterferencetechniquewas
usedthattargetedhigh-levelsemanticprocessing(Experiment3)howeverthematcheffectvanished.Visual
noisespecificallytargetinghigh-levelvisualprocesses(Experiment4)onlyhadaminimaleffectonthematch
effect.Weconcludethattheshapematcheffectinthesentence-pictureverificationparadigmisunlikelytorely
onperceptualsimulation.
1. Introduction Herrnberger,&Kiefer,2008;Ostarek&Huettig,2017a;vanDam,van
Dijk,Bekkering,&Rueschemeyer,2012;Yee&Thompson-Schill,2016)
In theoretical and empirical efforts to understand conceptual pro- by conceiving of conceptual processing as a form of ad hoc sampling
cessing during language comprehension recent work has focused on fromafeaturespacethatisconstrainedbybothlong-termmemoryand
twomainproblems.Thefirstisconcernedwithanaccuratedescription immediatecontext.
oftheinformationalcontentthatisactivatedasweprocesslanguage, Recent behavioural and neuroimaging studies have begun to un-
whereas theseconddeals withthenatureoftheneuralandcognitive ravel the underlying mechanisms and started painting a multifaceted
mechanisms that are used to provide this information. Even though picture of a widely distributed system that includes modality-specific
both are closely related, it is crucial to address both separately processes (Fernandinoet al.,2016; Hauk, Johnsrude, &Pulvermüller,
(Barsalou, 1999, 2016; Binder, 2016; Borghesani & Piazza, 2017; 2004; Lewis & Poeppel, 2014; Ostarek & Huettig, 2017b, 2017a;
Mahon&Caramazza,2008;Mahon,2015). Vukovic, Feurra, Shpektor, Myachykov, & Shtyrov, 2017), different
Regarding conceptual content, an overwhelming body of evidence stagesofconvergencepossiblyculminatinginamodality-independent
suggeststhatlanguageprocessinginvolvesthecontextualizedretrieval centralhub(Bruffaertsetal.,2013;Fernandinoetal.,2016;Patterson,
of a multitude of conceptual features that, together, constitute their Nestor, & Rogers, 2007; Ralph, Jefferies, Patterson, & Rogers, 2017),
meanings (Andersonetal.,2016;Binder&Desai,2011;Binderetal., andflexibleretrievalmechanisms(Kan&Thompson-Schill,2004).
2016;Collins&Loftus,1975;Cree&McRae,2003;Fernandinoetal., Thepresentstudyfocusesononeparticularsemanticfeature;object
2016; Fernandino, Humphries, Conant, Seidenberg, & Binder, 2016; shape.Visualworldeye-trackingstudiesindicatethatprocessingnouns
Huettig & McQueen, 2007; Vigliocco, Meteyard, Andrews, & Kousta, referring to concrete objects activates information about their typical
2009;Vigliocco,Vinson,Lewis,&Garrett,2004).Thisviewistheore- shapes(Dahan&Tanenhaus,2005;Huettig&Altmann,2007).Asmany
tically appealing because it nicely accounts for the high degree of objects can occur in multiple different shapes, listeners often need to
conceptual flexibility (Barsalou, 1993; Hoenig, Sim, Bochev, incorporatecontextualinformationinordertoretrievetheappropriate
⁎Correspondingauthorat:MPIPsycholinguistics,Wundtlaan1,Nijmegen,TheNetherlands.
E-mailaddress:markus.ostarek@mpi.nl(M.Ostarek).
https://doi.org/10.1016/j.cognition.2018.08.017
Received7September2017;Receivedinrevisedform27August2018;Accepted27August2018
0010-0277/ © 2018 Elsevier B.V. All rights reserved.

M.Ostareketal. Cognition 182 (2019) 84–94
shape representations. Using the sentence-picture verification task, a were listening to sentences to directly probe the functional role of
classic experiment by Zwaan, Stanfield, and Yaxley (2002) provided perceptualsimulationinthesentence-pictureverificationtask.
evidence that contextually appropriate shape information is readily
activated during sentence comprehension. In that paradigm, partici- 2. Experiment1
pantsreadorlistentosentencesaboutobjectsthatareimpliedtohavea
certain shape (e.g., The ranger saw the eagle in the sky; implying out- The basic rationale for this experiment was that interfering with
stretchedwings).Shortlyaftersentenceoffset,inthecriticalconditions basic visual processing while participants were listening to sentences
apictureappearsofthementionedobjecteitherinmatching(e.g.,an shouldsignificantlyreducetheusuallyobservedshape-matcheffectifit
eagle with outstretched wings) or mismatching shape (an eagle with relies on perceptual simulation. Conversely, if the match effect is in-
closedwings).Participantsthenhavetoindicateasquicklyandaccu- dependentofvisualsimulation,visualinterferenceshouldnothavean
ratelyaspossiblewhethertheobjectwasmentionedinthesentenceor impact on the match advantage. Experiment 1 used the same kind of
not by pressing one of two buttons. The critical finding (Zwaan & visualinterferencethatwasrecentlyshowntoimpairaccesstovisual
Pecher, 2012; Zwaanet al.,2002) isshorter response latencies inthe information during semantic processing (Edmiston & Lupyan, 2017;
matchingcondition,suggestingthatthesentencesactivateinformation Ostarek & Huettig, 2017a), consisting of dynamically changing Mon-
aboutobjectshapethatisspecificenoughtoproduceaprimingeffect drian-typemasksthatareusuallyusedforcontinuousflashsuppression
on the verification judgement. Although there has been some debate and are designed to maximally interfere with basic visual processing
aboutthereplicabilityofcongruencyeffectsofthistype(Papesh,2015; (Tsuchiya&Koch,2005).Wepredictedthatvisualinterferencewould
Rommers, Meyer, & Huettig, 2013) and about reproducibility more decrease the match advantage based on four considerations: (1) the
generally(Pashler&Wagenmakers,2012;Wagenmakersetal.,2016), match effect pertains to visual shape information, (2) processing of
theshapematchadvantage,atleastinthesentence-pictureverification shapeinformationinearlyvisualcortexhas beenshowntobemodu-
paradigm, has proven to be very robust and reproducible (Engelen, latedinthesentence-pictureverificationtask(Hirschfeldetal.,2011),
Bouwmeester,deBruin,&Zwaan,2011;Rommersetal.,2013;Zwaan (3) previous studies reported interference effects of visual noise on
&Pecher,2012). semanticprocessingofsinglewords(Edmiston&Lupyan,2017;Ostarek
Previousstudieshaveimplicitlyorexplicitlygonefurtherandsug- & Huettig, 2017a), and (4) the intuitive proposal that contextually
gested that the reaction time advantage in the match condition in- embeddedlanguagetendstoengagemorespecificrepresentationsand
dicatesthekindofprocessthatprovidesshapeinformation,namelythe mightthusbemorelikelytoactivatemodality-specificprocessesthan
process of perceptual simulation (Engelen et al., 2011; Pecher, van singlewords(Kurby&Zacks,2013;Zwaan,2014).
Dantzig,Zwaan,&Zeelenberg,2009;Yaxley&Zwaan,2007;Zwaan&
Pecher,2012;Zwaanetal.,2002).Accordingtothataccount,accessing 2.1. Method
conceptual shapeinformation (e.g., abouta flyingeagle) involvesthe
approximatere-instatementofsensoryprocessesthatareactiveduring 2.1.1. Participants
visualperceptionofrelevantobjects(e.g.,ofaflyingeagle). Werecruited115healthyparticipantswithnormalorcorrected-to-
However,onedoesnotneedtoinvokesimulationinordertoexplain normalvisionandnormalhearingfromthelocalMPIsubjectdatabase.
thebehaviouralpattern,asstudiesusingthesentence-pictureverifica- Four had to be excluded due to technical failure, and one due to ex-
tionparadigmcanonlytellussomethingaboutthekindofinformation cessive error rates (>20%), resulting in 110 participants that were
that is accessed, but not about the kinds of processes andrepresenta- used for analysis. We opted for a higher number of participants com-
tions involved. One way to get at the latter question is to study the paredtopreviousstudiesusingthisparadigmbasedonthefactthatour
neuralcorrelatesoftheshapematcheffect.Hirschfeld,Zwitserlood,and designincludedtheadditionalfactorofVisualCondition(visualnoise
Dobel (2011) conducted a magnetoencephalography study using the vs. blank screen) and the conviction that high-powered studies are
sentence-picture verification paradigmto assesschanges inneuralac- needed in the field of experimental psychology (Pashler &
tivity for shape matching vs. mismatching pictures. They observed a Wagenmakers,2012).Participantsreceivedapaymentof6euros.The
stronger positivity to pictures following shape matching vs. mis- study was covered by ethics approval from Radboud University Nij-
matchingsentencesinoccipitalcortexatca.120msafterpictureonset megen.
(M1),suggestingatop-downmodulationofearlyvisualprocessingasa
functionofshapematchvs.mismatch.However,changesinthewaythe 2.1.2. Materials,set-up,anddesign
targetpicturewasvisuallyprocesseddonotnecessarilyimplythatvi- WeusedthematerialsfromtheoriginalZwaanetal.(2002)study
sualprocesseswereactivatedduringcomprehension.Indeed,thatsce- that were provided by Rommers, Meyer, Praamstra, and Huettig
nario would predict repetition suppression, not enhancement. There- (2013). They included 40 quadruplets of pairs of sentences implying
fore,thedataareconsistentwithwithtop-downinputfromhigher-level shape A or shape B and corresponding pairs of pictures of the men-
cortical areas. Thus, this approach still cannot answer whether visual tionedobjectsinshapeAorshapeB,andtherewere40fillersentences
processeswereinvolvedinsentence comprehension,as,similartoRT pairedwithtargetpicturesthatarenotmentionedinthesentence.In
paradigms,whatismeasuredistheeffectofthecomprehensionprocess theoriginaldesign,everyparticipantsawoneoffoursentence-picture
onpictureverificationthathappensonlyaftersentencecomprehension combinations,resultinginfourlists.Inthepresentstudy,theadditional
isaccomplished(Mahon&Caramazza,2008). factor of Visual Condition (visual noise vs. blank screen) was added
One direct way of testing the hypothesis that visual processes are suchthateverysentence-picturepairwasstillonlyshownoncetoeach
functionallyinvolvedinvisualinformationretrievalistointerferewith participant,butacrossparticipantseverypairoccurredequallyoftenin
visual processing during language comprehension and assess whether thevisualnoiseandblankscreencondition,resultingineightlists.
visual information retrieval is impaired. Recent studies have demon- Participants were seated 60cm from the screen and placed their
stratedthatdynamiclow-levelvisualnoisepatternscanselectivelyin- headonachinrest.Presentation(NeurobehavioralSystems)wasused
terfere with the retrieval of visual information during auditory single tocontrolthedisplayoftargetpicturesandvisualnoiseaswellasthe
word processing (Ostarek & Huettig, 2017a) and in a property ver- sentences that were played back on headphones. Auditory sentences
ification task (Edmiston & Lupyan, 2017), and they can strongly di- wereusedinsteadofwrittensentencestobeabletointerferewithvisual
minish the effectiveness of a word cue on a subsequent picture dis- processingduringsentencecomprehension.Thetaskwastolistentothe
criminationtask(Edmiston&Lupyan,2017).Here,weusedthevisual sentences and to decide as quickly and accurately as possible by
noise technique to interfere with visual processing while participants pressing one of two buttons (left/right on a house-built button box,
85

M.Ostareketal. Cognition 182 (2019) 84–94
Fig.1.Illustrationofthedesignandtrialstructurewithoneofthesentencesinthevisualnoisecondition.
counterbalanced across participants) whether the subsequently dis- 2.2. Resultsanddiscussion
played picture represented an object that was mentioned in the sen-
tenceornot. The results showed a significant main effect of Visual Condition
Everytrial(seeFig.1)startedwithafixationcrossatthecentreof (estimate=−0.034, SE=0.014, t=−2.48, p=0.016) with slower
the screen (500ms) followed by an auditory sentence (ca. 2s on responses in visual noise trials (M=758ms, SD=252ms) compared
average). Sentences were accompanied by visual noise in half of the toblankscreentrials(M=741ms,SD=251ms).Wealsoobtaineda
trials. It consisted of 80 masks that were all generated by randomly main effect of Match Condition (estimate=−0.071, SE=0.030,
superimposing1000rectanglesofdifferentcoloursandsizes(similarto t=−2.35,p=0.021)withshorterRTsintheshape-matchingcondi-
Hesselmann,Hebart,&Malach,2011).Foreverytrialarandomorder tion(M=735ms,SD=241ms)comparedtothemismatchingcondi-
wasgeneratedforthe80masksandtheyweredisplayedatarateofca. tion(M=764ms,SD=261ms),thusreplicatingthematcheffect.As
10Hzuntil250msaftersentenceoffset,atwhichpointthetargetpic- canbeseeninFig.2,therewas,however,noevidenceforaninteraction
turewaspresented.Onceabuttonwaspressedor3selapsed,thenext betweenthetwofactors(t < 1).Thematcheffectwaspresentbothin
trial started. After half of the filler trials, a comprehension question blank screen trials (estimate=−0.076, SE=0.036, t=−2.13,
appearedonthescreentoencourageparticipantstolistencloselytothe p=0.037) and in visual noise trials (estimate=−0.067, SE=0.03,
sentences. t=−2.26,p=0.027).
Thus,ourresults suggestthatshapeinformationwasactivated(as
reflectedbythematcheffect),butlow-levelvisualprocesseswerenot
2.1.3. Analysis necessaryforit(asreflectedbytheirrelevanceofvisualinterferencefor
Priortoanalysis,fillersandtrialswithincorrectresponsesorwith the match effect). In the context of two recent studies that reported
RTsfasterthan300msorslowerthan2500mswereexcluded.Wethen disrupted access to visual information due to the same type of visual
removed trials with RTs 2.5 SDs or higher from the grand condition noise in paradigms using single words (Edmiston & Lupyan, 2017;
means.RTswerestandardizedbysubtractingthemeananddividingby Ostarek & Huettig, 2017a), it seems implausible that the visual noise
the SD for analysis. The resulting dataset was analysed using linear techniquedidnotsufficientlyinterferewithbasicvisualprocessing.By
mixedeffectsmodellingasimplementedintheRpackagelme4(Bates, extension,higherlevelprocessescanbeassumedtohaveprovidedthe
Mächler, Bolker, & Walker, 2014). The full model included Match implicit shape information. Regarding the study by Hirschfeld et al.
Condition(matchvs.mismatch)andVisualCondition(visualnoisevs. (2011),this result speaks against thepossibility that theoccipital M1
blank screen) and their interaction as fixed effects and by-participant modulation they observed reflected low-level visual simulations acti-
and by-sentence random intercepts and slopes for Match Condition, vatedduringcomprehension.Thisisconsistentwiththeincreasedpo-
Visual Condition, as well as the interaction term. The fixed effects sitivityobservedbyHirschfeldetal.(2011),asopposedtoadecrease
predictors were coded as (1, −1). To obtain p-values, we computed thatwouldbeexpectedinapriming-via-re-activationexplanation(due
type 3 conditional F-tests with Kenward-Roger approximation for de- torepetitionsuppression).
grees of freedom as implemented in the Anova function of the car
package(Fox&Weisberg,2011),whichcallsthefunctionKRmodcomp 3. Experiment2
ofthepbkrtestpackage(Halekoh&Højsgaard,2014).Ofmaininterest
waswhetherwewouldfindareductionofthematcheffectinthevisual Experiment1didnotprovideevidenceforthehypothesisthatthe
noise condition, as reflected in the interaction between Match Condi- shape match effect in the Sentence-Picture Verification task relies on
tionandVisualCondition.Weadditionallyperformedplannedfollow- low-levelperceptualsimulation.Itisimportanttonote,however,that
upanalyseslookingattheeffectofMatchConditioninthevisualnoise weusedatypeofvisualnoisethatselectivelyinterfereswiththemost
condition and the blank screen condition separately. Specifically, we basic computations related to local colour, edge, and orientation de-
useddummycodingoftheVisualConditionfactortoobtainthesimple tection involving only horizontal and vertical components. Thus, it
effectsofMatchConditionforblankscreenandvisualnoisetrials,re- remains possible that visual processes higher up in the hierarchy are
spectively. The analysis scripts and raw data can be found at the causally involved in providing conceptual shape information. To test
website of the Open Science Framework (https://doi.org/10.17605/ this possibility, we replaced the low-level visual noise with what we
OSF.IO/HNDG2). might call mid-level visual noise. 80 mid-level noise masks were
86

800
750
700
650
600
no yes
Visual Noise
generatedbysuperimposing30–40imagesofrandomobjects(fromDe unlikely to evoke consistent semantic associations. Our rationale for
Groot, Koelewijn, Huettig, & Olivers, 2016) and distorting them with using these masks as visual noise was that they should interfere with
theAdobePhotoshopfunctions“shear”,“ripple”,and“crystalize”such neuronpopulationswithlargerreceptivefieldsthataretunedtocom-
that they could no longer be recognised (see Fig. 3). The resulting plex conjunctions of multiple visual features (Peirce, 2015; Vernon,
images were at least as visually complex as real objects but were Gouws, Lawrence, Wade, & Morland, 2016). As such, the mid-level
)sm(
TR
M.Ostareketal. Cognition 182 (2019) 84–94
match
mismatch
Fig.2.Experiment1.MeanRTstoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.
Errorbarsindicate95%confidenceintervals.
Fig.3.Examplesofthemid-levelvisualnoisemasksusedinExperiment2.
87

visual noiseallowed us to test whether mid-level visual processes are
involvedintheshapematcheffect.
3.1. Method
3.1.1. Participants
Werecruited114participantsfromtheMPIsubjectdatabase,oneof
whichwasexcludedduetoanerrorratehigherthan20%.
3.1.2. Materials,set-up,design,andanalysis
EverythingwasidenticaltoExperiment1exceptfortheuseofmid-
levelvisualnoisethatwaspresentedatca.10Hzinvisualnoisetrials.
3.2. Resultsanddiscussion
Thedataweretrimmedforincorrectedresponsesandoutliersinthe
samewayasinExperiment1.Therewasagainasignificantmaineffect
of Visual Condition (estimate=−0.065, SE=0.014, t=−4.70,
p < 0.001)withslowerRTsinthevisualnoisecondition(M=777ms, prior interpretation (Zwaan et al., 2002; Zwaan, 2003). Given the re-
SD=239ms) compared to the blank screen condition (M=744ms, centdemonstrationsoftheinvolvementoflow-levelvisualprocessesin
SD=231ms), and a main effect of Match Condition (esti- semanticprocessingusingthesameinterferencetechnique(Edmiston&
mate=−0.069,SE=0.033,t=−2.10,p=0.039)withshorterRTs Lupyan, 2017; Ostarek & Huettig, 2017a), it is implausible that the
in the match (M=746ms, SD=229ms) compared to the mismatch visual noise we used was not capable of impeding simulation. Never-
condition(M=774ms,SD=241ms).Again,asFig.4indicates,there theless, Experiment 3 was designed to ascertain that dynamic visual
wasnoevidenceforaninteraction(t < 1).Thesizeofthematcheffect noise can inprinciple reducethe match effect inthe sentence-picture
was similar in the blank screen condition (estimate=−0.079, verificationparadigm.Tothatend,themeaninglessvisualnoisemasks
SE=0.035, t=−2.23, p=0.029) and in the visual noise condition werereplacedwithpicturesofintactobjectsthatwereagaindisplayed
(estimate=−0.06,SE=0.035,t=−1.70,p=0.094). atca.10Hz(henceforthsemanticnoise;seeFig.5).Thecriticaldiffer-
Thus, we again replicated the match effect but found no evidence ence to the previous two experiments was that pictures activate se-
thatmid-levelvisualprocesseswerefunctionallyinvolved. mantic representations, even when they are presented for very short
durations and in rapid succession (Potter, 1976; Potter, Wyble,
Hagmann, & McCourt, 2014; Thorpe, Fize, & Marlot, 1996). Recent
4. Experiment3
studieshavealsoshownthatvisualobjectrecognitioneveninvolvesthe
The results of the first two experiments point to a striking in- rapidactivationofobjectnamesinadults(McQueen&Huettig,2014),
dependence oftheshapematcheffectfromvisualprocesses,givenits andevenintoddlers(Mani&Plunkett,2010).Assuch,semanticnoise
800
750
700
650
600
no yes
Visual Noise
)sm(
TR
M.Ostareketal. Cognition 182 (2019) 84–94
Fig.5.Examplesoftheobjectsusedassemanticnoise.
match
mismatch
Fig.4.Experiment2.MeanRTstoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.
Errorbarsindicate95%confidenceintervals.
88

800
750
700
650
600
no yes
Visual Noise
can be expected to interfere with the access to conceptual shape in- thepreviousexperiments)aresummarisedinFig.6.Asintheprevious
formationimplicitinsentences. experiments, there was a main effect of Visual Condition (esti-
mate=−0.081,SE=0.018, t=−4.50,p < 0.001) withshorterre-
sponses in the blank screen condition (M=720ms, SD=209ms)
4.1. Method
compared to the visual noise condition (M=752ms, SD=206ms).
There was no significant main effect of Match Condition (matching:
4.1.1. Participants
M=727ms, SD=206ms; mismatching: M=745ms, SD=210ms;
We recruited 112 participants with normal or corrected-to-normal
estimate=−0.047,SE=0.034,t=−1.38,p=0.172),butcrucially,
vision and normal hearing from the MPI subject database. One parti-
the interaction was significant (estimate=−0.035, SE=0.017,
cipantwasexcludedduetoanerrorrate>20%.
t=−2.11, p=0.042), reflecting the match effect (of 31ms) in the
blank screen condition (estimate=−0.081, SE=0.039, t=−2.11,
4.1.2. Materials,set-up,design,andanalysis p=0.038) compared to the absent match effect (5ms) in the visual
EverythingwasidenticaltoExperiment1and2exceptfortheuseof noise condition (estimate=−0.012, SE=0.036, t=−0.34,
objectpicturesasdynamicvisualnoise(againca.10Hz).Tothatend,
p > 0.7).
80 pictures were randomly selected from the de Groot et al. (2016)
Thus, semantic noise strongly interfered with the access to shape
database with the constraint that they did not represent items men-
informationimplicitinsentences,demonstratingthatdynamicvisually
tionedinanyofthesentences. presented stimuli can be effective at interfering with the retrieval of
conceptualshapeinformationiftherelevantsystemistargetedbythe
4.1.3. Resultsanddiscussion noise.Thisconfirmsthattheabsentimpactofvisualnoiseonthematch
DuetoalargenumberofparticipantswithveryhighmeanRTsand effect in Experiments 1 and 2 was unlikely due to an inability of the
large SDs, the outlier removal procedure used in the previous experi- presentinterferencetechniquetodiminishit,butratherduetoitsin-
mentsresultedinveryfewobservationsperconditioninsomepartici- dependenceofmodality-specificvisualprocesses.
pantsandthemixedeffectsmodeldidnotconverge.Wethereforeex-
cluded participants who had mean RTs larger than 1000ms and SDs 5. Experiment4
higherthan400msinatleastoneconditiontoreducenoise.1Thedata
fromtheremaining73participants(thatweretrimmedforoutliersasin The experiments presented sofarindicate thatlow-level and mid-
levelprocessesdonotcontributefunctionallytotheshapematcheffect
(at least not to a theoretically interesting extent), whereas semantic
noisereducedittonear-zero.Oneinterpretationoftheseresultsisthat
the match effect does not rely on modality-specific visual processes.
Alternatively, it is conceivable that shape information is provided by
modality-specificvisualprocesses,butatahigherlevelthantestedin
Experiments1and2.Specifically,adifferencebetweenthevisualand
)sm(
TR
M.Ostareketal. Cognition 182 (2019) 84–94
match
mismatch
Fig.6.Experiment3.MeanRTsoftoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.
Errorbarsindicate95%confidenceintervals.
1Wealsoconductedageneralizedlinearmixedeffectsanalysiswithagamma
distributionusingallofthedata(nooutlierremovalexceptforRTs<300ms).
Thisanalysisiswellsuitedforpositivelyskeweddata,askindlypointedoutby
oneofthereviewers.Inshort,itsimilarlyshowedadecreaseofthematcheffect
in the semantic noise condition (estimate=-7.814, SE=3.842, t=-2.03,
p=0.042), thus confirming the results above without outlier removal. See
supplementarymaterialsformoreinformationandequivalentanalysesforthe
otherexperiments.
89

M.Ostareketal. Cognition 182 (2019) 84–94
Fig.7.Examplesofthehigh-levelvisualnoisestimuli.
semantic noise manipulations was that the pictures used as noise in participants with RTs slower than 1s and SD higher than 400ms (on
Experiment3areprocessedholisticallyandrequirefigure-grounddis- average)inatleastonecondition2.Theremaining93participantswere
crimination. Thus, it is possible that semantic noise interfered with usedfortheanalysesreportedbelow.TherewasamaineffectofVisual
accesstoshapeinformationbecauseofinterferenceonahighvisual- Condition (estimate=−0.047, SE=0.015, t=−3.14, p=0.003),
ratherthansemantic-level.Toteasethesetwopossibilitiesapart,we withslowerresponsesinthevisualnoisecomparedtotheblankscreen
ran a final experiment with visual noise consisting of abstract shapes condition. There was also a main effect of Match Condition (esti-
thatareperceivedasobjectsbutdonotevokeanyparticularsemantic mate=−0.076, SE=0.036, t=−2.09, p=0.04), with faster re-
associations(seeFig.7).Therationalewasthatthistypeofvisualnoise sponsesinthematchvs.mismatchcondition.However,therewaslittle
taps high-level visual processing related to holistic object perception evidence for an interaction between the two (estimate=−0.016,
and figure-ground separation, but has no or very limited effects on SE=0.014, t=−1.17, p=0.25), suggesting that high-level visual
(non-visual)semanticprocessing. noisedidnothavearobustimpactonthesizeofthematcheffect(see
Fig.8).Therewasadescriptivetrendtowardsareductionofthematch
5.1. Method effectreflectedinthepatternthatthematcheffectwassignificantinthe
blank screen trials (estimate=−0.092, SE=0.038, t=−2.42,
5.1.1. Participants p=0.018), but not in the visual noise trials (estimate=−0.06,
115 participants with normal hearing and normal or corrected-to- SE=0.040,t=−1.51,p=0.136).
normalvisionwererecruitedfromtheMPIdatabaseandwerepaidsix
eurosfortheirtime.Twohadtobeexcludedduetoerrorrateshigher
than20%. 6. Exploratoryfollow-upanalyses
5.1.2. Materials,set-up,design,andanalysis InExperiments1,2,and4,basedonthehighstatisticalpowerofthe
Theexperimentwasidenticaltothepreviousonesexceptforthe80 experiments and the small t-values we observed, we interpreted the
visual noise objects which were nonsense-objects designed in Adobe null-results for the interaction between Visual Condition and Match
Photoshop.Theyvariedinsize,colour,andshape,andweredesignedto Conditionasspeakingagainstafunctionalroleofvisualsimulationsfor
look like possible objects without resembling any particular existing the shape match effect. However, instead of a categorical yes/no
object. characterization of this result, it would be preferable to quantify the
amountofevidenceinfavourofaneffectofvisualnoiseonthematch
5.1.3. Resultsanddiscussion
Thedataweretrimmedforoutliersasinthepreviousexperiments.
2We again conducted a generalized linear mixed effects analysis with a
As in Experiment 3, there was an unexpected number of participants
gamma distribution using all of the data (no outlier removal except for
with very longRTs that resulted innon-convergence ofthe mixed ef- RTs<300ms).TherewasstrongevidenceforamaineffectofVisualCondition
fects model. As before, to reduce noise we therefore excluded (t=-3.35, p<.001) and a main effect of Match Condition (t=-5.15,
p<0.001),butnoevidenceforaninteraction(t<1,p>.7).Seethesupple-
mentarymaterialsforadditionalinformation.
90

800
750
700
650
600
no yes
Visual Noise
effect.Toachievethis,weconductedBayesianfollow-upanalysesusing directional BF favoured the null hypothesis, the directional BF sug-
thebrmspackageinR(Bürkner,2016).Thestrengthofthispackageis gested that thereis some evidence foran interaction in the predicted
thatitallowedustousethesamefixedandrandomeffectsstructureas direction (estimate=−0.017, SE=0.014, 95% CI=−0.044, 0.012,
inthelinearmixedeffectsmodelsreportedaboveandcalculateBaye- BF01=2.46,BF =7.62).ThepicturewasdifferentinExperiment3
dir
sian mixed effects estimates and 95% credible intervals (CrIs). The (semantic noise) where zero was not within the 95% CrI, the non-di-
hypothesisfunctionallowedustocomputeBayesFactorsbasedonthe rectionalBFprovidedevidenceagainstthenull,andthedirectionalBF
Savage-Dickey method (Wagenmakers, Lodewyckx, Kuriyal, & provided very strong evidence for an interaction in the predicted di-
Grasman,2010)whichcomputesanevidenceratiobetweenthefitofa rection (estimate=−0.032, SE=0.016, 95% CI: −0.063, −0.002,
modelthatassumestheregressionweightoftheinteractiontobezero BF01=0.34,BF =63.52).
dir
(nullhypothesis)andamodelthatassumesittobenon-zerotakinginto TheBayesian follow-up analyses thus confirmedthatthereisvery
account the priors and the new data (BF01; numbers larger than one little evidence that visual noise robustly diminished the match effect.
indicate evidence infavor ofthe null). We also calculated directional This was further supported by a linear mixed-effects analysis that
BayesFactorsasamoresensitivemeasureofareductionoftheshape pooledthedatafromallthreevisualnoiseexperiments(Experiments1,
matcheffect(BF ;numberslargerthanoneindicateevidenceinfavor 2,and4)toobtainmaximalpowertodetectasmallinterferenceeffect:
dir
oftheinteractioneffectinthepredicteddirection).Theyarebasedon The results indicate that Visual Condition (t=−6.85) and Match
anevidenceratiobetweenthefitofamodelthatassumestheregression Condition (t=−2.25) have robust effects, whereas the interaction
weight of the interaction to be negative (i.e. that assumes that noise doesnot(t=−1.22).
reducestheshapematcheffect)toamodelthatassumestheregression Notethatourstudieswerenotdesignedtoinvestigatewhetherse-
weighttobepositive(i.e.thatassumesthatnoiseincreasestheshape manticnoiseproducesmoreinterferencethanvisualnoise(asthiswas
matcheffect).PriorsforExperiment1wereanestimatebasedonpre- not our research question). Experiment 3 (semantic noise) was con-
vious experiments using the sentence-picture verification paradigm ductedtoprovideanotherdemonstration(inadditiontotheonesinthe
(Rommersetal.,2013;Zwaan&Pecher,2012;Zwaanetal.,2002).For literature) that the noise manipulation can in principle interfere with
the remaining three experiments, the estimates and 95% credible in- semantic processing. Nevertheless, on reviewer request, we assessed
tervals ofthe previous experiment (including all fixed effects andthe statistically whether semantic noise reduced the match effect more
intercept) were used as priors for the following experiment, respec- stronglythanvisualnoise.Ideally,onewouldperformananalysisthat
tively.100000iterationswererunpermodelandthethinningratewas adds Experiment (i.e. type of noise) as additional factor. This would
setto100. however yield a design with many more factors/levels for which our
In Experiment 1 (low-level visual noise) and Experiment 2 (mid- samplewouldbeunlikelytohaveadequatepower.Tomaintaina2x2
levelvisualnoise),the95%CrIsincludedzero,thenon-directionalBFs design, for which our sample was intended, we performed a subset
supportedthenullhypothesis,andthedirectionalBFprovidedminimal analysis without the blank screen trials and coded all types of visual
evidence for an interaction in the predicted direction (Experiment 1: noise(low,mid,andhigh-level)as“visual”.Thus,weranalinearmixed
estimate=−0.005, SE=0.013, 95% CI: −0.03, 0.02, BF01=7.82, effects model with Type of Noise (visual vs. semantic) and Match
BF =1.84; Experiment 2: estimate=−0.008, SE=0.012, 95% CI: Condition(matchingvs.mismatching)asfixedeffects,per-participant
dir
−0.031, 0.016, BF01=3.35, BF =2.76). In Experiment 4 (high- randominterceptsandslopesfortheeffectofMatchCondition,andper-
dir
levelvisualnoise),althoughthe95%CrIsincludedzeroandthenon- sentence random intercepts and slopes for both factors and their
)sm(
TR
M.Ostareketal. Cognition 182 (2019) 84–94
match
mismatch
Fig.8.Experiment4.MeanRTsoftoshape-matchingvs.mismatchingpictures(colour-coded)withandwithoutvisualnoiseduringauditorysentencepresentation.
Errorbarsindicate95%confidenceintervals.
91

M.Ostareketal. Cognition 182 (2019) 84–94
interaction (estimate=−0.025, SE=0.012, t=−1.99, p=0.05). This raises interesting questions for future research: One possible
Thisresultisconsistentwiththeviewthatsemanticnoisereducedthe accountisthatinterferenceincreasesastheactivationofsemanticin-
match effect more strongly than visual noise. Future studies could formation increases. The more high-level a mask, the more likely it
comparetherelativeeffectofdifferenttypesofnoisebymanipulating presumably is to activate semantic information which then leads to
thisfactorwithin-subjects. interference with the retrieval of semantic shape information during
sentenceprocessing.Relatedtothat,itwouldbeinterestingtofindout
7. Generaldiscussion what exactlyaccounts fortheinterference effect inducedbysemantic
noise.Itisplausiblethatthesemanticsystemissimplyoverloadeddue
Thesentence-pictureverificationtaskhasbeenofgreatvalueforthe tothehighrateofobjectsthatarebeingprocessed.Itisalsopossible
fieldoflanguageprocessing,asitisaversatiletooltorevealthecon- thattheirrelevantobjectsareimplicitlylabelled.Thiscouldbetested
tents of conceptual representations that are activated as listeners/ byusingobjectsthatareeasy(e.g.,apple)vs.difficult(e.g.,dragonfruit)
readers comprehend sentences. The key insight was that a match vs. tolabel.Alternatively,thepresenceofcertainvisualcharacteristicsin
mismatch ina feature ofinterest between asentence and afollowing themasksmightdeterminetheamountofinterference.Onelimitation
picturemodulatesresponselatenciesintheverificationtasktotheex- ofourstudyisthatvisualandsemanticnoisemasksdifferedinvisual
tentthatthefeaturewasactivatedduringsentenceprocessing.Previous features. Thus, it is possible that the types of visual noise we used
studies have shown that listeners activate object shape information lackedvisualcharacteristicsthatarepresentinsimulations,suchas3D
implicit in sentences by demonstrating a reaction time advantage in structure, or the presence of different parts or textures. Whereas it
subsequent sentence-picture verification for target pictures that mat- seemsunlikelythataccesstothesekindsofdetailsplaysabiggerrole
chedtheimpliedobjectshape(Hirschfeldetal.,2011;Rommersetal., fortheshapematcheffectthanaccesstoapproximateshapeinforma-
2013;Zwaan&Pecher,2012;Zwaanetal.,2002). tion(seeHirschfeldetal.,2011),moreworkisneededtofullymapout
Here, we asked which processes enable the retrieval of shape in- thefactorsthatdeterminetheamountofinterferencewiththeretrieval
formation during online sentence comprehension. The match effect is ofsemanticshapeinformation.
typically interpreted as indexing perceptual simulation in sentence Overall,ourfindingsaretheoreticallyimportantastheyconstitute
comprehension (Engelen et al., 2011; Zwaan & Pecher, 2012; Zwaan evidence against a strong perceptual simulation-based explanation of
et al., 2002; Zwaan, 2003). In particular, the idea is that modality- the shape match effect. Our findings are compatible with accounts of
specific visual processes relevant for shape perception are recruited amodalrepresentationthatassumeconceptualprimingeffectstoarise
duringlanguagecomprehensiontoprovideconceptualshapeinforma- in high-level systems with a non-modality-specific representational
tion.Consequently,whenasubsequentshapematchingpictureappears system(e.g.,Fodor,1975).Theyarealsoconsistentwiththegrounding-
it is processed more efficiently due to the pre-activation of relevant by-interaction model (Mahon & Caramazza, 2008; Mahon, 2015),
visual processes. However, while the sentence-picture verification which hypothesises amodal representations that are connected to the
paradigm in its basic form is well-suited to uncover the contents of sensory systems, but only to the extent that sensory states do not
representations activated during sentence comprehension, it does not measurably affect processing in the amodal system. This model can
allow inferences about how this content is represented. As Hirschfeld account for our behavioural results and the MEG data reported by
etal.(2011)pointedout;besidesperceptualsimulationthematcheffect Hirschfeldetal.(2011)bypostulatingacongruencyeffectinanamodal
is consistent with a top-down effect based on amodal semantic re- conceptualsystemthatoptionallyinteractswithvisualprocessing.One
presentations (Mahon, 2015) and with task-based expectations/pre- prediction following from this account that could be tested in future
dictionsaboutthetarget(Rabagliati,Doumas,&Bemis,2017;Rommers studies is that visual interference diminishes the modulation of early
etal.,2013),eventhough shapecongruency wasalsofoundtobere- visualprocessing(asobservedbyHirschfeldetal.(2011))withoutaf-
flected in the N400 (Coppens, Gootjes, & Zwaan, 2012) and in re- fectingthesizeofthematcheffect.
cognition memory performance (Pecher et al., 2009) when sentences It is important to stress that we are not denying that perceptual
andpicturesweretemporallydecoupled. simulation contributes to language comprehension, given the large
Todirectlyprobethefunctionalroleofperceptualsimulationforthe body of evidence for this view (e.g., Barsalou, 2008; Correia et al.,
shape match effect, in two high-powered experiments we employed 2014; Fernandino et al., 2016; Hauk et al., 2004; Lewis & Poeppel,
visual noise to interfere with basic visual processing during sentence 2014; Meteyard, Cuadrado, Bahrami, & Vigliocco, 2012; Ostarek &
comprehensionandobservedverylittleevidenceforadecreaseinthe Huettig, 2017b, 2017a; Pulvermüller, 2005; Vukovic et al., 2017).
matcheffect.Thiswasdespitetheuseofavisualnoisetechniquethat Otherparadigmshaverecentlyprovidedcompellingevidencethatlow-
wasdevelopedtomaximallyinterferewithvisualprocessing(Tsuchiya level visual processes (likely related to shape) are engaged in the
&Koch,2005)andthathasrecentlybeenshowntoselectivelyhinder comprehension of concrete object words (Edmiston & Lupyan, 2017;
access to visual information in single word processing (Edmiston & Lewis & Poeppel, 2014; Ostarek & Huettig, 2017b, 2017a). However,
Lupyan, 2017; Ostarek & Huettig, 2017a). A third experiment using thematcheffectinthesentence-picture verificationtaskseemstode-
picturesofirrelevant objectsasnoise(semanticnoise) obliteratedthe pendonhigher-levelprocesses.
match effect, suggesting that relatively high-level cognitive processes Thisisastrikingresultnotonlybecausetheshapematcheffectis
drivethematcheffect.Toprobewhetherthese processesarebestde- consideredahallmarkfindingfortheoriesofembodiedcognition,but
scribed as high-level visual or modality-independent semantic pro- alsobecausetheparadigmseemssuchagoodcandidateforperceptual
cesses, we conducted a final experiment in which we used nonsense simulation(whichmaypartlyexplainwhythematcheffecthasusually
objects to interfere with high-level visual processes related to figure- beeninterpretedthewayithas).EdmistonandLupyan(2017)observed
ground separation and holistic object perception whilst severely lim- acleareffectofvisualnoiseinsingleword-picture-verification where
iting the likelihood of(non-visual) semantic processing. Linear mixed wordcuesarefollowedbyamatchingormismatchingpictureincorrect
effects analyses did not provide evidence for the view that high-level and inverted orientation and participants have to indicate which the
visualnoiserobustlyreducedthematcheffect,whichwasconfirmedby correctly oriented picture is. These results in combination with ours
exploratory Bayesian analyses that provided only weak evidence for suggest that single words activate low-level visual processes (likely
thathypothesis.Ourresultsthusbestfitwiththeviewthattheshape reflectingtypical objectfeatures), whereas implicitshapeinformation
matcheffectreliesmostlyonnon-visualsemanticprocesseswhichonly derived from event-level representations involves abstraction away
minimally interface with low-level visual processes. The exploratory fromthesensorysystems.Thisisconsistentwithneuroimagingstudies
Bayesian analyses suggest that the likelihood of interference of high- that implicated anterior temporal regions with high-level semantic
levelvisualnoisewassmallbutnon-zero. processing and conceptual combination, whereas visual regions are
92

M.Ostareketal. Cognition 182 (2019) 84–94
linked to individual object features, such as size, colour, and shape othersuchconcretenouns).JournalofExperimentalPsychology:General,132(2),163.
(Borghesanietal.,2016;Coutanche&Thompson-Schill,2014).Itwill Dahan,D.,&Tanenhaus,M.K.(2005).Lookingattheropewhenlookingforthesnake:
Conceptuallymediatedeyemovementsduringspoken-wordrecognition.Psychonomic
becrucialforfuturestudiestopreciselydelineatewhatdeterminesthe Bulletin&Review,12(3),453–459.
involvementandroleofsensoryprocessesinconceptualprocessing. DeGroot,F.,Koelewijn,T.,Huettig,F.,&Olivers,C.N.(2016).Astimulussetofwords
Theargumentsandmethodpresentedinthispapercanreadilybe andpicturesmatchedforvisualandsemanticsimilarity.JournalofCognitive
Psychology,28(1),1–15.
applied to other paradigms relying on congruency between sensory-
Edmiston,P.,&Lupyan,G.(2017).Visualinterferencedisruptsvisualknowledge.Journal
motorcontentevokedbylinguisticinputandataskinvolvingsensory- ofMemoryandLanguage,92,281–292.
motor processing: Congruency effects do not provide evidence that Engelen,J.A.,Bouwmeester,S.,deBruin,A.B.,&Zwaan,R.A.(2011).Perceptualsi-
sensory-motorsystemsproducethemunlessthetaskwhichlanguageis mulationindevelopinglanguagecomprehension.JournalofExperimentalChild
Psychology,110(4),659–675.
found to have an effect on only involves sensory-motor processes Fernandino,Leonardo,Binder,J.R.,Desai,R.H.,Pendl,S.L.,Humphries,C.J.,Gross,W.
(Lupyan & Ward, 2013; Ostarek & Huettig, 2017b). Therefore, match L.,...Seidenberg,M.S.(2016).ConceptRepresentationreflectsmultimodalab-
effectsareausefulfirststepafterwhichfurtherinvestigationsarere- straction:Aframeworkforembodiedsemantics.CerebralCortex,26(5),2018–2034.
https://doi.org/10.1093/cercor/bhv020.
quired to reveal the underlying mechanisms. Interference techniques
Fernandino,L.,Humphries,C.J.,Conant,L.L.,Seidenberg,M.S.,&Binder,J.R.(2016).
areappealingbecausetheygobeyondcorrelationalapproachesbydi- Heteromodalcorticalareasencodesensory-motorfeaturesofwordmeaning.Journal
rectlytestingcausality.
ofNeuroscience,36(38),9763–9769.https://doi.org/10.1523/JNEUROSCI.4095-15.
2016.
Fodor,J.A.(1975).Thelanguageofthought(Vol.5).HarvardUniversityPress.
Acknowledgements Fox,J.,&Weisberg,S.(2011).An{R}companiontoappliedregression(2nded.).Thousand
Oaks,CA:SageURL:http://socserv.socsci.mcmaster.ca/jfox/Books/Companion.
Halekoh,U.,&Højsgaard,S.(2014).AKenward-Rogerapproximationandparametric
We would like to thank Christoph Scheepers, Diane Pecher, and bootstrapmethodsfortestsinlinearmixedmodels–TheRpackagepbkrtest.Journal
TomHeymanfortheirhelpfulsuggestionsonpreviousversionsofthe ofStatisticalSoftware,59(9),1–30.
manuscript,andPhillipAldayforhisadviceonBayesianstatistics. Hauk,O.,Johnsrude,I.,&Pulvermüller,F.(2004).Somatotopicrepresentationofaction
wordsinhumanmotorandpremotorcortex.Neuron,41(2),301–307.
Hesselmann,G.,Hebart,M.,&Malach,R.(2011).DifferentialBOLDactivityassociated
AppendixA. Supplementarymaterial withsubjectiveandobjectivereportsduring“blindsight”innormalobservers.Journal
ofNeuroscience,31(36),12936–12944.
Supplementarydataassociatedwiththisarticlecanbefound,inthe
Hirschfeld,G.,Zwitserlood,P.,&Dobel,C.(2011).Effectsoflanguagecomprehensionon
visualprocessing–MEGdissociatesearlyperceptualandlateN400effects.Brainand
onlineversion,athttps://doi.org/10.1016/j.cognition.2018.08.017. Language,116(2),91–96.
Hoenig,K.,Sim,E.-J.,Bochev,V.,Herrnberger,B.,&Kiefer,M.(2008).Conceptual
References
flexibilityinthehumanbrain:Dynamicrecruitmentofsemanticmapsfromvisual,
motor,andmotion-relatedareas.JournalofCognitiveNeuroscience,20(10),
1799–1814.
Anderson,A.J.,Binder,J.R.,Fernandino,L.,Humphries,C.J.,Conant,L.L.,Aguilar,M., Huettig,F.,&Altmann,G.T.(2007).Visual-shapecompetitionduringlanguage-mediated
...Raizada,R.D.(2016).Predictingneuralactivitypatternsassociatedwithsentences attentionisbasedonlexicalinputandnotmodulatedbycontextualappropriateness.
usinganeurobiologicallymotivatedmodelofsemanticrepresentation.Cerebral VisualCognition,15(8),985–1018.
Cortex.http://cercor.oxfordjournals.org/content/early/2016/08/12/cercor.bhw240. Huettig,F.,&McQueen,J.M.(2007).Thetugofwarbetweenphonological,semanticand
abstract. shapeinformationinlanguage-mediatedvisualsearch.JournalofMemoryand
Barsalou,L.W.(1993).Flexibility,structure,andlinguisticvagaryinconcepts: Language,57(4),460–482.
Manifestationsofacompositionalsystemofperceptualsymbols.TheoriesofMemory, Kan,I.P.,&Thompson-Schill,S.L.(2004).Selectionfromperceptualandconceptual
1,29–31. representations.Cognitive,Affective,&BehavioralNeuroscience,4(4),466–482.
Barsalou,L.W.(1999).Perceptionsofperceptualsymbols.BehavioralandBrainSciences, Kurby,C.A.,&Zacks,J.M.(2013).Theactivationofmodality-specificrepresentations
22(04),637–660. duringdiscourseprocessing.BrainandLanguage,126(3),338–349.
Barsalou,L.W.(2008).Groundedcognition.AnnualReviewofPsychology,59(1),617–645. Lewis,G.,&Poeppel,D.(2014).Theroleofvisualrepresentationsduringthelexical
https://doi.org/10.1146/annurev.psych.59.103006.093639. accessofspokenwords.BrainandLanguage,134,1–10.
Barsalou,L.W.(2016).Onstayinggroundedandavoidingquixoticdeadends. Lupyan,G.,&Ward,E.J.(2013).Languagecanboostotherwiseunseenobjectsintovisual
PsychonomicBulletin&Review,23(4),1122–1142. awareness.ProceedingsoftheNationalAcademyofSciences,110(35),14196–14201.
Bates,D.,Mächler,M.,Bolker,B.,&Walker,S.(2014).Fittinglinearmixed-effectsmodels Mahon,B.Z.(2015).Whatisembodiedaboutcognition?Language,Cognitionand
usinglme4.ArXivPreprintArXiv:1406.5823.Retrievedfromhttps://arxiv.org/abs/ Neuroscience,30(4),420–429.
1406.5823. Mahon,B.Z.,&Caramazza,A.(2008).Acriticallookattheembodiedcognitionhy-
Binder,J.R.(2016).Indefenseofabstractconceptualrepresentations.Psychonomic pothesisandanewproposalforgroundingconceptualcontent.JournalofPhysiology-
Bulletin&Review,23(4),1096–1108. Paris,102(1),59–70.
Binder,J.R.,Conant,L.L.,Humphries,C.J.,Fernandino,L.,Simons,S.B.,Aguilar,M.,& Mani,N.,&Plunkett,K.(2010).Intheinfant’smind’sear:Evidenceforimplicitnamingin
Desai,R.H.(2016).Towardabrain-basedcomponentialsemanticrepresentation. 18-month-olds.PsychologicalScience,21(7),908–913.
CognitiveNeuropsychology,33(3–4),130–174.https://doi.org/10.1080/02643294. McQueen,J.M.,&Huettig,F.(2014).Interferenceofspokenwordrecognitionthrough
2016.1147426. phonologicalprimingfromvisualobjectsandprintedwords.Attention,Perception,&
Binder,J.R.,&Desai,R.H.(2011).Theneurobiologyofsemanticmemory.Trendsin Psychophysics,76(1),190–200.
CognitiveSciences,15(11),527–536. Meteyard,L.,Cuadrado,S.R.,Bahrami,B.,&Vigliocco,G.(2012).Comingofage:A
Borghesani,V.,&Piazza,M.(2017).Theneuro-cognitiverepresentationsofsymbols:The reviewofembodimentandtheneuroscienceofsemantics.Cortex,48(7),788–804.
caseofconcretewords.Neuropsychologia.Retrievedfromhttp://www.sciencedirect. Ostarek,M.,&Huettig,F.(2017b).Spokenwordscanmaketheinvisiblevisible—Testing
com/science/article/pii/S0028393217302397. theinvolvementoflow-levelvisualrepresentationsinspokenwordprocessing.
Borghesani,V.,Pedregosa,F.,Buiatti,M.,Amadon,A.,Eger,E.,&Piazza,M.(2016). JournalofExperimentalPsychology:HumanPerceptionandPerformance,43(3),499.
Wordmeaningintheventralvisualpath:Aperceptualtoconceptualgradientof Ostarek,M.,&Huettig,F.(2017a).Atask-dependentcausalroleforlow-levelvisual
semanticcoding.NeuroImage,143,128–140. processesinspokenwordcomprehension.JournalofExperimentalPsychology:
Bruffaerts,R.,Dupont,P.,Peeters,R.,DeDeyne,S.,Storms,G.,&Vandenberghe,R. Learning,Memory,andCognition,43(8),1215.https://doi.org/10.1037/xlm0000375.
(2013).SimilarityoffMRIactivitypatternsinleftperirhinalcortexreflectssemantic Papesh,M.H.(2015).Justoutofreach:Onthereliabilityoftheaction-sentencecom-
similaritybetweenwords.JournalofNeuroscience,33(47),18597–18607. patibilityeffect.JournalofExperimentalPsychology:General,144(6),e116.
Bürkner,P.-C.(2016).brms:AnRpackageforBayesianmultilevelmodelsusingStan. Pashler,H.,&Wagenmakers,E.-J.(2012).Editors’introductiontothespecialsectionon
JournalofStatisticalSoftware,80(1),1–28. replicabilityinpsychologicalscience:Acrisisofconfidence?Perspectiveson
Collins,A.M.,&Loftus,E.F.(1975).Aspreading-activationtheoryofsemanticproces- PsychologicalScience,7(6),528–530.
sing.PsychologicalReview,82(6),407. Patterson,K.,Nestor,P.J.,&Rogers,T.T.(2007).Wheredoyouknowwhatyouknow?
Coppens,L.C.,Gootjes,L.,&Zwaan,R.A.(2012).Incidentalpictureexposureaffects Therepresentationofsemanticknowledgeinthehumanbrain.NatureReviews
laterreading:EvidencefromtheN400.BrainandLanguage,122(1),64–69. Neuroscience,8(12),976–987.https://doi.org/10.1038/nrn2277.
Correia,J.,Formisano,E.,Valente,G.,Hausfeld,L.,Jansma,B.,&Bonte,M.(2014). Pecher,D.,vanDantzig,S.,Zwaan,R.A.,&Zeelenberg,R.(2009).Languagecompre-
Brain-basedtranslation:fMRIdecodingofspokenwordsinbilingualsrevealslan- 

## 引用

```

```
